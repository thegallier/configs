import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Define the Mamba Model
class MambaModel(nn.Module):
    def __init__(self, vocab_size1, vocab_size2, embedding_dim, hidden_dim):
        super(MambaModel, self).__init__()
        self.embedding1 = nn.Embedding(vocab_size1, embedding_dim)
        self.embedding2 = nn.Embedding(vocab_size2, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, batch_first=True)
        self.fc_words1 = nn.Linear(hidden_dim, vocab_size1)
        self.fc_words2 = nn.Linear(hidden_dim, vocab_size2)
        self.fc_floats = nn.Linear(hidden_dim, 2)  # 2 floats output

    def forward(self, x1, x2):
        embedded1 = self.embedding1(x1)
        embedded2 = self.embedding2(x2)
        embedded = torch.cat((embedded1, embedded2), dim=2)
        lstm_out, _ = self.lstm(embedded)
        word_out1 = self.fc_words1(lstm_out[:, 0])
        word_out2 = self.fc_words2(lstm_out[:, 1])
        float_out = self.fc_floats(lstm_out[:, 2:])
        return word_out1, word_out2, float_out

# Create Data Loader
class CustomDataset(Dataset):
    def __init__(self, word_sequences1, word_sequences2, float_sequences):
        self.word_sequences1 = word_sequences1
        self.word_sequences2 = word_sequences2
        self.float_sequences = float_sequences

    def __len__(self):
        return len(self.word_sequences1)

    def __getitem__(self, idx):
        return self.word_sequences1[idx], self.word_sequences2[idx], self.float_sequences[idx]

word_sequences1 = [torch.tensor([1, 2]), torch.tensor([3, 4])]  # Example word indices for list 1
word_sequences2 = [torch.tensor([5, 6]), torch.tensor([7, 8])]  # Example word indices for list 2
float_sequences = [torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0])]  # Example floats

dataset = CustomDataset(word_sequences1, word_sequences2, float_sequences)
data_loader = DataLoader(dataset, batch_size=2, shuffle=True)

# Training the Model
model = MambaModel(vocab_size1=10, vocab_size2=10, embedding_dim=16, hidden_dim=32)
criterion_words1 = nn.CrossEntropyLoss()
criterion_words2 = nn.CrossEntropyLoss()
criterion_floats = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(100):
    for word_seq1, word_seq2, float_seq in data_loader:
        optimizer.zero_grad()
        word_out1, word_out2, float_out = model(word_seq1, word_seq2)
        
        loss_words1 = criterion_words1(word_out1.view(-1, 10), word_seq1.view(-1))
        loss_words2 = criterion_words2(word_out2.view(-1, 10), word_seq2.view(-1))
        loss_floats = criterion_floats(float_out.view(-1, 2), float_seq.view(-1, 2))
        loss = loss_words1 + loss_words2 + loss_floats
        
        loss.backward()
        optimizer.step()
        
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')


import torch
import torch.nn as nn
import torch.optim as optim

# Model definition
class TimeSeriesNet(nn.Module):
    def __init__(self):
        super(TimeSeriesNet, self).__init__()
        self.fc1 = nn.Linear(50 * 25, 500)  # Flatten input and pass through FC layers
        self.fc2 = nn.Linear(500, 10 * 7 * 3)  # Output for 10 time steps, 7 features, 3 values each

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x.view(-1, 10, 7, 3)  # Reshape to match the output shape with softmax on features

# Initialize the model, loss function, and optimizer
model = TimeSeriesNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Generate random data for demonstration (100 batches, 50 time steps, 25 features)
x_train = torch.randn(100, 50, 25)
y_train = torch.randint(0, 3, (100, 10, 7))  # Random target with 3 classes for 10 time steps, 7 features

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(x_train)
    # Loss calculation needs to consider the categorical output
    loss = criterion(outputs.view(-1, 3), y_train.view(-1))
    loss.backward()
    optimizer.step()
    
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

print("Training complete!")


[[[[[

import pandas as pd

# Sample DataFrame
data = {
    'time': pd.to_datetime(['2021-01-01 10:00', '2021-01-01 10:01', '2021-01-01 10:02', '2021-01-01 10:03', '2021-01-01 10:04', '2021-01-01 10:05']),
    'sym': ['AAPL', 'AAPL', 'AAPL', 'GOOG', 'GOOG', 'AAPL'],
    'price': [150, 150, 150, 800, 800, 150],
    'qty': [100, 100, 100, 50, 50, 100]
}
df = pd.DataFrame(data)

# Step 1: Copy DataFrame without the 'time' column for comparison
df_no_time = df.drop('time', axis=1)

# Step 2: Use shift and compare to find changes (True if change)
changes = (df_no_time != df_no_time.shift()).any(axis=1)

# Step 3: Create a group identifier for each set of consecutive rows
df['group_id'] = changes.cumsum()

# Step 4: Select the first row from each group
result = df.groupby('group_id').first().reset_index(drop=True)

print(result)



/==
import re

# Sample string
text = "abCdef abGhi abJkl) some other words abMno abPqr) anotherAbC abXyz)"

# Regular expression pattern
pattern = r'\bab[A-Z][a-zA-Z]*[ \)]'

# Find all matches
matches = re.findall(pattern, text)

# Clean up matches by removing trailing spaces or parentheses
matches = [match.rstrip(' )') for match in matches]

print(matches)

++++
import re

# Sample string
text = "This is a string containing htto123, htto_456, and other htto words."

# Regular expression pattern
pattern = r'htto\w*'

# Find all matches
matches = re.findall(pattern, text)

print("Extracted substrings:", matches)


++++

import torch
import torch.nn as nn

class CustomLoss(nn.Module):
    def __init__(self, class_weights):
        super(CustomLoss, self).__init__()
        self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))

    def forward(self, outputs, targets):
        batch_size = outputs.size(0)
        num_outputs = outputs.size(1)
        num_classes = outputs.size(2)
        
        # Reshape outputs to (batch_size * 31, 3)
        outputs = outputs.view(batch_size * num_outputs, num_classes)
        
        # Reshape targets to (batch_size * 31)
        targets = targets.view(batch_size * num_outputs)
        
        # Compute the loss
        loss = self.criterion(outputs, targets)
        
        return loss

# Example usage
if __name__ == "__main__":
    # Dummy data
    batch_size = 4
    num_outputs = 31
    num_classes = 3
    
    # Random predictions (logits) from the model
    outputs = torch.randn(batch_size, num_outputs, num_classes, requires_grad=True)
    
    # Random targets
    targets = torch.randint(0, num_classes, (batch_size, num_outputs))
    
    # Example class counts (for demonstration)
    class_counts = np.array([500, 100, 400])
    total_samples = np.sum(class_counts)
    class_weights = total_samples / (len(class_counts) * class_counts)
    
    # Instantiate the custom loss function with class weights
    criterion = CustomLoss(class_weights)
    
    # Calculate the loss
    loss = criterion(outputs, targets)
    
    print(f"Loss: {loss.item()}")



class HiPPOLayer(nn.Module):
    def __init__(self, A, B):
        super(HiPPOLayer, self).__init__()
        self.A = A
        self.B = B

    def forward(self, x):
        batch_size, seq_len, feature_dim = x.size()
        state = torch.zeros((batch_size, self.A.size(0), feature_dim), device=x.device)
        for t in range(seq_len):
            state = torch.matmul(self.A, state) + torch.matmul(self.B, x[:, t, :].unsqueeze(1))
        return state

class HiPPOTransformerModel(nn.Module):
    def __init__(self, configs):
        super(HiPPOTransformerModel, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed_dim, configs.dropout)
        self.class_strategy = configs.class_strategy

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=configs.d_model, nhead=8)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=configs.e_layers)
        self.projector = nn.Linear(configs.d_model, configs.output_size, bias=True)
        
        self.A = nn.Parameter(A_legS)
        self.B = nn.Parameter(B_legS)
        self.hippo_layer = HiPPOLayer(self.A, self.B)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.use_norm:
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

        enc_out = self.enc_embedding(x_enc, x_mark_enc)
        hippo_out = self.hippo_layer(enc_out)
        enc_out = hippo_out.permute(1, 0, 2)
        enc_out = self.encoder(enc_out)
        enc_out = enc_out.permute(1, 0, 2)
        dec_out = self.projector(enc_out)

        if self.use_norm:
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))

        return dec_out

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        return dec_out.permute(0, 2, 1)[:, :, :self.pred_len]

------

class HiPPOLayer(nn.Module):
    def __init__(self, A, B):
        super(HiPPOLayer, self).__init__()
        self.A = A
        self.B = B

    def forward(self, x):
        # x shape: (batch_size, seq_len, feature_dim)
        batch_size, seq_len, feature_dim = x.size()
        
        # Initialize state
        state = torch.zeros((batch_size, self.A.size(0), feature_dim), device=x.device)
        
        # Iterate through the sequence
        for t in range(seq_len):
            state = torch.matmul(self.A, state) + torch.matmul(self.B, x[:, t, :].unsqueeze(1))
        
        return state

class HiPPOTransformerModel(nn.Module):
    def __init__(self, configs):
        super(HiPPOTransformerModel, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm
        self.embed_dim = configs.embed_dim
        self.d_model = configs.d_model
        self.enc_embedding = nn.Linear(self.embed_dim, self.d_model)
        self.class_strategy = configs.class_strategy

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=configs.d_model, nhead=8)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=configs.e_layers)

        self.projector = nn.Linear(configs.d_model, configs.output_size, bias=True)
        
        # HiPPO matrices
        self.A = nn.Parameter(A_legS)
        self.B = nn.Parameter(B_legS)
        
        # HiPPO layer
        self.hippo_layer = HiPPOLayer(self.A, self.B)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.use_norm:
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

        # Apply HiPPO layer to the original data
        hippo_out = self.hippo_layer(x_enc)
        
        # Embed the HiPPO output to match d_model
        enc_out = self.enc_embedding(hippo_out)
        enc_out = enc_out.permute(1, 0, 2)  # (L, N, E)
        enc_out = self.encoder(enc_out)
        enc_out = enc_out.permute(1, 0, 2)  # (N, L, E)
        dec_out = self.projector(enc_out)  # Project to the target dimensions

        if self.use_norm:
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))

        return dec_out

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        return dec_out.permute(0, 2, 1)[:, :, :self.pred_len]  # [B, D, L]

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset

# Specifications
num_batches = 100
seq_len = 100
num_features = 25
pred_len = 25
output_features = 5
group_size = 4
column_group_size = 5

# Generate input data
x = np.zeros((num_batches, seq_len, num_features))
for batch in range(num_batches):
    for i in range(1, seq_len):
        x[batch, i] = x[batch, i - 1] + 1

# Generate target data
y = np.zeros((num_batches, pred_len, output_features))
for batch in range(num_batches):
    for i in range(pred_len):
        row_start = i * group_size
        row_end = row_start + group_size
        y[batch, i] = np.sum(np.mean(x[batch, row_start:row_end], axis=0).reshape(-1, column_group_size), axis=1)

# Convert to torch tensors
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# Generate encoder and decoder marks (using random data for example purposes)
x_mark_enc = torch.randn(num_batches, seq_len, num_features)
x_mark_dec = torch.randn(num_batches, pred_len, output_features)

# Create DataLoader
dataset = TensorDataset(x, x_mark_enc, x_mark_dec, y)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

# Initialize configurations
configs = Configs()

# Initialize the HiPPO-Transformer model
model = HiPPOTransformerModel(configs)

# Set up the optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.MSELoss()

# Training loop
for epoch in range(10):  # Number of epochs
    for data, x_mark_enc, x_mark_dec, target in dataloader:
        optimizer.zero_grad()
        x_enc = data  # Encoder input
        output = model(x_enc, x_mark_enc, x_enc, x_mark_dec)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

/===

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset

# Specifications
num_batches = 100
seq_len = 100
num_features = 25
pred_len = 25
output_features = 5
group_size = 4
column_group_size = 5

# Generate input data
x = np.zeros((num_batches, seq_len, num_features))
for batch in range(num_batches):
    for i in range(1, seq_len):
        x[batch, i] = x[batch, i - 1] + 1

# Generate target data
y = np.zeros((num_batches, pred_len, output_features))
for batch in range(num_batches):
    for i in range(pred_len):
        row_start = i * group_size
        row_end = row_start + group_size
        y[batch, i] = np.sum(np.mean(x[batch, row_start:row_end], axis=0).reshape(-1, column_group_size), axis=1)

# Convert to torch tensors
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# Generate encoder and decoder marks (using random data for example purposes)
x_mark_enc = torch.randn(num_batches, seq_len, num_features)
x_mark_dec = torch.randn(num_batches, pred_len, output_features)

# Create DataLoader
dataset = TensorDataset(x, x_mark_enc, x_mark_dec, y)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

/====

import torch
import torch.nn as nn
import torch.nn.functional as F

class DataEmbedding_inverted(nn.Module):
    def __init__(self, seq_len, d_model, embed, freq, dropout=0.1):
        super(DataEmbedding_inverted, self).__init__()
        self.seq_len = seq_len
        self.d_model = d_model
        self.position_embedding = nn.Embedding(seq_len, d_model)
        self.value_embedding = nn.Linear(embed, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, x_mark):
        x = self.value_embedding(x) + self.position_embedding(torch.arange(self.seq_len).to(x.device))
        return self.dropout(x)

class TransformerModel(nn.Module):
    def __init__(self, configs):
        super(TransformerModel, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq, configs.dropout)
        self.class_strategy = configs.class_strategy
        
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=configs.d_model, nhead=8)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=configs.e_layers)
        
        self.projector = nn.Linear(configs.d_model, configs.output_size, bias=True)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.use_norm:
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

        _, _, N = x_enc.shape
        enc_out = self.enc_embedding(x_enc, x_mark_enc)
        enc_out = enc_out.permute(1, 0, 2)  # (L, N, E)
        enc_out = self.encoder(enc_out)
        enc_out = enc_out.permute(1, 0, 2)  # (N, L, E)
        dec_out = self.projector(enc_out)  # Project to the target dimensions

        if self.use_norm:
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))

        return dec_out

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        return dec_out.permute(0, 2, 1)[:, :, :self.pred_len]  # [B, D, L]


/====

ChatGPT
To use the S-D-Mamba model with the specified data dimensions (3 batches, 30 features, 100 time steps) and forecast 25 and 50 steps, you can follow these steps based on the repository's code structure. Here is an example in Python:

Prepare the data:

python
Copy code
import torch

# Example data
batch_size = 3
num_features = 30
time_steps = 100
forecast_steps = 25  # or 50

data = torch.randn(batch_size, num_features, time_steps)
Initialize the model:

python
Copy code
from model import S_Mamba  # Assuming the model class is in a file named model.py

# Initialize the model
model = S_Mamba(input_size=num_features, output_size=forecast_steps)
Forward pass through the model:

python
Copy code
output = model(data)
print(output.shape)  # Should be (3, 30, 25) for forecast_steps = 25
For different forecast steps, simply change the forecast_steps variable to 50 and re-initialize the model.

Make sure to refer to the repository's documentation and code for specific parameter names and model initialization details. You can access the repository here.

Were there not other models like itransformer etc?
ChatGPT
To use different models like iTransformer along with S-Mamba for forecasting with your data, here are examples for both models assuming they are implemented similarly:

1. Prepare the data:
python
Copy code
import torch

# Example data
batch_size = 3
num_features = 30
time_steps = 100

data = torch.randn(batch_size, num_features, time_steps)
2. Initialize and use S-Mamba:
python
Copy code
from model import S_Mamba

# Forecast 25 steps
forecast_steps_25 = 25
model_25 = S_Mamba(input_size=num_features, output_size=forecast_steps_25)
output_25 = model_25(data)
print(output_25.shape)  # Expected shape: (3, 30, 25)

# Forecast 50 steps
forecast_steps_50 = 50
model_50 = S_Mamba(input_size=num_features, output_size=forecast_steps_50)
output_50 = model_50(data)
print(output_50.shape)  # Expected shape: (3, 30, 50)
3. Initialize and use iTransformer:
python
Copy code
from model import iTransformer

# Forecast 25 steps
model_iTransformer_25 = iTransformer(input_size=num_features, output_size=forecast_steps_25)
output_iTransformer_25 = model_iTransformer_25(data)
print(output_iTransformer_25.shape)  # Expected shape: (3, 30, 25)

# Forecast 50 steps
model_iTransformer_50 = iTransformer(input_size=num_features, output_size=forecast_steps_50)
output_iTransformer_50 = model_iTransformer_50(data)
print(output_iTransformer_50.shape)  # Expected shape: (3, 30, 50)
Ensure to check the repository for the correct model names and initialization parameters. The repository can be found here.

Does the code also allow to forecast derived ys, so for example the time slices have 30 features but we forecast 5 ys
ChatGPT
To forecast derived ys using the models, you can modify the output dimensions of the models accordingly. Here’s how you can do it:

1. Prepare the data:
python
Copy code
import torch

# Example data
batch_size = 3
num_features = 30
time_steps = 100
forecast_steps = 25  # or 50
output_features = 5  # Number of ys to forecast

data = torch.randn(batch_size, num_features, time_steps)
2. Initialize and use S-Mamba:
python
Copy code
from model import S_Mamba

# Initialize the model for forecasting derived ys
model = S_Mamba(input_size=num_features, output_size=output_features)

# Perform forecasting
output = model(data)
print(output.shape)  # Expected shape: (3, 5, forecast_steps)
3. Initialize and use iTransformer:
python
Copy code
from model import iTransformer

# Initialize the model for forecasting derived ys
model_iTransformer = iTransformer(input_size=num_features, output_size=output_features)

# Perform forecasting
output_iTransformer = model_iTransformer(data)
print(output_iTransformer.shape)  # Expected shape: (3, 5, forecast_steps)
This assumes that the models allow specifying different output feature dimensions. Check the repository’s documentation for exact details and ensure the models are compatible with forecasting derived ys.

What other models does the code have?
ChatGPT

Searched 4 sites

The S-D-Mamba repository includes several models besides the primary S-Mamba model, designed for time series forecasting. The notable models mentioned are:

S-Mamba: The core model that uses a Mamba-based architecture with a bidirectional Mamba block and a Feed-Forward network to capture inter-variate correlations and temporal dependencies effectively. It is noted for its efficiency and lower GPU memory requirements while maintaining superior performance compared to other state-of-the-art models.

iTransformer: This model is inspired by the Transformer architecture and is included for comparative analysis. It emphasizes tokenizing the time series data, similar to how sequential text is handled in natural language processing. This helps in standardizing the temporal series format and extracting mutual information among variates.

Example Usage for Forecasting Derived Ys
Here's an example demonstrating how to use these models to forecast derived ys:

1. Data Preparation:
python
Copy code
import torch

# Example data
batch_size = 3
num_features = 30
time_steps = 100
forecast_steps = 25  # or 50
output_features = 5  # Number of derived ys to forecast

data = torch.randn(batch_size, num_features, time_steps)
2. Using S-Mamba:
python
Copy code
from model import S_Mamba

# Initialize the model for forecasting derived ys
model = S_Mamba(input_size=num_features, output_size=output_features)

# Perform forecasting
output = model(data)
print(output.shape)  # Expected shape: (3, 5, forecast_steps)
3. Using iTransformer:
python
Copy code
from model import iTransformer

# Initialize the model for forecasting derived ys
model_iTransformer = iTransformer(input_size=num_features, output_size=output_features)

# Perform forecasting
output_iTransformer = model_iTransformer(data)
print(output_iTransformer.shape)  # Expected shape: (3, 5, forecast_steps)
These models and the examples provided can be adapted based on specific needs and datasets. For more details on the implementation and other available scripts, you can refer to the repository's README and documentation​ (ar5iv)​​ (GitHub)​.

Can you tell me what other models there are in the readme 
ChatGPT
The S-D-Mamba repository README file mentions several other models used in the context of time series forecasting. Here are the key models included:

S-Mamba: The primary model proposed in the repository, which is a Mamba-based model designed to handle time series forecasting effectively.

iTransformer: A model inspired by the Transformer architecture, used for tokenizing and processing time series data in a way similar to how sequential text is handled in natural language processing.

ARIMA: Autoregressive Integrated Moving Average, a traditional statistical method for time series analysis that is included for comparative analysis.

LSTM: Long Short-Term Memory networks, a type of recurrent neural network well-suited for learning from sequences and widely used for time series forecasting.

TFT: Temporal Fusion Transformer, a model specifically designed for interpretable multi-horizon time series forecasting.

Informer: A model that enhances the efficiency of the Transformer architecture for long sequence time-series forecasting by reducing the computation complexity.

These models are compared to evaluate the performance and effectiveness of S-Mamba in various time series forecasting tasks. For more details and the specific scripts used for these models, you can refer to the repository's README​ (GitHub)​.

Show me an example call for each
ChatGPT
Here are example calls for each model mentioned in the S-D-Mamba repository, assuming they are implemented and available for use:

1. S-Mamba:
python
Copy code
import torch
from model import S_Mamba

# Example data
batch_size = 3
num_features = 30
time_steps = 100
forecast_steps = 25
output_features = 5  # Number of derived ys to forecast

data = torch.randn(batch_size, num_features, time_steps)

# Initialize and use S-Mamba
model = S_Mamba(input_size=num_features, output_size=output_features)
output = model(data)
print(output.shape)  # Expected shape: (3, 5, 25)
2. iTransformer:
python
Copy code
import torch
from model import iTransformer

# Example data
data = torch.randn(batch_size, num_features, time_steps)

# Initialize and use iTransformer
model_iTransformer = iTransformer(input_size=num_features, output_size=output_features)
output_iTransformer = model_iTransformer(data)
print(output_iTransformer.shape)  # Expected shape: (3, 5, 25)
3. ARIMA:
For ARIMA, you would typically use the statsmodels library in Python:

python
Copy code
import numpy as np
import statsmodels.api as sm

# Example data for ARIMA
data = np.random.randn(100)  # Assuming univariate time series data

# Fit ARIMA model
model_arima = sm.tsa.ARIMA(data, order=(5, 1, 0))
model_fit = model_arima.fit()
forecast = model_fit.forecast(steps=25)
print(forecast)
4. LSTM:
python
Copy code
import torch
import torch.nn as nn

# Example LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Initialize and use LSTM
hidden_size = 50
model_lstm = LSTMModel(num_features, hidden_size, output_features)
output_lstm = model_lstm(data)
print(output_lstm.shape)  # Expected shape: (3, 5)
5. TFT (Temporal Fusion Transformer):
For TFT, you would use a specific library like PyTorch Forecasting:

python
Copy code
from pytorch_forecasting.models import TemporalFusionTransformer
from pytorch_forecasting.data.examples import get_stallion_data

# Example data and dataloaders
data = get_stallion_data()
# Preprocess and create TimeSeriesDataSet and DataLoader

# Initialize and use TFT
model_tft = TemporalFusionTransformer.from_dataset(dataset, learning_rate=0.03)
output_tft = model_tft.predict(data)
print(output_tft.shape)
6. Informer:
python
Copy code
import torch
from informer_model import Informer  # Assuming an Informer model is defined

# Example data
data = torch.randn(batch_size, num_features, time_steps)

# Initialize and use Informer
model_informer = Informer(enc_in=num_features, dec_in=output_features, c_out=output_features, seq_len=time_steps, label_len=forecast_steps, out_len=forecast_steps)
output_informer = model_informer(data)
print(output_informer.shape)  # Expected shape: (3, 5, 25)
For the Informer and TFT models, the exact implementation may vary, and you might need to adapt the example according to the actual library or codebase you're using. Check the specific documentation or implementation details provided in the repository or corresponding libraries.

Can you pad the timeseries?
ChatGPT
Padding a time series involves adding values (usually zeros) to ensure that all sequences have the same length, which is often required for batch processing in machine learning models. Here's how you can pad the time series data using PyTorch:

Example with PyTorch:
python
Copy code
import torch
import torch.nn.utils.rnn as rnn_utils

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)  # (batch_size, num_features, time_steps)
time_series_2 = torch.randn(3, 30, 80)   # Shorter sequence

# Create a list of these tensors
time_series_list = [time_series_1, time_series_2]

# Pad the sequences to the same length
padded_data = rnn_utils.pad_sequence(time_series_list, batch_first=True)

print(padded_data.shape)  # Expected shape: (2, 3, 30, 100)
Explanation:
Import Libraries: Import torch and torch.nn.utils.rnn.
Create Time Series Data: Create tensors representing your time series data. In this example, we have two time series with different lengths.
Pad Sequences: Use pad_sequence from torch.nn.utils.rnn to pad the sequences. Setting batch_first=True ensures that the batch dimension is first.
Output Shape: Print the shape of the padded data to verify the padding.
Handling Time Series Data with Models:
Here’s an example integrating padded time series data into a model like S-Mamba:

python
Copy code
import torch
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

# Example data
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Determine lengths of each time series for packing
lengths = [seq.size(2) for seq in time_series_list]

# Pad sequences to the same length
padded_data = pad_sequence(time_series_list, batch_first=True)

# Initialize the model (example with S-Mamba)
from model import S_Mamba
output_features = 5  # Forecasting derived ys
model = S_Mamba(input_size=30, output_size=output_features)

# Pack padded sequence
packed_data = pack_padded_sequence(padded_data, lengths, batch_first=True, enforce_sorted=False)

# Forward pass through the model
packed_output, _ = model(packed_data)

# Pad the packed output back to sequence
output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)

print(output.shape)  # Expected shape will depend on the model's output handling
Explanation:
Lengths: Calculate the lengths of each sequence for packing.
Pad Sequences: Use pad_sequence to ensure all sequences have the same length.
Initialize Model: Initialize the model you are using, here S-Mamba.
Pack Sequences: Use pack_padded_sequence to pack the padded sequences. This step is crucial for handling variable-length sequences efficiently.
Forward Pass: Perform the forward pass through the model.
Unpack Sequences: Use pad_packed_sequence to unpack the sequences back.
These steps ensure that your model can handle variable-length sequences by padding them and then packing them for efficient processing. This approach works similarly for other models like iTransformer, LSTM, etc. Adapt the padding and packing/unpacking process as needed for each specific model implementation.

How do you do it with this library?
ChatGPT
To pad time series data and use it with models from the S-D-Mamba library, you need to ensure the sequences are of equal length before feeding them into the models. Here’s how you can do it:

Step-by-Step Guide
Import Libraries: Ensure you have the necessary libraries installed and imported.

Create Example Data: Simulate some example time series data with varying lengths.

Pad Sequences: Use PyTorch's pad_sequence to make the sequences of equal length.

Integrate with Models: Pass the padded data to your model.

Example Code
Here is a complete example:

python
Copy code
import torch
import torch.nn.utils.rnn as rnn_utils
from model import S_Mamba, iTransformer

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)  # (batch_size, num_features, time_steps)
time_series_2 = torch.randn(3, 30, 80)   # Shorter sequence
time_series_3 = torch.randn(3, 30, 90)   # Another sequence with different length

# List of sequences
time_series_list = [time_series_1, time_series_2, time_series_3]

# Determine lengths of each time series for packing
lengths = [seq.size(2) for seq in time_series_list]

# Pad sequences to the same length
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)

# Verify the shape of padded data
print(padded_data.shape)  # Expected shape: (3, 30, max_time_steps)

# Example using S-Mamba
output_features = 5  # Number of derived ys to forecast
forecast_steps = 25  # Forecasting steps
model_s_mamba = S_Mamba(input_size=30, output_size=output_features)

# Forward pass through S-Mamba
output_s_mamba = model_s_mamba(padded_data)
print(output_s_mamba.shape)  # Expected shape: (3, 5, forecast_steps)

# Example using iTransformer
model_itransformer = iTransformer(input_size=30, output_size=output_features)

# Forward pass through iTransformer
output_itransformer = model_itransformer(padded_data)
print(output_itransformer.shape)  # Expected shape: (3, 5, forecast_steps)
Explanation
Creating Example Data: Three example time series of varying lengths are created.
Padding Sequences: The sequences are padded using pad_sequence and then transposed to match the input format expected by the models (batch_size, num_features, max_time_steps).
Model Initialization: S-Mamba and iTransformer models are initialized with the appropriate input and output sizes.
Forward Pass: The padded data is passed through both models, and the output shapes are printed to verify.
This ensures that the models can handle variable-length time series by padding them to a common length. Make sure to adapt the code based on the exact implementation details of the models in the S-D-Mamba repository.

How do you train the different models in the library?
ChatGPT
To train different models in the S-D-Mamba library, you'll typically follow a process involving data preparation, model initialization, loss function definition, optimization, and training loop implementation. Here's an example approach for each model mentioned:

General Steps
Data Preparation: Load and preprocess your time series data.
Model Initialization: Initialize the model with appropriate parameters.
Define Loss Function and Optimizer: Choose a suitable loss function and optimizer.
Training Loop: Implement the training loop, including forward pass, loss computation, backward pass, and parameter update.
Example Code for Training S-Mamba and iTransformer
Data Preparation
Ensure your data is padded to handle variable sequence lengths. Here's a quick example:

python
Copy code
import torch
import torch.nn.utils.rnn as rnn_utils
from torch.utils.data import DataLoader, TensorDataset

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Pad sequences to the same length
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)
labels = torch.randn(3, 5, 25)  # Example labels

# Create DataLoader
dataset = TensorDataset(padded_data, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
Model Initialization
python
Copy code
from model import S_Mamba, iTransformer

# Initialize models
input_size = 30
output_size = 5  # Number of ys to forecast
model_s_mamba = S_Mamba(input_size=input_size, output_size=output_size)
model_itransformer = iTransformer(input_size=input_size, output_size=output_size)
Define Loss Function and Optimizer
python
Copy code
import torch.optim as optim

# Define loss function and optimizers
criterion = torch.nn.MSELoss()
optimizer_s_mamba = optim.Adam(model_s_mamba.parameters(), lr=0.001)
optimizer_itransformer = optim.Adam(model_itransformer.parameters(), lr=0.001)
Training Loop
python
Copy code
# Training loop for S-Mamba
num_epochs = 10

for epoch in range(num_epochs):
    for data, target in dataloader:
        optimizer_s_mamba.zero_grad()
        output = model_s_mamba(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_s_mamba.step()
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

# Training loop for iTransformer
for epoch in range(num_epochs):
    for data, target in dataloader:
        optimizer_itransformer.zero_grad()
        output = model_itransformer(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_itransformer.step()
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')
Additional Models: ARIMA, LSTM, TFT, Informer
ARIMA
Training an ARIMA model is different as it typically doesn't involve a standard training loop like deep learning models. Instead, you fit the model to your data:

python
Copy code
import numpy as np
import statsmodels.api as sm

# Example data for ARIMA
data = np.random.randn(100)
model_arima = sm.tsa.ARIMA(data, order=(5, 1, 0))
model_fit = model_arima.fit()
print(model_fit.summary())
LSTM
python
Copy code
import torch
import torch.nn as nn
import torch.optim as optim

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Initialize LSTM model
hidden_size = 50
model_lstm = LSTMModel(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)

# Training loop for LSTM
for epoch in range(num_epochs):
    for data, target in dataloader:
        optimizer_lstm.zero_grad()
        output = model_lstm(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_lstm.step()
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')
TFT and Informer
These models require specific libraries or implementations. Refer to the documentation of libraries like PyTorch Forecasting for TFT or custom implementations for Informer.

Conclusion
Each model requires specific handling depending on its architecture. The general steps of data preparation, model initialization, loss function and optimizer setup, and the training loop are consistent across deep learning models. Adjust the implementation details according to the specific model requirements and the library documentation. For more detailed implementation, refer to the S-D-Mamba GitHub repository and relevant model documentation.

Show me for each how to handle padded data
ChatGPT
Handling Padded Data in Different Models
Here’s how you can handle padded data for each of the models discussed:

1. S-Mamba
python
Copy code
import torch
import torch.nn.utils.rnn as rnn_utils
from model import S_Mamba

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Determine lengths of each time series
lengths = [seq.size(2) for seq in time_series_list]

# Pad sequences to the same length
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)

# Example labels
labels = torch.randn(3, 5, 25)

# Initialize DataLoader
dataset = TensorDataset(padded_data, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Initialize model
model_s_mamba = S_Mamba(input_size=30, output_size=5)

# Training loop
criterion = torch.nn.MSELoss()
optimizer_s_mamba = torch.optim.Adam(model_s_mamba.parameters(), lr=0.001)

for epoch in range(10):
    for data, target in dataloader:
        optimizer_s_mamba.zero_grad()
        output = model_s_mamba(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_s_mamba.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
2. iTransformer
python
Copy code
import torch
from model import iTransformer

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Determine lengths of each time series
lengths = [seq.size(2) for seq in time_series_list]

# Pad sequences to the same length
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)

# Example labels
labels = torch.randn(3, 5, 25)

# Initialize DataLoader
dataset = TensorDataset(padded_data, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Initialize model
model_itransformer = iTransformer(input_size=30, output_size=5)

# Training loop
criterion = torch.nn.MSELoss()
optimizer_itransformer = torch.optim.Adam(model_itransformer.parameters(), lr=0.001)

for epoch in range(10):
    for data, target in dataloader:
        optimizer_itransformer.zero_grad()
        output = model_itransformer(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_itransformer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
3. ARIMA
ARIMA models typically do not require padding as they are used for univariate time series analysis.

4. LSTM
python
Copy code
import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn_utils
from torch.utils.data import DataLoader, TensorDataset

# Example LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Pad sequences
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)

# Example labels
labels = torch.randn(3, 5, 25)

# Initialize DataLoader
dataset = TensorDataset(padded_data, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Initialize model
hidden_size = 50
model_lstm = LSTMModel(input_size=30, hidden_size=hidden_size, output_size=5)

# Training loop
criterion = nn.MSELoss()
optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=0.001)

for epoch in range(10):
    for data, target in dataloader:
        optimizer_lstm.zero_grad()
        output = model_lstm(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_lstm.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
5. TFT (Temporal Fusion Transformer)
For TFT, you typically use a specific library like PyTorch Forecasting, which handles padding internally. Here's a conceptual example:

python
Copy code
from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline, TemporalFusionTransformer

# Example data and dataloaders
# Note: PyTorch Forecasting handles padding and batching internally
data = get_stallion_data()  # Example dataset
max_encoder_length = 36
max_prediction_length = 6

training = TimeSeriesDataSet(
    data,
    time_idx="time_idx",
    target="target",
    group_ids=["group"],
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    static_categoricals=["static_category"],
    static_reals=["static_real"],
    time_varying_known_categoricals=["known_categorical"],
    time_varying_known_reals=["known_real"],
    time_varying_unknown_categoricals=["unknown_categorical"],
    time_varying_unknown_reals=["unknown_real"],
    target_normalizer=GroupNormalizer(
        groups=["group"], transformation="softplus"
    ),
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
)

train_dataloader = training.to_dataloader(train=True, batch_size=32, num_workers=0)

# Initialize and use TFT
model_tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=0.03,
    hidden_size=16,
    attention_head_size=1,
    dropout=0.1,
    hidden_continuous_size=8,
    output_size=7,  # Quantiles
    loss=QuantileLoss(),
    log_interval=10,  # Log every 10 batches
    reduce_on_plateau_patience=4,
)

trainer = pl.Trainer(
    max_epochs=30,
    gpus=0,  # Train on CPU, or use gpus=1 to train on GPU
)
trainer.fit(
    model_tft,
    train_dataloaders=train_dataloader,
)
6. Informer
python
Copy code
import torch
from informer_model import Informer  # Assuming an Informer model is defined

# Example data with variable lengths
time_series_1 = torch.randn(3, 30, 100)
time_series_2 = torch.randn(3, 30, 80)
time_series_list = [time_series_1, time_series_2]

# Pad sequences to the same length
padded_data = rnn_utils.pad_sequence([seq.transpose(1, 2) for seq in time_series_list], batch_first=True).transpose(1, 2)

# Example labels
labels = torch.randn(3, 5, 25)

# Initialize DataLoader
dataset = TensorDataset(padded_data, labels)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Initialize model
model_informer = Informer(enc_in=30, dec_in=5, c_out=5, seq_len=100, label_len=25, out_len=25)

# Training loop
criterion = torch.nn.MSELoss()
optimizer_informer = torch.optim.Adam(model_informer.parameters(), lr=0.001)

for epoch in range(10):
    for data, target in dataloader:
        optimizer_informer.zero_grad()
        output = model_informer(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer_informer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
Summary
The above examples demonstrate how to handle padded data for training various models in the S-D-Mamba library and related models. The process involves preparing padded sequences, defining models, setting up loss functions and optimizers, and implementing training loops. Ensure you adapt the code based on the specific implementation details and requirements of each model.

Give me a one paragraph summary for each of the models
ChatGPT
S-Mamba
The S-Mamba model is a time series forecasting model that utilizes a Mamba-based architecture with a bidirectional Mamba block and a Feed-Forward network. This design enables the model to effectively capture inter-variate correlations and temporal dependencies. The S-Mamba model stands out for its efficiency and low GPU memory requirements, while still achieving superior performance compared to other state-of-the-art models. The architecture includes several layers, such as a Linear Tokenization Layer, a Mamba inter-Variate correlation Encoding Layer, and a FFN Temporal Dependencies Encoding Layer, which work together to process and forecast time series data efficiently​ (GitHub)​​ (GitHub)​.

iTransformer
The iTransformer model, inspired by the Transformer architecture, is designed for time series forecasting by tokenizing and processing time series data similarly to how sequential text is handled in natural language processing. It emphasizes capturing mutual information among variates and utilizes a tokenization method to standardize the temporal series format. The iTransformer model leverages attention mechanisms to compute the impact of all variables on a given variable, facilitating precise information learning. This approach allows the model to handle complex dependencies within the time series data effectively​ (ar5iv)​​ (GitHub)​.

ARIMA
The Autoregressive Integrated Moving Average (ARIMA) model is a traditional statistical method for analyzing and forecasting univariate time series data. ARIMA combines autoregressive (AR) terms, differencing (I) to make the data stationary, and moving average (MA) terms to model the temporal dependencies. It is particularly effective for short-term forecasting and can handle various types of time series data, including those with trends and seasonality. The model is fitted to historical data to identify the best parameters for forecasting future values​ (GitHub)​.

LSTM
Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) well-suited for learning from sequences and handling long-term dependencies. LSTM networks include memory cells that can maintain information over long periods, mitigating the vanishing gradient problem common in traditional RNNs. This makes LSTMs particularly effective for time series forecasting tasks where long-term dependencies are crucial. The architecture typically includes layers of LSTM cells followed by fully connected layers to produce the final forecast​ (GitHub)​​ (ar5iv)​.

TFT (Temporal Fusion Transformer)
The Temporal Fusion Transformer (TFT) is an advanced model specifically designed for interpretable multi-horizon time series forecasting. It combines the strengths of recurrent networks and attention mechanisms to handle both short-term and long-term dependencies. TFT is capable of integrating various types of input data, including static, known, and unknown future inputs. It features interpretable attention layers that allow users to understand the importance of different features and time steps, providing transparency in the forecasting process​ (GitHub)​​ (ar5iv)​.

Informer
The Informer model is designed to improve the efficiency and effectiveness of the Transformer architecture for long sequence time-series forecasting. It addresses the computational challenges of standard Transformers by introducing a ProbSparse self-attention mechanism that reduces the complexity from quadratic to linear. Informer also includes a generative style decoder that enhances its ability to handle long-range dependencies, making it suitable for large-scale time-series data. This model excels in applications requiring extensive forecasting horizons with improved computational efficiency​ (ar5iv)​​ (GitHub)​.

These summaries provide an overview of the unique characteristics and functionalities of each model, highlighting their specific strengths and use cases in the context of time series forecasting.

Can you show me a diagram for each
ChatGPT
Create diagrams for the following time series forecasting models:

1. S-Mamba: Show a model with layers such as Linear Tokenization Layer, Mamba inter-Variate correlation Encoding Layer, FFN Temporal Dependencies Encoding Layer, and a Projection Layer.

2. iTransformer: Illustrate a Transformer architecture adapted for time series with tokenization, self-attention, and feed-forward network layers.

3. ARIMA: Depict the ARIMA model with components like AR terms, differencing, and MA terms.

4. LSTM: Display an LSTM model with an input layer, LSTM cells, and a fully connected output layer.

5. TFT (Temporal Fusion Transformer): Show a model combining recurrent layers and attention mechanisms, with separate static, known, and unknown input handling.

6. Informer: Illustrate an Informer model with ProbSparse self-attention, generative style decoder, and efficient long sequence handling.

Ensure each diagram clearly labels the key components and shows the flow of data through the model.
Create diagrams for the following time series forecasting models:

1. S-Mamba: Show a model with layers such as Linear Tokenization Layer, Mamba inter-Variate correlation Encoding Layer, FFN Temporal Dependencies Encoding Layer, and a Projection Layer.

2. iTransformer: Illustrate a Transformer architecture adapted for time series with tokenization, self-attention, and feed-forward network layers.

3. ARIMA: Depict the ARIMA model with components like AR terms, differencing, and MA terms.

4. LSTM: Display an LSTM model with an input layer, LSTM cells, and a fully connected output layer.

5. TFT (Temporal Fusion Transformer): Show a model combining recurrent layers and attention mechanisms, with separate static, known, and unknown input handling.

6. Informer: Illustrate an Informer model with ProbSparse self-attention, generative style decoder, and efficient long sequence handling.

Ensure each diagram clearly labels the key components and shows the flow of data through the model.
Here are the diagrams for each of the time series forecasting models:

S-Mamba:

Layers include the Linear Tokenization Layer, Mamba inter-Variate correlation Encoding Layer, FFN Temporal Dependencies Encoding Layer, and a Projection Layer.
This model captures inter-variate correlations and temporal dependencies effectively.
iTransformer:

This architecture includes tokenization, self-attention, and feed-forward network layers.
It processes time series data similarly to how sequential text is handled in natural language processing.
ARIMA:

Components include AR terms (autoregressive), differencing (to make the data stationary), and MA terms (moving average).
It's used for univariate time series analysis.
LSTM:

Includes an input layer, multiple LSTM cells, and a fully connected output layer.
This model is effective for learning from sequences and handling long-term dependencies.
TFT (Temporal Fusion Transformer):

Combines recurrent layers and attention mechanisms with separate handling for static, known, and unknown inputs.
Designed for interpretable multi-horizon time series forecasting.
Informer:

Features ProbSparse self-attention, a generative style decoder, and efficient handling of long sequences.
It's designed to improve the efficiency and effectiveness of Transformer models for long sequence time-series forecasting.
The diagrams visually represent the architecture and data flow for each model, highlighting key components and their interactions.






Message ChatGPT

ChatGPT can make mistakes. Check important info.
/===
import ast

class DependencyVisitor(ast.NodeVisitor):
    def __init__(self):
        self.all_dependencies = {}
        self.current_function = None

    def visit_FunctionDef(self, node):
        self.current_function = node.name
        self.all_dependencies[self.current_function] = {'functions': [], 'variables': []}
        self.generic_visit(node)
        self.current_function = None

    def visit_Call(self, node):
        if self.current_function is not None:
            if isinstance(node.func, ast.Name):
                self.all_dependencies[self.current_function]['functions'].append(node.func.id)
            elif isinstance(node.func, ast.Attribute):
                self.all_dependencies[self.current_function]['functions'].append(node.func.attr)
        self.generic_visit(node)

    def visit_Name(self, node):
        if self.current_function is not None and isinstance(node.ctx, ast.Load):
            self.all_dependencies[self.current_function]['variables'].append(node.id)
        self.generic_visit(node)

def generate_dependency_graph(code):
    tree = ast.parse(code)
    visitor = DependencyVisitor()
    visitor.visit(tree)

    # Filter dependencies to only include "dp" functions and their dependencies
    filtered_dependencies = {}
    for func, deps in visitor.all_dependencies.items():
        if func.startswith("dp") or any(dep.startswith("dp") for dep in deps['functions']):
            filtered_dependencies[func] = {
                'functions': [f for f in deps['functions'] if f.startswith("dp")],
                'variables': deps['variables']
            }

    return filtered_dependencies

# Example usage
code = '''
def dp_foo():
    dp_bar()
    x = 10
    y = dp_baz(x)

def dp_bar():
    pass

def dp_baz(a):
    return a * 2

def foo():
    dp_bar()
    x = 10
    y = dp_baz(x)

def bar():
    pass

def baz(a):
    return a * 2
'''

dependencies = generate_dependency_graph(code)
for func, deps in dependencies.items():
    print(f"Function: {func}")
    print(f"  Calls: {deps['functions']}")
    print(f"  Variables: {deps['variables']}")

import ast

class DependencyVisitor(ast.NodeVisitor):
    def __init__(self):
        self.dependencies = {}
        self.current_function = None

    def visit_FunctionDef(self, node):
        if node.name.startswith("dp"):
            self.current_function = node.name
            self.dependencies[self.current_function] = {'functions': [], 'variables': []}
            self.generic_visit(node)
            self.current_function = None
        else:
            self.generic_visit(node)

    def visit_Call(self, node):
        if self.current_function is not None:
            if isinstance(node.func, ast.Name):
                self.dependencies[self.current_function]['functions'].append(node.func.id)
            elif isinstance(node.func, ast.Attribute):
                self.dependencies[self.current_function]['functions'].append(node.func.attr)
        self.generic_visit(node)

    def visit_Name(self, node):
        if self.current_function is not None and isinstance(node.ctx, ast.Load):
            self.dependencies[self.current_function]['variables'].append(node.id)
        self.generic_visit(node)

def generate_dependency_graph(code):
    tree = ast.parse(code)
    visitor = DependencyVisitor()
    visitor.visit(tree)
    return visitor.dependencies

# Example usage
code = '''
def dp_foo():
    dp_bar()
    x = 10
    y = dp_baz(x)

def dp_bar():
    pass

def dp_baz(a):
    return a * 2

def foo():
    bar()
    x = 10
    y = baz(x)

def bar():
    pass

def baz(a):
    return a * 2
'''

dependencies = generate_dependency_graph(code)
for func, deps in dependencies.items():
    print(f"Function: {func}")
    print(f"  Calls: {deps['functions']}")
    print(f"  Variables: {deps['variables']}")

/===
import ast

class DependencyVisitor(ast.NodeVisitor):
    def __init__(self):
        self.dependencies = {}
        self.current_function = None

    def visit_FunctionDef(self, node):
        self.current_function = node.name
        self.dependencies[self.current_function] = {'functions': [], 'variables': []}
        self.generic_visit(node)
        self.current_function = None

    def visit_Call(self, node):
        if self.current_function is not None:
            if isinstance(node.func, ast.Name):
                self.dependencies[self.current_function]['functions'].append(node.func.id)
            elif isinstance(node.func, ast.Attribute):
                self.dependencies[self.current_function]['functions'].append(node.func.attr)
        self.generic_visit(node)

    def visit_Name(self, node):
        if self.current_function is not None and isinstance(node.ctx, ast.Load):
            self.dependencies[self.current_function]['variables'].append(node.id)
        self.generic_visit(node)

def generate_dependency_graph(code):
    tree = ast.parse(code)
    visitor = DependencyVisitor()
    visitor.visit(tree)
    return visitor.dependencies

# Example usage
code = '''
def foo():
    bar()
    x = 10
    y = baz(x)

def bar():
    pass

def baz(a):
    return a * 2
'''

dependencies = generate_dependency_graph(code)
for func, deps in dependencies.items():
    print(f"Function: {func}")
    print(f"  Calls: {deps['functions']}")
    print(f"  Variables: {deps['variables']}")

/===

import re

text = "prefix content first text role middle content second text role suffix"
pattern = re.compile(r'(.*?)content(.*?)role')

# Start the initial search from the start of the text
matches = list(re.finditer(pattern, text))
results = []

for i in range(len(matches)):
    if i < len(matches) - 1:
        # Get the end of the current 'role'
        end_current_match = matches[i].end()
        # Start of the next 'content'
        start_next_match = matches[i+1].start()
        part1, part2 = matches[i].groups()
        # Part 3 is the text between the end of the current 'role' and the start of the next 'content'
        part3 = text[end_current_match:start_next_match]
    else:
        # For the last match, take everything after 'role'
        part1, part2 = matches[i].groups()
        part3 = text[matches[i].end():]

    results.append((part1, part2, part3))

# Print results
for part1, part2, part3 in results:
    print("Part 1:", part1)
    print("Part 2:", part2)
    print("Part 3:", part3)
    print("---")

# Example usage
json_str = '''
[
    {"name": "example1", "content": "This is a \\"test\\" string with \\"quotes\\"."},
    {"name": "example2", "content": "Another \\"example\\" here.", "nested": {"content": "Nested \\"content\\" with \\"quotes\\"."}},
    {"name": "example3", "content": "Quotes in a \\"list\\": [\\"item1\\", \\"item2\\"]}
]
'''
exceptions = ["name", "content"]

replaced_json_str = replace_single_word_quotes(json_str, exceptions)
print(replaced_json_str)
In this script, the replacement function checks if the matched word is in the exceptions list. If it is, the quotes around the word are not changed; otherwise, the double quotes are replaced with single quotes. The regular expression pattern remains the same, ensuring that only standalone words outside of lists are processed.












replaced_json_str = replace_quotes_in_content(json_str)
print(replaced_json_str)


===
import streamlit as st
import re
import pickle
from datetime import datetime

# Simulate the reply function
def replyFnc(user_input):
    # This is a mock function. Replace with actual logic.
    return {
        "reply": f"This is a reply with dpExampleWord and a link to [Google](https://www.google.com).",
        "relatedFunctions": [f"Related info for: {user_input}"],
        "example": f"Example related to: {user_input}",
        "description": f"Description related to: {user_input}"
    }

# Function to highlight specific words in a text
def highlight_words(text):
    pattern = r'\bdp[A-Z]\w*'
    highlighted_text = re.sub(pattern, lambda x: f'<span style="color:red;">{x.group()}</span>', text)
    return highlighted_text

# Initialize the session state to store conversation history and feedback
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

if 'feedback' not in st.session_state:
    st.session_state.feedback = []

# Adjust the layout to make the sidebar wider
st.set_page_config(layout="wide")

# Layout containers
sidebar_container = st.sidebar.container()
main_container = st.container()

# Sidebar for related info
with sidebar_container:
    st.header("Related Information")
    related_info_placeholder = st.empty()
    example_placeholder = st.empty()
    description_placeholder = st.empty()
    st.header("Feedback Collected")
    st.write(st.session_state.feedback)

# Main content
with main_container:
    st.title("Library Assistant")

    # Input form for the user to type their message
    with st.form(key='chat_form', clear_on_submit=True):
        user_input = st.text_input("You: ", "")
        submit_button = st.form_submit_button(label='Send')

    if submit_button and user_input:
        # Call the reply function
        response = replyFnc(user_input)
        # Update conversation history
        st.session_state.conversation.append({"user": user_input, "bot": response["reply"]})
        # Update related info in the sidebar
        related_info_placeholder.write(response["relatedFunctions"])
        example_placeholder.write(response["example"])
        description_placeholder.write(response["description"])

    # Feedback buttons and clear button
    if st.session_state.conversation:
        latest_idx = len(st.session_state.conversation) - 1
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            if st.button('👍', key=f"thumbsup_{latest_idx}"):
                st.session_state.feedback.append({"index": latest_idx, "feedback": "thumbsup"})
        with col2:
            if st.button('👎', key=f"thumbsdown_{latest_idx}"):
                st.session_state.feedback.append({"index": latest_idx, "feedback": "thumbsdown"})
        with col3:
            if st.button('❌', key=f"bad_{latest_idx}"):
                st.session_state.feedback.append({"index": latest_idx, "feedback": "bad"})
        with col4:
            if st.button('Clear Messages'):
                def clear_messages():
                    data_to_save = {
                        "conversation": st.session_state.conversation,
                        "feedback": st.session_state.feedback
                    }
                    with open(f"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl", 'wb') as f:
                        pickle.dump(data_to_save, f)
                    st.session_state.conversation = []
                    st.session_state.feedback = []

                clear_messages()

    # Display the conversation history with scrolling
    chat_container = st.container()
    with chat_container:
        for idx, chat in reversed(list(enumerate(st.session_state.conversation))):
            st.write(f"You: {chat['user']}")
            st.markdown(highlight_words(chat['bot']), unsafe_allow_html=True)


/===
import streamlit as st
import re

# Simulate the reply function
def replyFnc(user_input):
    # This is a mock function. Replace with actual logic.
    return {
        "reply": f"This is a reply with dpExampleWord and a link to [Google](https://www.google.com).",
        "relatedFunctions": [f"Related info for: {user_input}"],
        "example": f"Example related to: {user_input}",
        "description": f"Description related to: {user_input}"
    }

# Function to highlight specific words in a text
def highlight_words(text):
    pattern = r'\bdp[A-Z]\w*'
    highlighted_text = re.sub(pattern, lambda x: f'<span style="color:red;">{x.group()}</span>', text)
    return highlighted_text

# Initialize the session state to store conversation history and feedback
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

if 'feedback' not in st.session_state:
    st.session_state.feedback = []

# Sidebar for related info
st.sidebar.header("Related Information")
related_info_placeholder = st.sidebar.empty()
example_placeholder = st.sidebar.empty()
description_placeholder = st.sidebar.empty()

st.title("Library Assistant")

# Input form for the user to type their message
with st.form(key='chat_form', clear_on_submit=True):
    user_input = st.text_input("You: ", "")
    submit_button = st.form_submit_button(label='Send')

if submit_button and user_input:
    # Call the reply function
    response = replyFnc(user_input)
    # Update conversation history
    st.session_state.conversation.append({"user": user_input, "bot": response["reply"]})
    # Update related info in the sidebar
    related_info_placeholder.write(response["relatedFunctions"])
    example_placeholder.write(response["example"])
    description_placeholder.write(response["description"])

# Feedback buttons
if st.session_state.conversation:
    latest_idx = len(st.session_state.conversation) - 1
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button('👍', key=f"thumbsup_{latest_idx}"):
            st.session_state.feedback.append({"index": latest_idx, "feedback": "thumbsup"})
    with col2:
        if st.button('👎', key=f"thumbsdown_{latest_idx}"):
            st.session_state.feedback.append({"index": latest_idx, "feedback": "thumbsdown"})
    with col3:
        if st.button('❌', key=f"bad_{latest_idx}"):
            st.session_state.feedback.append({"index": latest_idx, "feedback": "bad"})

# Display the conversation history with scrolling
chat_container = st.container()
with chat_container:
    for idx, chat in reversed(list(enumerate(st.session_state.conversation))):
        st.write(f"You: {chat['user']}")
        st.markdown(highlight_words(chat['bot']), unsafe_allow_html=True)

# Optionally, display feedback collected for debugging
st.sidebar.header("Feedback Collected")
st.sidebar.write(st.session_state.feedback)
/===
from pyxlsb import open_workbook as open_xlsb
import openpyxl

def convert_xlsb_to_xlsx(xlsb_file_path, xlsx_file_path):
    wb_xlsx = openpyxl.Workbook()
    ws_xlsx = wb_xlsx.active

    with open_xlsb(xlsb_file_path) as wb:
        for sheetname in wb.sheets:
            ws_xlsx = wb_xlsx.create_sheet(title=sheetname)
            with wb.get_sheet(sheetname) as sheet:
                for row in sheet.rows():
                    ws_xlsx.append([item.v for item in row])

    # Remove the default sheet created by openpyxl
    if 'Sheet' in wb_xlsx.sheetnames:
        wb_xlsx.remove(wb_xlsx['Sheet'])

    wb_xlsx.save(xlsx_file_path)

# Example usage
xlsb_file_path = 'path_to_your_file.xlsb'
xlsx_file_path = 'converted_file.xlsx'
convert_xlsb_to_xlsx(xlsb_file_path, xlsx_file_path)



import openpyxl
import networkx as nx

def extract_dependency_graph(file_path):
    workbook = openpyxl.load_workbook(file_path, data_only=False)
    dependency_graph = nx.DiGraph()

    for sheet in workbook.sheetnames:
        worksheet = workbook[sheet]
        for row in worksheet.iter_rows():
            for cell in row:
                if cell.data_type == 'f':  # Check if the cell contains a formula
                    formula = cell.value
                    dependencies = extract_dependencies(formula, sheet)
                    for dep in dependencies:
                        dependency_graph.add_edge(dep, (sheet, cell.coordinate))

    return dependency_graph

def extract_dependencies(formula, current_sheet):
    # Extract cell references from the formula
    # This is a simplified example; you might need to handle more complex formulas
    dependencies = []
    tokens = formula.replace('=', '').replace('+', ' ').replace('-', ' ').replace('*', ' ').replace('/', ' ').split()
    for token in tokens:
        if '!' in token:
            sheet, cell = token.split('!')
            dependencies.append((sheet, cell))
        else:
            dependencies.append((current_sheet, token))
    return dependencies

# Example usage
file_path = 'path_to_your_excel_file.xlsx'
graph = extract_dependency_graph(file_path)

# Print the edges of the dependency graph
for edge in graph.edges:
    print(edge)

//
import ast

def parse_functions_and_dependencies_with_disk_reads(source_code):
    tree = ast.parse(source_code)
    functions = []
    dependencies = []
    disk_reads = []

    def extract_functions(node, parent_function=None):
        for sub_node in ast.iter_child_nodes(node):
            if isinstance(sub_node, ast.FunctionDef):
                func_name = sub_node.name
                if parent_function:
                    func_name = f"{parent_function}.{func_name}"
                args = [arg.arg for arg in sub_node.args.args]
                outputs = []
                for sub_sub_node in ast.walk(sub_node):
                    if isinstance(sub_sub_node, ast.Return):
                        if isinstance(sub_sub_node.value, ast.Tuple):
                            outputs.extend([elt.id for elt in sub_sub_node.value.elts if isinstance(elt, ast.Name)])
                        elif isinstance(sub_sub_node.value, ast.Name):
                            outputs.append(sub_sub_node.value.id)
                    if isinstance(sub_sub_node, ast.Call):
                        if isinstance(sub_sub_node.func, ast.Name):
                            dependencies.append((func_name, sub_sub_node.func.id))
                        elif isinstance(sub_sub_node.func, ast.Attribute):
                            dependencies.append((func_name, sub_sub_node.func.attr))
                functions.append({
                    'name': func_name,
                    'args': args,
                    'outputs': outputs,
                })
                extract_functions(sub_node, func_name)

    # Extract top-level functions
    extract_functions(tree)

    # Identify objects read from disk
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign):
            for target in node.targets:
                if isinstance(target, ast.Name):
                    if isinstance(node.value, ast.Call):
                        if hasattr(node.value.func, 'attr') and node.value.func.attr == 'read':
                            disk_reads.append(target.id)
                            break

    return functions, dependencies, disk_reads

# Example usage:
with open('your_script.py', 'r') as file:
    source_code = file.read()

functions, dependencies, disk_reads = parse_functions_and_dependencies_with_disk_reads(source_code)

/=====

from neo4j import GraphDatabase

class Neo4jHandler:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
    
    def close(self):
        self.driver.close()
    
    def create_function_node(self, function_name, args, outputs):
        with self.driver.session() as session:
            session.run(
                "MERGE (f:Function {name: $function_name}) "
                "SET f.args = $args, f.outputs = $outputs",
                function_name=function_name, args=args, outputs=outputs
            )
    
    def create_argument_node(self, argument_name):
        with self.driver.session() as session:
            session.run(
                "MERGE (a:Argument {name: $argument_name})",
                argument_name=argument_name
            )
    
    def create_relationship(self, from_node, to_node, relationship_type):
        with self.driver.session() as session:
            session.run(
                f"MATCH (a {{name: $from_node}}), (b {{name: $to_node}}) "
                f"MERGE (a)-[:{relationship_type}]->(b)",
                from_node=from_node, to_node=to_node
            )

# Example usage:
neo4j_handler = Neo4jHandler("bolt://localhost:7687", "neo4j", "password")

# Create function nodes and argument nodes
for func in functions:
    neo4j_handler.create_function_node(func['name'], func['args'], func['outputs'])
    for arg in func['args']:
        neo4j_handler.create_argument_node(arg)
    for output in func['outputs']:
        neo4j_handler.create_argument_node(output)

# Create dependency relationships
for dep in dependencies:
    neo4j_handler.create_relationship(dep[1], dep[0], "DEPENDS_ON")

# Create relationships between functions and their arguments
for func in functions:
    for arg in func['args']:
        neo4j_handler.create_relationship(arg, func['name'], "USED_BY")
    for output in func['outputs']:
        neo4j_handler.create_relationship(func['name'], output, "PRODUCES")

# Create nodes and relationships for objects read from disk
for disk_read in disk_reads:
    neo4j_handler.create_argument_node(disk_read)
    for func in functions:
        if disk_read in func['args']:
            neo4j_handler.create_relationship(disk_read, func['name'], "READS_FROM_DISK")
        if disk_read in func['outputs']:
            neo4j_handler.create_relationship(func['name'], disk_read, "PRODUCES")

neo4j_handler.close()

/===
# Example usage:
with open('your_script.py', 'r') as file:
    source_code = file.read()

functions, dependencies, disk_reads = parse_functions_and_dependencies_with_disk_reads(source_code)
/===
from neo4j import GraphDatabase

class Neo4jHandler:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
    
    def close(self):
        self.driver.close()
    
    def create_function_node(self, function_name, args, outputs):
        with self.driver.session() as session:
            session.run(
                "MERGE (f:Function {name: $function_name}) "
                "SET f.args = $args, f.outputs = $outputs",
                function_name=function_name, args=args, outputs=outputs
            )
    
    def create_argument_node(self, argument_name):
        with self.driver.session() as session:
            session.run(
                "MERGE (a:Argument {name: $argument_name})",
                argument_name=argument_name
            )
    
    def create_relationship(self, from_node, to_node, relationship_type):
        with self.driver.session() as session:
            session.run(
                f"MATCH (a {{name: $from_node}}), (b {{name: $to_node}}) "
                f"MERGE (a)-[:{relationship_type}]->(b)",
                from_node=from_node, to_node=to_node
            )

# Example usage:
neo4j_handler = Neo4jHandler("bolt://localhost:7687", "neo4j", "password")

# Create function nodes and argument nodes
for func in functions:
    neo4j_handler.create_function_node(func['name'], func['args'], func['outputs'])
    for arg in func['args']:
        neo4j_handler.create_argument_node(arg)
    for output in func['outputs']:
        neo4j_handler.create_argument_node(output)

# Create dependency relationships
for dep in dependencies:
    neo4j_handler.create_relationship(dep[1], dep[0], "DEPENDS_ON")

# Create relationships between functions and their arguments
for func in functions:
    for arg in func['args']:
        neo4j_handler.create_relationship(arg, func['name'], "USED_BY")
    for output in func['outputs']:
        neo4j_handler.create_relationship(func['name'], output, "PRODUCES")

# Create nodes and relationships for objects read from disk
for disk_read in disk_reads:
    neo4j_handler.create_argument_node(disk_read)
    for func in functions:
        if disk_read in func['args']:
            neo4j_handler.create_relationship(disk_read, func['name'], "READS_FROM_DISK")
        if disk_read in func['outputs']:
            neo4j_handler.create_relationship(func['name'], disk_read, "PRODUCES")

neo4j_handler.close()
/===
import ast

def parse_functions_and_dependencies_with_disk_reads(source_code):
    tree = ast.parse(source_code)
    functions = []
    dependencies = []
    disk_reads = []

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            func_name = node.name
            args = [arg.arg for arg in node.args.args]
            outputs = []
            for sub_node in ast.walk(node):
                if isinstance(sub_node, ast.Return):
                    if isinstance(sub_node.value, ast.Tuple):
                        outputs.extend([elt.id for elt in sub_node.value.elts if isinstance(elt, ast.Name)])
                    elif isinstance(sub_node.value, ast.Name):
                        outputs.append(sub_node.value.id)
            functions.append({
                'name': func_name,
                'args': args,
                'outputs': outputs,
            })
            for call in [n for n in ast.walk(node) if isinstance(n, ast.Call)]:
                if isinstance(call.func, ast.Name):
                    dependencies.append((func_name, call.func.id))

    # Identify objects read from disk
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign):
            for target in node.targets:
                if isinstance(target, ast.Name):
                    if isinstance(node.value, ast.Call):
                        if hasattr(node.value.func, 'attr') and node.value.func.attr == 'read':
                            disk_reads.append(target.id)
                            break

    return functions, dependencies, disk_reads

# Example usage:
with open('your_script.py', 'r') as file:
    source_code = file.read()

functions, dependencies, disk_reads = parse_functions_and_dependencies_with_disk_reads(source_code)
//
import ast

def parse_functions_with_dependencies(source_code):
    tree = ast.parse(source_code)
    functions = []
    dependencies = []

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            func_name = node.name
            args = [arg.arg for arg in node.args.args]
            outputs = []
            for sub_node in ast.walk(node):
                if isinstance(sub_node, ast.Return):
                    if isinstance(sub_node.value, ast.Tuple):
                        outputs.extend([elt.id for elt in sub_node.value.elts if isinstance(elt, ast.Name)])
                    elif isinstance(sub_node.value, ast.Name):
                        outputs.append(sub_node.value.id)
            functions.append({
                'name': func_name,
                'args': args,
                'outputs': outputs,
            })
            for call in [n for n in ast.walk(node) if isinstance(n, ast.Call)]:
                if isinstance(call.func, ast.Name):
                    dependencies.append((func_name, call.func.id))

    return functions, dependencies

# Example usage:
with open('your_script.py', 'r') as file:
    source_code = file.read()

functions, dependencies = parse_functions_with_dependencies(source_code)

/===

# Extend the parsing to capture outputs (assuming outputs are returned variables)
def parse_functions_with_outputs(source_code):
    tree = ast.parse(source_code)
    functions = []

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            output_vars = []
            for sub_node in ast.walk(node):
                if isinstance(sub_node, ast.Return):
                    if isinstance(sub_node.value, ast.Tuple):
                        output_vars.extend([elt.id for elt in sub_node.value.elts if isinstance(elt, ast.Name)])
                    elif isinstance(sub_node.value, ast.Name):
                        output_vars.append(sub_node.value.id)
            functions.append({
                'name': node.name,
                'args': [arg.arg for arg in node.args.args],
                'outputs': output_vars,
                'body': ast.get_source_segment(source_code, node)
            })
    
    return functions

# Example usage:
functions_with_outputs = parse_functions_with_outputs(source_code)

# Creating relationships for outputs used as inputs
for func in functions_with_outputs:
    for output in func['outputs']:
        neo4j_handler.create_argument_node(output)
        neo4j_handler.create_relationship(func['name'], output)

    for arg in func['args']:
        for producing_func in functions_with_outputs:
            if output in producing_func['outputs']:
                neo4j_handler.create_relationship(producing_func['name'], arg)

neo4j_handler.close()

/=====


def get_llama3_chat_template():
    return (
        "<|begin_of_text|>"
        "{% for message in messages %}"
            "{% if message.role == 'system' %}"
                "<|start_header_id|>system<|end_header_id|>"
                "{{message.content}}"
                "<|eot_id|>"
            "{% endif %}"
            "{% if message.role == 'user' %}"
                "<|start_header_id|>user<|end_header_id|>"
                "{{message.content}}"
                "<|eot_id|>"
            "{% endif %}"
            "{% if message.role == 'assistant' %}"
                "<|start_header_id|>assistant<|end_header_id|>"
                "{{message.content}}"
                "<|eot_id|>"
            "{% endif %}"
        "{% endfor %}"
        "<|end_of_text|>"
    )


text	<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are an AI assistant that provides information about the Calvin temperature scale.<|eot_id|><|start_header_id|>user<|end_header_id|>Can you provide a list of the classification ranges for the Calvin scale?<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Calvin scale classification ranges are: - You've taken that too far: -inf°Ca to -2.0°Ca - Bloomin' Freezin': -2.0°Ca to 0.0°Ca - Freezin': 0.0°Ca to 1.0°Ca - Bloomin' Cold: 1.0°Ca to 2.0°Ca - A bit Cold: 2.0°Ca to 3.0°Ca - A wee bit nippy: 3.0°Ca to 4.0°Ca - Alright: 4.0°Ca to 5.0°Ca - Getting a bit Lovely: 5.0°Ca to 6.0°Ca - Lovely: 6.0°Ca to 7.0°Ca - Nice and Warm: 7.0°Ca to 8.0°Ca - Getting a bit Hot: 8.0°Ca to 9.0°Ca - Hot: 9.0°Ca to 10.0°Ca - Toasty: 10.0°Ca to 11.0°Ca - Roasty Toasty: 11.0°Ca to 12.0°Ca - Ridiculous: 12.0°Ca to inf°Ca<|eot_id|><|end_of_text|>

config.json
{
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}

generation_config.json

{
  "_from_model_config": true,
  "bos_token_id": 128000,
  "eos_token_id": [128001, 128009],
  "transformers_version": "4.40.0.dev0"
}



from neo4j import GraphDatabase
import pandas as pd

# Example DataFrame
df = pd.DataFrame({
    'column1': ['value1', 'value2'],
    'column2': ['value3', 'value4']
})

# Convert DataFrame to a dictionary
data_as_dict = df.to_dict(orient='list')

# Additional properties for relationships
additional_properties = {
    'additionalProperty1': 'value',
    'additionalProperty2': 123
}

# Connect to Neo4j
uri = "neo4j://localhost:7687"  # Replace with your Neo4j instance URI
username = "neo4j"              # Replace with your username
password = "password"           # Replace with your password
driver = GraphDatabase.driver(uri, auth=(username, password))

def add_data_and_relationships(tx, data, additional_props):
    # Create node from DataFrame
    create_node_query = """
    CREATE (n:DataFrameNode {properties})
    RETURN id(n) AS nodeId
    """
    result = tx.run(create_node_query, properties=data)



node_id = result.single()["nodeId"]

    # Create relationships using additional properties
    for key, value in additional_props.items():
        create_rel_query = """
        MATCH (n) WHERE id(n) = $nodeId
        MATCH (m:OtherLabel) WHERE m.property = $propertyValue
        CREATE (n)-[:RELATES_TO {propertyKey: $propertyKey}]->(m)
        """
        tx.run(create_rel_query, nodeId=node_id, propertyValue=value, propertyKey=key)

# Upload data to Neo4j and create relationships
with driver.session() as session:
    session.write_transaction(add_data_and_relationships, data_as_dict, additional_properties)

driver.close()

/===
import re

def parse_string(input_str):
    # Regular expression to match patterns
    pattern = r'([A-Za-z]*\s*\d+\.\d+|\d+\.\d+|[A-Za-z]+)'

    # Find all matches
    matches = re.findall(pattern, input_str)

    # Post-process matches to handle special cases
    parsed = []
    for match in matches:
        # Remove unnecessary spaces and convert to float if it's a number
        cleaned_match = match.strip()
        try:
            # Convert to float if possible
            parsed.append(float(cleaned_match))
        except ValueError:
            # Keep as string if not a number
            parsed.append(cleaned_match)

    return parsed

# Test the function
print(parse_string("3.4/(5.6)")) # -> [3.4, 5.6]
print(parse_string("3.4(SOFR)")) # -> [3.4, "SOFR"]
print(parse_string("EUR 2.3/(USD 5.6)")) # -> ["EUR 2.3", "USD 5.6"]


model_name: str = "mistralai/Mixtral-8x7B-Instruct-v0.1"
api_key: str = hf_inference_api_key
context_window: int = 4096
num_output: int = 512

/===


import requests
from llama_index.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.llms.base import llm_completion_callback
from typing import Any


class MixtralLLM(CustomLLM):
    context_window: int = 4096
    num_output: int = 512
    model_name: str = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    api_key: str = hf_inference_api_key

    @property
    def metadata(self) -> LLMMetadata:
        """Get LLM metadata."""
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    def do_hf_call(self, prompt: str) -> str:
        data = {
            "inputs": prompt
        }

        response = requests.post(
            'https://api-inference.huggingface.co/models/' + self.model_name,
            headers={
                'authorization': f'Bearer {self.api_key}',
                'content-type': 'application/json',
            },
            json=data,
            stream=True
        )
        if response.status_code != 200 or not response.json() or 'error' in response.json():
            print(f"Error: {response}")
            return "Unable to answer for technical reasons."
        full_txt = response.json()[0]['generated_text']
        offset = full_txt.find("---------------------")
        ss = full_txt[offset:]
        offset = ss.find("Answer:")
        return ss[offset+7:].strip()

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        response = self.do_hf_call(prompt)
        return CompletionResponse(text=response)

    @llm_completion_callback()
    def stream_complete(
            self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        response = ""
        for token in self.do_hf_call(prompt):
            response += token
            yield CompletionResponse(text=response, delta=token)


mixtral_llm = MixtralLLM()

==
from llama_index.embeddings.jinaai import JinaEmbedding

jina_embedding_model = JinaEmbedding(
    api_key=jinaai_api_key,
    model="jina-embeddings-v2-base-en",
)

from llama_index import VectorStoreIndex, ServiceContext

service_context = ServiceContext.from_defaults(
    llm=mixtral_llm, embed_model=jina_embedding_model
)
index = VectorStoreIndex.from_documents(
    documents=docs, service_context=service_context
)


/===
def parse_10k_filing(link, section):

    if section not in [0, 1, 2, 3]:
        print("Not a valid section")
        sys.exit()

    def get_text(link):
        page = requests.get(link, headers={'User-Agent': 'Mozilla'})
        html = bs(page.content, "lxml")
        text = html.get_text()
        text = unicodedata.normalize("NFKD", text).encode('ascii', 'ignore').decode('utf8')
        text = text.split("\n")
        text = " ".join(text)
        return(text)

    def extract_text(text, item_start, item_end):
        item_start = item_start
        item_end = item_end
        starts = [i.start() for i in item_start.finditer(text)]
        ends = [i.start() for i in item_end.finditer(text)]
        positions = list()
        for s in starts:
            control = 0
            for e in ends:
                if control == 0:
    
if s < e:
                        control = 1
                        positions.append([s,e])
        item_length = 0
        item_position = list()
        for p in positions:
            if (p[1]-p[0]) > item_length:
                item_length = p[1]-p[0]
                item_position = p

        item_text = text[item_position[0]:item_position[1]]

        return(item_text)


    text = get_text(link)


    if section == 1 or section == 0:
        try:
            item1_start = re.compile("item\s*[1][\.\;\:\-\_]*\s*\\b", re.IGNORECASE)
            item1_end = re.compile("item\s*1a[\.\;\:\-\_]\s*Risk|item\s*2[\.\,\;\:\-\_]\s*Prop", re.IGNORECASE)
            businessText = extract_text(text, item1_start, item1_end)
        except:
            businessText = "Something went wrong!"

    if section == 2 or section == 0:
        try:
            item1a_start = re.compile("(?<!,\s)item\s*1a[\.\;\:\-\_]\s*Risk", re.IGNORECASE)
            item1a_end = re.compile("item\s*2[\.\;\:\-\_]\s*Prop|item\s*[1][\.\;\:\-\_]*\s*\\b", re.IGNORECASE)
            riskText = extract_text(text, item1a_start, item1a_end)
        except:
            riskText = "Something went wrong!"

    if section == 3 or section == 0:
        try:
            item7_start = re.compile("item\s*[7][\.\;\:\-\_]*\s*\\bM", re.IGNORECASE)
            item7_end = re.compile("item\s*7a[\.\;\:\-\_]\sQuanti|item\s*8[\.\,\;\:\-\_]\s*", re.IGNORECASE)
            mdaText = extract_text(text, item7_start, item7_end)
        except:
            mdaText = "Something went wrong!"

    if section == 0:
        data = [businessText, riskText, mdaText]
    elif section == 1:
        data = [businessText]
    elif section == 2:
        data = [riskText]
    elif section == 3:
        data = [mdaText]
    return(data)

/=====

import re

text = "Your text goes here. This is a sample text. First target is here. Another sentence. Second target found here. And yet another one."

# Define a list of target strings
targets = ["First", "Second", "Third"]

# Create a regex pattern that matches any of the target strings
# The pattern will look for:
# (1) a sentence ending before any target,
# (2) the sentence containing any target,
# (3) and the sentence following any target
pattern = r"(\S.+\.\s)?\S.*?({}).*?\.\s(\S.+\.)?".format("|".join(map(re.escape, targets)))

# Find and print all matches, case-insensitive
for match in re.finditer(pattern, text, re.IGNORECASE):
    print(match.group())
/====

import torch
from transformers import BitsAndBytesConfig
from llama_index.prompts import PromptTemplate
from llama_index.llms import HuggingFaceLLM

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)


llm = HuggingFaceLLM(
    model_name="mistralai/Mistral-7B-Instruct-v0.1",
    tokenizer_name="mistralai/Mistral-7B-Instruct-v0.1",
    query_wrapper_prompt=PromptTemplate("<s>[INST] {query_str} [/INST] </s>\n"),
    context_window=3900,
    max_new_tokens=256,
    model_kwargs={"quantization_config": quantization_config},
    # tokenizer_kwargs={},
    generate_kwargs={"temperature": 0.2, "top_k": 5, "top_p": 0.95},
    device_map="auto",
)

from llama_index import ServiceContext

from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings
from llama_index import ServiceContext

embed_model = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-base-en")

#service_context = ServiceContext.from_defaults(embed_model=embed_model,llm=llm)
service_context = ServiceContext.from_defaults(embed_model="local",llm=llm)

from llama_index import ServiceContext, set_global_service_context

set_global_service_context(service_context)

from llama_index.node_parser import (
    UnstructuredElementNodeParser,
)

node_parser = UnstructuredElementNodeParser(llm=llm)

raw_nodes_2021 = node_parser.get_nodes_from_documents(documents,llm=llm,embed_model="local")

base_nodes_2021, node_mappings_2021 = node_parser.get_base_nodes_and_mappings(
    raw_nodes_2021
)

/=====

dpo

def return_prompt_and_responses(samples) -> Dict[str, str, str]:
    return {
        "prompt": [
            "Question: " + question + "\n\nAnswer: "
            for question in samples["question"]
        ],
        "chosen": samples["response_j"],   # rated better than k
        "rejected": samples["response_k"], # rated worse than j
    }

dataset = load_dataset(
    "lvwerra/stack-exchange-paired",
    split="train",
    data_dir="data/rl"
)
original_columns = dataset.column_names

dataset.map(
    return_prompt_and_responses,
    batched=True,
    remove_columns=original_columns
)

dpo_trainer = DPOTrainer(
    model,                 # base model from SFT pipeline
    model_ref,             # typically a copy of the SFT trained base model
    beta=0.1,              # temperature hyperparameter of DPO
    train_dataset=dataset, # dataset prepared above
    tokenizer=tokenizer,   # tokenizer
    args=training_args,    # training arguments e.g. batch size, lr, etc.
)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,        # "meta-llama/Llama-2-7b-hf"
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True,
    use_auth_token=True,
)
base_model.config.use_cache = False

# add LoRA layers on top of the quantized base model
peft_config = LoraConfig(
    r=script_args.lora_r,
    lora_alpha=script_args.lora_alpha,
    lora_dropout=script_args.lora_dropout,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)
...
trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    packing=True,
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_args,         # HF Trainer arguments
)
trainer.train()

dpo_trainer.train()
dpo_trainer.save_model()
//newnew
from bs4 import BeautifulSoup, Tag
import json

def xml_to_dict(element):
    # Base case: if the element is a NavigableString, return its string content
    if not isinstance(element, Tag):
        return element.strip() if element and element.strip() else None

    # If the tag is empty, return None or its text
    if not element.attrs and not element.contents:
        return element.text.strip() if element.text and element.text.strip() else None

    # Create a dictionary to hold the element's attributes and children
    tag_dict = element.attrs.copy()

    # Add children to the dictionary
    for child in element.contents:
        if isinstance(child, Tag):
            child_result = xml_to_dict(child)
            # If a tag of the same name has been encountered before, create/update a list
            if child.name in tag_dict:
                if isinstance(tag_dict[child.name], list):
                    tag_dict[child.name].append(child_result)
                else:
                    tag_dict[child.name] = [tag_dict[child.name], child_result]
            else:
                tag_dict[child.name] = child_result
        else:  # Handle strings directly within the tag
            text = child.strip()
            if text:
                tag_dict['text'] = text

    return tag_dict

# Parse the XML content with BeautifulSoup
xml_content = """
... (your XML content) ...
"""
soup = BeautifulSoup(xml_content, 'xml')

# Find all 'derivativeinfo' elements
derivative_infos = soup.find_all('derivativeinfo')

# Convert each 'derivativeInfo' to a dictionary
json_objects = [xml_to_dict(derivative_info) for derivative_info in derivative_infos]

# If needed, convert dictionaries to JSON strings
json_strings = [json.dumps(obj, indent=4) for obj in json_objects]

# Now `json_strings` contains the JSON strings of the 'derivativeInfo' elements



///new

from bs4 import BeautifulSoup


# Parse your XML string with BeautifulSoup
xml_content = """
... (your XML content) ...
"""
soup = BeautifulSoup(xml_content, 'xml')

# Find all 'derivativeinfo' elements
derivative_infos = soup.find_all('derivativeinfo')

# Convert each 'derivativeInfo' to a dictionary
json_objects = [xml_to_dict(derivative_info) for derivative_info in derivative_infos]

# If needed, convert dictionaries to JSON strings
json_strings = [json.dumps(obj, indent=4) for obj in json_objects]

# Now `json_strings` contains the JSON strings of the 'derivativeInfo' elements


#import required libraries
import pandas as pd
import requests
from bs4 import BeautifulSoup
import json

url = 'https://www.sec.gov/Archives/edgar/data/1618627/000114554923004968/primary_doc.xml'
url = 'https://www.sec.gov/Archives/edgar/data/844779/000175272421060799/primary_doc.xml'
#set headers with a user-agent
headers = {"User-agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36"}    
req =  requests.get(url, headers=headers)

#define the columns you want to drop (based on the data in your question)
to_drop = ['identifiers', 'curCd','valUSD','isRestrictedSec','fairValLevel','debtSec','securityLending'] #maybe 

#the filing uses namespaces (too complicated to get into here), so you need to define that as well
namespaces = {"nport": "http://www.sec.gov/edgar/nport"}

#create the first df, for the securities which are debt instruments
invest = pd.read_xml(req.text,xpath="//nport:invstOrSec[.//nport:debtSec]",namespaces=namespaces).drop(to_drop, axis=1)

#crete the 2nd df, for the debt details:
debt = pd.read_xml(req.text,xpath="//nport:debtSec",namespaces=namespaces).iloc[:,0:3]

fwdDerivs = pd.read_xml(req.text,xpath="//nport:fwdDeriv",namespaces=namespaces) #don't think we use should use this
swapDerivs = pd.read_xml(req.text,xpath="//nport:swapDeriv",namespaces=namespaces) #don't think we use should use this

soup=BeautifulSoup(xml_data,parser='xml')
# Assuming `soup` is your BeautifulSoup object initialized with your XML content
invst_or_secs = soup.find_all('invstorsec')

# List to hold JSON objects
json_objects = []

# Iterate over each 'invstOrSec' element
for invst_or_sec in invst_or_secs:
    # Check if 'derivativeInfo' is present
    if invst_or_sec.find('derivativeinfo'):
        # Convert the 'invstOrSec' to a dictionary and then to a JSON string
        json_obj = json.dumps(xml_to_dict(invst_or_sec))
        json_objects.append(json_obj)



//counterparties
import xml.etree.ElementTree as ET
import json

def extract_all_counterparties_including_duplicates(xml_file_path):
    # Parse the XML file
    tree = ET.parse(xml_file_path)
    root = tree.getroot()

    # Define the namespace used in the XML file
    namespaces = {'nport': 'http://www.sec.gov/edgar/nport'}

    # Create a list to hold all counterparty names, including duplicates
    counterparty_names_list = []

    # Find all counterpartyName elements and add their text to the list
    for counterparty_name in root.findall('.//nport:counterpartyName', namespaces):
        counterparty_names_list.append(counterparty_name.text)

    return counterparty_names_list

def save_to_json(data, json_file_path):
    # Convert the list to JSON format
    data_json = json.dumps(data, indent=2)

    # Save the JSON to a file
    with open(json_file_path, 'w') as file:
        file.write(data_json)

# Usage example
xml_file_path = 'path_to_your_xml_file.xml'  # Replace with your XML file path
json_file_path = 'path_to_your_output_file.json'  # Replace with your desired output file path

# Extract all counterparty names including duplicates
all_counterparties = extract_all_counterparties_including_duplicates(xml_file_path)

# Save the list of all counterparties to a JSON file
save_to_json(all_counterparties, json_file_path)

# Now you have a JSON file with all the extracted counterparties


//n-port

import re
import pandas as pd
from xml.etree import ElementTree as ET

# Function to find the namespace
def find_namespace(root):
    return re.match(r'\{.*\}', root.tag).group(0)[:-1]

# Function to recursively search for elements containing keywords
def find_derivative_info(element, namespaces):
    derivatives_info = []
    if any(keyword in element.tag.lower() for keyword in derivative_keywords):
        derivatives_info.append(element)
    else:
        for attribute in element.attrib.values():
            if any(keyword in attribute.lower() for keyword in derivative_keywords):
                derivatives_info.append(element)
                break
    for child in element.findall('.//*', namespaces):
        derivatives_info.extend(find_derivative_info(child, namespaces))
    return derivatives_info

# Function to extract relevant data from a derivative element
def extract_derivative_data(element):
    derivative_data = {
        'Tag': element.tag,
        'Text': element.text,
        'Attributes': element.attrib
    }
    for child in element:
        child_data = extract_derivative_data(child)
        derivative_data[child.tag] = child_data
    return derivative_data

# Load and parse the XML file
def parse_xml_to_csv(xml_file_path, csv_file_path):
    # Load XML file
    with open(xml_file_path, 'r') as file:
        xml_content = file.read()

    # Parse the XML content
    tree = ET.ElementTree(ET.fromstring(xml_content))
    root = tree.getroot()
    namespaces = {'ns': find_namespace(root)}

    # Keywords to search for in the XML elements and attributes
    derivative_keywords = ['derivative', 'swap', 'option', 'future', 'forward', 'contract']

    # Find all elements related to derivatives
    derivatives_elements = find_derivative_info(root, namespaces)

    # Extract data from all found derivative elements
    derivatives_data = [extract_derivative_data(el) for el in derivatives_elements]

    # Create a DataFrame from the extracted data
    df_derivatives = pd.json_normalize(derivatives_data)

    # Save the DataFrame to a CSV file
    df_derivatives.to_csv(csv_file_path, index=False)

# File paths
xml_file_path = 'path_to_your_xml_file.xml'
csv_file_path = 'path_to_your_output_csv_file.csv'

# Parse XML and save to CSV
parse_xml_to_csv(xml_file_path, csv_file_path)
/==

use_flash_attention=True
model_name = "../meta-llama/Llama-2-70b-chat-hf" # use this if you have access to the official LLaMA 2 model "meta-llama/Llama-2-7b-chat-hf", though keep in mind you'll need to pass a Hugging Face key argument
dataset_name = "./train.jsonl"
new_model = "llama-2-70b-custom"
lora_r = 64
lora_alpha = 16
lora_dropout = 0.1
use_4bit = False #True
bnb_4bit_compute_dtype = "float16"
bnb_4bit_quant_type = "nf4"
use_nested_quant = False
output_dir = "./results"
num_train_epochs = 1
fp16 = True, #False
bf16 = True, #False
per_device_train_batch_size = 2
per_device_eval_batch_size = 2
gradient_accumulation_steps = 1
gradient_checkpointing = True
max_grad_norm = 0.3
learning_rate = 2e-4
weight_decay = 0.001
optim = "paged_adamw_32bit"
lr_scheduler_type = "constant"
max_steps = -1
warmup_ratio = 0.03
group_by_length = True
save_steps = 25
logging_steps = 5
max_seq_length = None
packing = False
device_map = "auto" #{"": 0}

# Load datasets
#train_dataset = load_dataset('json', data_files='./train.jsonl', split="train")
#valid_dataset = load_dataset('json', data_files='./test.jsonl', split="train")
system_message="You are an expert in neo4j and gds.  You will truthfuly answer the following question"
# Preprocess datasets
#train_dataset=train_df
#valid_dataset=test_df
#train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['question'], examples['answer'])]}, batched=True)
#valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['question'], examples['answer'])]}, batched=True)

#compute_dtype = getattr(torch, bnb_4bit_compute_dtype)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=False, #use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    use_cache=False,
    use_flash_attention_2=use_flash_attention,
#    quantization_config=bnb_config,
    device_map="auto" # device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)
# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    #fp16=fp16,
    bf16=bf16,
    tf32=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="all",
    evaluation_strategy="steps",
    disable_tqdm=True,
    eval_steps=5  # Evaluate every 20 steps
)
# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset_mapped,
    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=2048,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)
trainer.train(resume_from_checkpoint=True)
trainer.model.save_pretrained(new_model)

///

IEF

import pandas as pd

# Specify the path to the CSV file
file_path = '/mnt/data/IEF_holdings.csv'

# Read the 15 lines of data after the headers
ief_holdings_section_df = pd.read_csv(file_path, skiprows=9, header=0, nrows=15)

# (Optional) Display the extracted data to verify
print(ief_holdings_section_df)


///
pom.xml

<dependencies>
    <dependency>
        <groupId>org.neo4j.procedure</groupId>
        <artifactId>neo4j-procedure-api</artifactId>
        <version>your-neo4j-version</version>
        <scope>provided</scope>
    </dependency>
    <!-- Add any other dependencies, like the kdb+/q Java client -->
</dependencies>

----

package com.example;

import org.neo4j.procedure.Description;
import org.neo4j.procedure.Name;
import org.neo4j.procedure.UserFunction;

public class QFunction {

    @UserFunction
    @Description("com.example.callQ(query) - execute a q query and return result")
    public String callQ(@Name("query") String qQuery) {
        // Use the kdb+/q Java client to connect to your kdb+ instance
        // Execute the qQuery
        // Return the result or process as needed
        // For simplicity, I'm returning a string. You can adapt as necessary.
        
        // Example:
        String result = "Result from " + qQuery; // Replace this with actual kdb+/q call
        return result;
    }
}

mvn clean package

restart neo4j

RETURN com.example.callQ("your-q-query") AS result


====
import os
import re
import json

def extract_context(text, word, context_size=15000):
    matches = [match.start() for match in re.finditer(re.escape(word), text)]
    contexts = []

    for match in matches:
        start = max(0, match - context_size)
        end = min(len(text), match + context_size + len(word))
        contexts.append(text[start:end])

    return contexts

def find_files(base_dir, word1, word2):
    results = []

    for dirpath, dirnames, filenames in os.walk(base_dir):
        for filename in filenames:
            if word1 in filename:
                with open(os.path.join(dirpath, filename), 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    if word2 in content:
                        contexts = extract_context(content, word2)
                        for context in contexts:
                            results.append({
                                'pathToFile': os.path.join(dirpath, filename),
                                'context': context
                            })

    return results

base_dir = '/path/to/start/directory'
word1 = 'word1'
word2 = 'word2'

results = find_files(base_dir, word1, word2)

# Output the results to a JSON file
with open('output.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

////
import gradio as gr
import pandas as pd
import matplotlib.pyplot as plt

# Sample dataframe
# You'd usually read this from a file: df = pd.read_csv('your_data.csv')
data = {
    'date': ['2023-10-24', '2023-10-25', '2023-10-24', '2023-10-25'],
    'Symbol': ['AAPL', 'AAPL', 'GOOGL', 'GOOGL'],
    'Sector': ['Tech', 'Tech', 'Tech', 'Tech'],
    'Cusip': ['cusip1', 'cusip1', 'cusip2', 'cusip2'],
    'amount': [100, 110, 150, 140]
}
df = pd.DataFrame(data)

def plot_amount_change(portfolio, sector_selection):
    # Filter dataframe based on selection
    if sector_selection != 'All':
        filtered_df = df[df['Sector'] == sector_selection]
    else:
        filtered_df = df.copy()

    filtered_df = filtered_df[filtered_df['Symbol'] == portfolio]

    # Calculate change in amount
    filtered_df = filtered_df.sort_values(by='date')
    filtered_df['amount_change'] = filtered_df.groupby(['Cusip'])['amount'].diff().fillna(0)

    # Filter to only the latest date's changes
    latest_date = filtered_df['date'].max()
    filtered_df = filtered_df[filtered_df['date'] == latest_date]

    # Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    filtered_df.plot.bar(x='Cusip', y='amount_change', ax=ax)
    plt.title(f"Amount Change on {latest_date} for {portfolio} in {sector_selection} Sector")
    plt.ylabel("Change in Amount")
    plt.tight_layout()
    plt.close(fig)
    return fig

interface = gr.Interface(
    fn=plot_amount_change,
    inputs=[
        gr.inputs.Dropdown(choices=df['Symbol'].unique().tolist(), label="Portfolio"),
        gr.inputs.Dropdown(choices=['All'] + df['Sector'].unique().tolist(), label="Sector")
    ],
    outputs=gr.outputs.ImagePlot(label="Amount Change Barplot"),
    live=True
)
interface.launch()


------

import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

def calculate_change_in_amount_on_day(df):
  """Calculates the change in amount on day for the given dataframe.

  Args:
    df: A Pandas DataFrame with the columns 'date', 'portfolio', 'sector',
      'cusip', and 'amount'.

  Returns:
    A Pandas DataFrame with the columns 'date', 'portfolio', 'sector',
      'cusip', and 'amount_change'.
  """

  df['amount_change'] = df.groupby(['portfolio', 'sector', 'cusip'])['amount'].diff(1)
  return df

def generate_barplot(df, portfolio, sector):
  """Generates a barplot of the change in amount on day for the given dataframe,
  filtered by portfolio and sector.

  Args:
    df: A Pandas DataFrame with the columns 'date', 'portfolio', 'sector',
      'cusip', and 'amount_change'.
    portfolio: The portfolio to filter the dataframe by.
    sector: The sector to filter the dataframe by.

  Returns:
    A matplotlib Figure object.
  """

  fig, ax = plt.subplots()
  df_filtered = df[(df['portfolio'] == portfolio) & (df['sector'] == sector)]
  ax.bar(df_filtered['cusip'], df_filtered['amount_change'])
  ax.set_xlabel('CUSIP')
  ax.set_ylabel('Change in Amount on Day')
  ax.set_title('Change in Amount on Day by CUSIP for {} ({})'.format(portfolio, sector))
  return fig

st.title('Change in Amount on Day by CUSIP')

# Create a sidebar for the user to select the portfolio and sector.
st.sidebar.header('Portfolio and Sector Selection')
portfolio = st.sidebar.selectbox('Portfolio:', df['portfolio'].unique())
sector = st.sidebar.selectbox('Sector:', df['sector'].unique())

# Calculate the change in amount on day for the user selection.
df_filtered = calculate_change_in_amount_on_day(df[(df['portfolio'] == portfolio) & (df['sector'] == sector)])

# Generate the barplot.
fig = generate_barplot(df_filtered, portfolio, sector)

# Display the barplot.
st.pyplot(fig)


///

import requests
from bs4 import BeautifulSoup
import pandas as pd

def fetch_etf_data():
    # URL of the website to scrape data from
    url = 'https://etfdb.com/etfs/asset-class/bond/'
    
    # Send a GET request
    response = requests.get(url)
    response.raise_for_status()

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the table with the data
    table = soup.find('table', class_='table')

    # Extract the header and rows
    headers = [th.text for th in table.findAll('th')]
    rows = table.findAll('tr')[1:]

    # Extract the data for each row
    data = []
    for row in rows:
        cols = row.findAll('td')
        row_data = [col.text.strip() for col in cols]
        data.append(row_data)

    # Convert the data into a pandas dataframe
    df = pd.DataFrame(data, columns=headers)

    # Filter out the columns we're interested in
    desired_cols = [
        'Symbol',
        'ETF Name',
        'Total Assets ($MM)',
        'YTD',
        'Avg Volume',
        'Previous Closing Price',
        '1-Day Change'
    ]
    df = df[desired_cols]
    
    return df

df = fetch_etf_data()
print(df.head())  # Print the first 5 rows of the dataframe
/===




[{'question': 'Which movies have the actor $actor?',
  'Cypher': 'MATCH (a:Actor) WHERE toLower(a.name) CONTAINS toLower($actor) MATCH (a)-[:ACTED_IN]->(m:Movie) RETURN m.name',
  'schema': 'Actor(name),Movie(name)'},
 {'question': 'Which actors have won the most Academy Awards?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD]->(award:AcademyAward) RETURN a.name, COUNT(award) as count\nORDER BY count DESC',
  'schema': 'Actor(name),AcademyAward(name)'},
 {'question': 'Which movies are of the $genre genre and have a rating of at least 8?',
  'Cypher': 'MATCH (m:Movie) WHERE toLower(m.genre) CONTAINS toLower($genre) AND m.rating >= 8 RETURN m.name, m.rating',
  'schema': 'Movie(name, genre, rating)'},
 {'question': 'Which actors have won the most $awardType?',
  'Cypher': 'MATCH (a:Actor)-[r:WON]->(m:Movie) WHERE toLower(r.awardType) CONTAINS toLower($awardType) RETURN a.name, COUNT(m) AS totalAwards ORDER BY totalAwards DESC',
  'schema': 'Actor(name),Movie(title),Actor-[WON {awardType}]->Movie'},
 {'question': 'What are the top 10 movies by gross revenue?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.grossRevenue ORDER BY m.grossRevenue DESC LIMIT 10',
  'schema': 'Movie(name, grossRevenue)'},
 {'question': 'What are the top 5 movies by $attribute?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m[$attribute] ORDER BY m[$attribute] DESC LIMIT 5',
  'schema': 'Movie(name, totalGross)'},
 {'question': 'Which actors have won the most $awardCategory?',
  'Cypher': 'MATCH (a:Actor)-[r:WON_OSCAR]->(m:Movie) WHERE toLower(r.category) CONTAINS toLower($awardCategory) RETURN a.name, COUNT(m) AS num_oscars ORDER BY num_oscars DESC',
  'schema': 'Actor(name),Movie(name),Actor-[WON_OSCAR {category}]->Movie'},
 {'question': 'What are the top 10 movies by $attribute?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m[$attribute] ORDER BY m[$attribute] DESC LIMIT 10',
  'schema': 'Movie(name, boxOfficeRevenue)'},
 {'question': 'Which movies are in the $genre genre and have a rating of at least 8?',
  'Cypher': 'MATCH (m:Movie)-[:HAS_RATING]->(r:Rating) WHERE toLower(m.genre) CONTAINS toLower($genre) AND r.rating >= 8 RETURN m.name, r.rating',
  'schema': 'Movie(name,genre),Rating(rating)'},
 {'question': 'Which movies have the actor $actor?',
  'Cypher': 'MATCH (a:Actor) WHERE toLower(a.name) CONTAINS toLower($actor) MATCH (a)-[:ACTED_IN]->(m:Movie) RETURN m.name',
  'schema': 'Actor(name),Movie(name)'},
 {'question': 'Which actors have won the most $award?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD]->(award:AcademyAward) RETURN a.name, COUNT(award) as count ORDER BY count DESC',
  'schema': 'Actor(name),AcademyAward(name)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_OSCAR]->(m:Movie) RETURN a.name, count(m) as num_oscars ORDER BY num_oscars DESC',
  'schema': 'Actor(name),Movie(name)'},
 {'question': 'Which movies have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie) WHERE m.awards > 5 RETURN m.name, m.awards ORDER BY m.awards DESC',
  'schema': 'Movie(name, awards)'},
 {'question': 'Which movies are in the $genre genre and have a rating greater than $rating?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE]->(g:Genre) WHERE toLower(g.name) CONTAINS toLower($genre) AND m.rating > $rating RETURN m.name, m.rating, g.name',
  'schema': 'Movie(name, rating), Genre(name)'},
 {'question': 'Which movies have the same genre as the movie $movieTitle?',
  'Cypher': 'MATCH (m1:Movie {name: $movieTitle})-[:GENRE]->(g:Genre)-[:GENRE]-(m2:Movie) WHERE NOT m1.name = m2.name RETURN m2.name, m2.releaseYear',
  'schema': 'Movie(name, releaseYear), Genre(name)'},
 {'question': 'What are the movies that have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:AWARDED_TO]->(a:Award) WHERE toLower(a.type) CONTAINS toLower($awardType) RETURN m.name, collect(a.name) as awards ORDER BY size(awards) DESC',
  'schema': 'Movie(name),Award(name)'},
 {'question': 'Which actors have won an $awardName for their role in a movie?',
  'Cypher': 'MATCH (a:Actor)-[relation:ACTED_IN]->(m:Movie) WHERE toLower(relation.name) CONTAINS toLower($awardName) RETURN a.name, m.name',
  'schema': 'Movie(name), Actor(name), Actor-[ACTED_IN]->Movie'},
 {'question': 'What are the top 10 movies by $attribute?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.revenue ORDER BY m.revenue DESC LIMIT 10',
  'schema': 'Movie(name, revenue)'},
 {'question': 'Which movies have the same $genre?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE]->(g:Genre)<-[:GENRE]-(m2:Movie) WHERE NOT m.name = m2.name RETURN m.name, m2.name, g.name',
  'schema': 'Movie(name), Genre(name)'},
 {'question': 'Which actors have won the most $awardType?',
  'Cypher': 'MATCH (a:Actor)-[:WON]->(award:Award) WHERE toLower(award.name) CONTAINS toLower($awardType) RETURN a.name, COUNT(award) AS numAwards ORDER BY numAwards DESC',
  'schema': 'Actor(name),Movie(name),Actor-[:WON]->Award'},
 {'question': 'Which movies are in the $genre genre and have a rating of at least $rating?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE]->(g:Genre) WHERE toLower(g.name) CONTAINS toLower($genre) AND m.rating >= $rating RETURN m.name, m.rating, g.name',
  'schema': 'Movie(name, rating), Genre(name)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_OSCAR]->(m:Movie) WHERE toLower(m.type) CONTAINS toLower($awardName) RETURN a.name, COUNT(m) AS num_oscars ORDER BY num_oscars DESC',
  'schema': 'Actor(name),Movie(name)'},
 {'question': 'Which movies did $actorName star in?',
  'Cypher': 'MATCH (a:Actor)-[relation:ACTED_IN]->(m:Movie) WHERE toLower(a.name) CONTAINS toLower($actorName) RETURN m.title, a.name',
  'schema': 'Movie(title), Actor(name), Actor-[:ACTED_IN]->Movie'},
 {'question': 'Which movies are of the genre $genre and have a rating of at least $rating?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE]->(g:Genre) WHERE toLower(g.name) CONTAINS toLower($genre) AND m.rating > $rating RETURN m.name, m.rating',
  'schema': 'Movie(name,rating),Movie:Genre(name)'},
 {'question': 'Which movies are of the genre $genre and have an average rating of at least $avgRating?',
  'Cypher': 'MATCH (m:Movie) WHERE toLower(m.genre) CONTAINS toLower($genre) AND avg(m.rating) > $avgRating RETURN m.name, avg(m.rating)',
  'schema': 'Movie(name, rating, genre)'},
 {'question': 'Which movies have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:WON]->(award:Award) WHERE toLower(award.name) CONTAINS toLower($awardType) RETURN m.name, award.name, COUNT(award) AS total_awards',
  'schema': 'Movie(name),Award(name,award),Movie-[:WON]->Award'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD]->(o:Award) WHERE toLower(o.name) CONTAINS toLower($awardName) RETURN a.name, COUNT(o) AS award_count ORDER BY award_count DESC',
  'schema': 'Actor(name),Award(name)'},
 {'question': 'Which movies had an actor with last name $lastName in it?',
  'Cypher': 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) WHERE toLower(a.lastName) CONTAINS toLower($lastName) RETURN m.title, a.name',
  'schema': 'Movie(title),Actor(name,lastName)'},
 {'question': 'Which movies have the same genre as the movie $movieName?',
  'Cypher': 'MATCH (m:Movie)-[:SAME_GENRE*1..]->(g:Genre)-[:HAS_GENRE*1..]->(other_movies:Movie) WHERE toLower(m.name) CONTAINS toLower($movieName) RETURN other_movies.name, g.name',
  'schema': 'Movie(name, genre), Genre(name), Movie-[:SAME_GENRE]->Genre, Genre-[:HAS_GENRE]->Movie'},
 {'question': 'Which movies have the same director as the movie $movieName?',
  'Cypher': 'MATCH (m:Movie)-[:DIRECTED_BY]->(d:Director)-[:DIRECTED]->(m2:Movie) WHERE toLower(m.name) CONTAINS toLower($movieName) RETURN m2.name, d.name',
  'schema': 'Movie(name, directedBy), Director(name), Movie-[:DIRECTED_BY]->Director'},
 {'question': 'Which movies have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:WON]->(a:Award) WHERE toLower(a.name) CONTAINS toLower($awardType) RETURN m.name, COUNT(a) AS count',
  'schema': 'Movie(name),Award(name)'},
 {'question': 'Which actors have won the most $awardCategory?',
  'Cypher': 'MATCH (a:Actor)-[:WON_OSCAR {category: $awardCategory}]-(o:Oscar) RETURN a.name, COUNT(o) AS num_oscars',
  'schema': 'Actor(name),Oscar(name, category)'},
 {'question': 'Which movies have the same genres?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE]->(g:Genre)-[:GENRE]->(m2:Movie) RETURN m.name, m2.name, g.name',
  'schema': 'Movie(name), Genre(name), Movie-[:GENRE]->Genre, Genre-[:GENRE]->Movie'},
 {'question': 'What are the top $limitNumber most popular movies?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.release_year, count(*) as popularity ORDER BY popularity DESC LIMIT $limitNumber',
  'schema': 'Movie(name, release_year)'},
 {'question': 'Which actors were in the movies that had a budget over $budgetAmount?',
  'Cypher': 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) WHERE m.budget > $budgetAmount RETURN a.name,m.name,m.budget',
  'schema': 'Movie(name,budget),Actor(name),Actor-[ACTED_IN]->Movie'},
 {'question': 'Which movies are of the genre $genreName and have a rating of at least $minimumRating?',
  'Cypher': 'MATCH (m:Movie {genre: $genreName})-[:RATING]->(r:Rating) WHERE r.rating >= $minimumRating RETURN m.name, r.rating',
  'schema': 'Movie(name),Rating(rating)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_OSCAR { category: $awardCategory }]-(m:Movie) RETURN a.name, COUNT(m) AS num_oscars\nORDER BY num_oscars DESC',
  'schema': 'Actor(name),Movie(name),Actor-[:WON_OSCAR]->Movie'},
 {'question': 'Which movies have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:WON {award: $awardType}]-(a:Award) RETURN m.name, a.name, COUNT(a) AS total_awards',
  'schema': 'Movie(name),Award(name),Award-[WON]->Movie'},
 {'question': 'Which movies are of the genre $genreName and have a rating of at least $minRating?',
  'Cypher': 'MATCH (m:Movie {genre: $genreName, rating: {[minRating..]}})-[:DIRECTED]->(d:Director) RETURN m.title, d.name, m.rating',
  'schema': 'Movie(title, genre, rating), Director(name)'},
 {'question': 'Which actors have won the most $awardType?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD {type: $awardType}]-(m:Movie) RETURN a.name, count(m) as num_awards',
  'schema': 'Actor(name),Movie(name),Actor-[WON_AWARD {type: $awardType}]->Movie'},
 {'question': 'What are the top $numberLimit countries with the highest number of $prizeType winners?',
  'Cypher': 'MATCH (c:Country)-[:HAS_PRIZE_WINNER {category: $prizeCategory}]-(w:PrizeWinner) RETURN c.name, COUNT(w) AS count ORDER BY count DESC LIMIT $numberLimit',
  'schema': 'Country(name),PrizeWinner(name,category)'},
 {'question': 'Which movies have won the most $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:WON {award: $awardType}]-(a:Award) RETURN m.name, collect(a.name)',
  'schema': 'Movie(name),Award(name)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD]->(o:$awardEntity) RETURN a.name, COUNT(o) AS award_count ORDER BY award_count DESC',
  'schema': 'Actor(name),$awardEntity(name)'},
 {'question': 'What are the top $numberLimit movies by $metricName?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.$metricField ORDER BY m.$metricField DESC LIMIT $numberLimit',
  'schema': 'Movie(name, $metricField)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_AWARD {type: $awardType}]-(m:Movie) RETURN a.name, COUNT(m) AS num_award_wins',
  'schema': 'Actor(name),Movie(name)'},
 {'question': 'What are the top $numberLimit movies with the highest $metricName?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.$metricField ORDER BY m.$metricField DESC LIMIT $numberLimit',
  'schema': 'Movie(name, $metricField)'},
 {'question': 'What are the names of all the movies that have won $awardName for $awardType?',
  'Cypher': 'MATCH (m:Movie)-[:HAS_AWARD {type: $awardType}]-(a:$awardEntity) RETURN m.name',
  'schema': 'Movie(name),$awardEntity(type)'},
 {'question': 'Which movies have the same director and were released in the same year?',
  'Cypher': 'MATCH (m:Movie {releaseYear: $year})-[:DIRECTED_BY]->(d:Director {name: $directorName})-[:DIRECTED_BY]->(m2:Movie) RETURN m.name, m2.name',
  'schema': 'Movie(name, releaseYear), Director(name), Movie-[:DIRECTED_BY]->Director, Director-[:DIRECTED_BY]->Movie'},
 {'question': 'Which movies have won the most $awardName?',
  'Cypher': 'MATCH (m:Movie)-[:WON {award: $awardType}]-(a:Award) RETURN m.name, COUNT(a) AS award_count',
  'schema': 'Movie(name),Award(award)'},
 {'question': 'Which movies have $adjective actors?',
  'Cypher': 'MATCH (m:Movie)-[:ACTED_IN {$attribute: true}]-(a:Actor) RETURN m.name, a.name',
  'schema': 'Movie(name), Actor(name), Actor-[:ACTED_IN {$attribute: true}]->Movie'},
 {'question': 'Which movies have the same actor?',
  'Cypher': 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)<-[:ACTED_IN]-(b:Actor) RETURN a.name, b.name, m.name',
  'schema': 'Actor(name), Movie(name), Actor-[ACTED_IN]->Movie'},
 {'question': 'Which actors were in the movies with the highest total $metricName?',
  'Cypher': 'MATCH (a:Actor)-[:ACTED_IN {Movie.name: $movieName }]-(m:Movie) WHERE m.$metricAttribute > $thresholdValue RETURN a.name, m.name, m.$metricAttribute',
  'schema': 'Movie(name, $metricAttribute), Actor(name), Actor-[ACTED_IN]->Movie'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON {awardType: $awardType}]-(m:Movie) RETURN a.name, count(m) as numAwards',
  'schema': 'Actor(name),Movie(name),Actor-[WON]->Movie(awardType)'},
 {'question': 'Which movies have genres of $genreList and have a rating of at least $ratingThreshold?',
  'Cypher': 'MATCH (m:Movie)-[:GENRE {name: $genreName1 or name: $genreName2}]-(g:Genre) WHERE m.rating >= $ratingThreshold RETURN m.name, g.name, m.rating',
  'schema': 'Movie(name, rating), Genre(name)'},
 {'question': 'Which actors have won the most $awardName?',
  'Cypher': 'MATCH (a:Actor)-[:WON_OSCAR {category: $awardCategory}]-(o:$awardEntity) RETURN a.name, COUNT(o) AS num_wins',
  'schema': 'Actor(name),$awardEntity(category,name)'},
 {'question': 'Which movies are in the same genre as the movie with the title $movieTitle?',
  'Cypher': 'MATCH (m:Movie {title: $referenceMovieTitle})-[:SAME_GENRE]->(g:Genre)-[:SAME_GENRE]->(other_m:Movie) RETURN other_m.title, g.name',
  'schema': 'Movie(title), Genre(name), Movie-[:SAME_GENRE]->Genre, Genre-[:SAME_GENRE]->Movie'},
 {'question': 'What are the top $number movies by $metricName?',
  'Cypher': 'MATCH (m:Movie) RETURN m.name, m.$metricAttribute ORDER BY m.$metricAttribute DESC LIMIT $number',
  'schema': 'Movie(name, $metricAttribute)'},
 {'question': 'Which movies have won the most $awardName?',
  'Cypher': 'MATCH (m:Movie)-[:WON_AWARD]->(a:Award) RETURN m.name, a.name, count(a) as count ORDER BY count DESC',
  'schema': 'Movie(name),Award(name)'},
 {'question': 'Which $entityName have won the most $awardType?',
  'Cypher': "MATCH (a:$entity)-[:WON_OSCAR {role: '$roleType'}]-(o:$awardEntity) RETURN a.name, COUNT(o) as num_oscars",
  'schema': '$entity(name),$awardEntity(name,role)'},
 {'question': "Which movies are of the genre '$genreName' and have a rating of at least $ratingThreshold?",
  'Cypher': "MATCH (m:Movie {genre: '$genreType'})-[:RATING]->(r:Rating) WHERE r.rating >= $ratingThreshold RETURN m.name, r.rating",
  'schema': 'Movie(name, genre), Rating(rating)'},
 {'question': 'What are the most popular movie genres?',
  'Cypher': 'MATCH (m:Movie) RETURN m.genre, COUNT(*) as count ORDER BY count DESC',
  'schema': 'Movie(genre)'}]

///
import neo4j

def create_graph_in_neo4j_from_compressed_schema(compressed_schema):
  """Creates a graph in Neo4j from an arbitrary compressed schema.

  Args:
    compressed_schema: A string representing the compressed schema.

  Returns:
    None
  """

  # Connect to the Neo4j database.
  driver = neo4j.GraphDatabaseDriver("bolt://localhost:7687", auth=("neo4j", "neo4j"))

  # Create a session.
  session = driver.session()

  # Create the nodes.
  nodes, relationships = create_nodes_and_relationships_from_compressed_schema(
      compressed_schema)

  for node_type in nodes:
    for node in nodes[node_type]:
      node_query = "CREATE ({})".format(node_type)
      for attribute_name, attribute_value in node.items():
        node_query += f" {{{attribute_name}: '{attribute_value}'}}"
      session.run(node_query)

  # Create the relationships.
  for relationship in relationships:
    relationship_query = "CREATE ()-[:{}]->()".format(relationship["type"])
    relationship_query += f" {{source: '{relationship['source']}', target: '{relationship['target']}'}}"
    session.run(relationship_query)

  # Close the session.
  session.close()


if __name__ == "__main__":
  compressed_schema = convert_neo4j_schema_to_compressed_format(schema)

  create_graph_in_neo4j_from_compressed_schema(compressed_schema)


----
def convert_neo4j_schema_to_compressed_format(schema):
  """Converts a Neo4j schema to a compressed format.

  Args:
    schema: A Neo4j schema, represented as a dictionary.

  Returns:
    A string representing the compressed schema.
  """

  compressed_schema = ""
  for node_type in schema["nodes"]:
    compressed_schema += f"{node_type}{{{schema['attributes'][node_type]}}}"
    for relationship in schema["relationships"]:
      if relationship["sourceLabel"] == node_type:
        compressed_schema += f" - {relationship['type']} -> "
      elif relationship["targetLabel"] == node_type:
        compressed_schema += f" <- {relationship['type']} "
    compressed_schema += "\n"
  return compressed_schema


# Example usage:

schema = {
  "nodes": ["Person", "Movie", "Character"],
  "relationships": [
    {
      "type": "ACTED_IN",
      "sourceLabel": "Person",
      "targetLabel": "Movie"
    },
    {
      "type": "PLAYED",
      "sourceLabel": "Person",
      "targetLabel": "Character"
    },
    {
      "type": "APPEARS_IN",
      "sourceLabel": "Character",
      "targetLabel": "Movie"
    }
  ],
  "attributes": {
    "Person": ["name"],
    "Movie": ["title", "release_year"],
    "Character": ["name"]
  }
}

compressed_schema = convert_neo4j_schema_to_compressed_format(schema)

print(compressed_schema)

/====
from neo4j import GraphDatabase

class Neo4jService(object):
    def __init__(self, uri, user, password):
        self._driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self._driver.close()

    def insert_data_from_dataframe(self, df):
        with self._driver.session() as session:
            for index, row in df.iterrows():
                session.run(
                    """
                    MERGE (c:managerName {name: $managerName})
                    MERGE (p:companyName {name: $companyName})
                    MERGE (cu:cusip {name: $cusip})
                    MERGE (c)-[r:MANAGE {date: $reportCalendarOrQuarter}]->(p)
                    MERGE (p)-[r2:WITH_CUSIP]->(cu)
                    SET r.value = $value
                    """,
                    managerName=row['managerName'],
                    companyName=row['companyName'],
                    reportCalendarOrQuarter=row['reportCalendarOrQuarter'],
                    value=row['value'],
                    cusip=row['cusip']
                )

# Connect to Neo4j and insert data
uri = NEO4J_URI
user = "neo4j"
password = NEO4J_PASSWORD
neo4j_service = Neo4jService(uri, user, password)
neo4j_service.insert_data_from_dataframe(df)
neo4j_service.close()

///



parsing data

graph2 = Neo4jGraph(
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD
)


from neo4j import GraphDatabase

class Neo4jService(object):
    def __init__(self, uri, user, password):
        self._driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self._driver.close()

    def insert_data(self, list_of_dicts):
        with self._driver.session() as session:
            for record in list_of_dicts:
                session.run(
                    """
                    CREATE (n:NodeType)
                    SET n += $properties
                    """,
                    properties=record
                )

# Connect to Neo4j and insert data
uri = NEO4J_URI
user = "neo4j"
password = NEO4J_PASSWORD
neo4j_service = Neo4jService(uri, user, password)
neo4j_service.insert_data(reslist)


neo4j_service.close()

//batches

def insert_in_batches(df, batch_size):
    num_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)
    
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = start_index + batch_size
        batch = df[start_index:end_index]
        
        neo4j_service.insert_data_from_dataframe(batch)
        print(f"Inserted batch {i + 1}/{num_batches}")

///

neo4j_service = Neo4jService(uri, user, password)

# Define your batch size
batch_size = 1000
insert_in_batches(df, batch_size)

neo4j_service.close()

////


def chunks(xs, n=10_000):
    n = max(1, n)
    return [xs[i:i + n] for i in range(0, len(xs), n)]

for record in chunks(reslist):
  gds.query('''
  UNWIND $records AS record
  MERGE (c:Company {cusip: record.cusip})
  SET c.companyName = record.companyName
  RETURN count(c) AS company_node_merge_count
  ''', params={'records':record})

for d in chunks(reslist):
    res = gds.query('''
    UNWIND $records AS record
    MATCH (m:Manager {managerName: record.managerName})
    MATCH (c:Company {cusip: record.cusip})
    MERGE(m)-[r:OWNS]->(c)
    SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,
        r.value = record.value,
        r.shares = record.shares
    RETURN count(r) AS owns_relationship_merge_count
    ''', params={'records':d})
    print(res)

/===
!pip install edgartools
from edgar import *
filings = get_filings(form="13F-HR")
filing = filings[0]
thirteenf = filing.obj()
test=thirteenf.infotable_xml
thirteenf.parse_infotable_xml(test) #this is pandas dataframe


Given a table of contents with sections, items, descriptions, and page numbers, parse the information into a structured JSON format. The JSON should have keys for "item", "description", and "pages". If a section or item does not have a page number, it should be represented as an empty list. If a page number is labeled as "Not Applicable", it should be represented as ["Not Applicable"].

```
###Input
Part I	
1.		Business	1–23
1A.		Risk Factors	41–54
1B.		Unresolved Staff Comments	Not Applicable
2.		Properties	Not Applicable
###Output
[
  {
    "part": "Part I",
    "entries": [
      {
        "item": "1.",
        "description": "Business",
        "pages": ["1–23"]
      },
      {
        "item": "1A.",
        "description": "Risk Factors",
        "pages": ["41–54"]
      },
      {
        "item": "1B.",
        "description": "Unresolved Staff Comments",
        "pages": ["Not Applicable"]
      },
      {
        "item": "2.",
        "description": "Properties",
        "pages": ["Not Applicable"]
      }
    ]
  }
]

/==

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

model_name = "TinyPixel/Llama-2-7B-bf16-sharded"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

from peft import LoraConfig, get_peft_model

lora_alpha = 16
lora_dropout = 0.1
lora_r = 64

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM"
)

from transformers import TrainingArguments

output_dir = "./results"
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
optim = "paged_adamw_32bit"
save_steps = 100
logging_steps = 10
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 100
warmup_ratio = 0.03
lr_scheduler_type = "constant"

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
)

from trl import SFTTrainer

max_seq_length = 512

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

for name, module in trainer.model.named_modules():
    if "norm" in name:
        module = module.to(torch.float32)

trainer.train()

model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training
model_to_save.save_pretrained("outputs")

model = AutoModelForCausalLM.from_pretrained(
    model_name,
#    quantization_config=bnb_config,
    trust_remote_code=True
)
lora_config = LoraConfig.from_pretrained('outputs')
model = get_peft_model(model, lora_config)
model_merged=model.merge_and_unload()
model_merged.save_pretrained("test_merge")

tokenizer.save_pretrained("test_merge")





/===

from neo4j import GraphDatabase

class Neo4jService:

    def __init__(self, uri, user, password):
        self._driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self._driver.close()

    def add_function_and_relationships(self, function_name, related_functions, text_string):
        with self._driver.session() as session:
            session.write_transaction(self._create_or_update_function, function_name, text_string)
            for related_function in related_functions:
                session.write_transaction(self._create_or_update_relationship, function_name, related_function)

    @staticmethod
    def _create_or_update_function(tx, function_name, text_string):
        # Create or merge the function node and add the code as a new example
        query = """
        MERGE (f:Code {name: $function_name})
        ON CREATE SET f.examples = [$text_string]
        ON MATCH SET f.examples = f.examples + $text_string
        """
        tx.run(query, function_name=function_name, text_string=text_string)

    @staticmethod
    def _create_or_update_relationship(tx, function_name, related_function):
        # Create or merge the related function node and create the 'tests' relationship
        query = """
        MERGE (f1:Code {name: $function_name})
        MERGE (f2:Code {name: $related_function})
        MERGE (f1)-[:tests]->(f2)
        """
        tx.run(query, function_name=function_name, related_function=related_function)

# Connect to Neo4j (replace with your connection details)
neo4j_service = Neo4jService("bolt://localhost:7687", "neo4j", "your_password")

# Add function, code, and relationships
function_name = "myFunction"
related_functions = ["relatedFunc1", "relatedFunc2"]
text_string = "Example code for myFunction"
neo4j_service.add_function_and_relationships(function_name, related_functions, text_string)

neo4j_service.close()

/===
content = raw_10k
soup = BeautifulSoup(content, 'lxml')
contexts = soup.find_all('xbrli:context')
dimensions=[]
values=[]
identifiers=[]
context_ids=[]
for context in contexts:
    # Extract the identifier and period
    identifier = context.find('xbrli:identifier').text
    period = context.find('xbrli:period').text
    
    # Extract member data
    members = context.find_all('xbrldi:explicitmember')
    for member in members:
        dimension = member.attrs.get('dimension')
        value = member.text
        dimensions.append(dimension)
        values.append(value)
        identifiers.append(identifier)
   #     print(f"Identifier: {identifier}, Period: {period}, Dimension: {dimension}, Value: {value}")
        if (dimension == 'us-gaap:FinancialInstrumentAxis') and (value == 'us-gaap:USGovernmentAgenciesDebtSecuritiesMember'):
          print(f"{value} {member.text}")
          context_id = context.attrs.get('id')
          context_ids.append(context_id)

/=====
lauch.slurm
#!/bin/bash
#SBATCH --job-name=ift_llama
#SBATCH --nodes=8
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=96
#SBATCH --mem-per-cpu=11G # Important to enable "mix" use of GPUs across cluster users
#SBATCH --partition=XXXXX
#SBATCH --gres=gpu:8 # Adjust number of GPUs here
#SBATCH --output=shared_storage/sourab/temp/logs/%x-%j.out
#SBATCH --err=shared_storage/sourab/temp/logs/%x-%j.err

set -x -e

# CHANGE HERE THE CONDA EVN AND ANY STARTUP SCRIPTS
source ~/sourab/.bashrc
source shared_storage/sourab/miniconda3/etc/profile.d/conda.sh
conda activate hf
cd shared_storage/sourab/DHS-LLM-Workshop/code_assistant/training
git pull

# have the below in case of debugging nccl issues such as nccl timeout.
# export NCCL_DEBUG=INFO
# export NCCL_DEBUG_SUBSYS=ALL
# export TORCH_DISTRIBUTED_DEBUG=INFO
# hide duplicated errors using this hack - will be properly fixed in pt-1.12
# export TORCHELASTIC_ERROR_FILE=/tmp/torch-elastic-error.json

# force crashing on nccl issues like hanging broadcast
export NCCL_ASYNC_ERROR_HANDLING=1
# export NCCL_DEBUG=INFO
# export NCCL_DEBUG_SUBSYS=COLL
# export NCCL_SOCKET_NTHREADS=1
# export NCCL_NSOCKS_PERTHREAD=1
# export CUDA_LAUNCH_BLOCKING=1

# AWS specific
export NCCL_PROTO=simple
export RDMAV_FORK_SAFE=1
export FI_EFA_FORK_SAFE=1
export FI_EFA_USE_DEVICE_RDMA=1
export FI_PROVIDER=efa
export FI_LOG_LEVEL=1
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=ens

echo "START TIME: $(date)"

# CHANGE TO CUMMULATIVELY LOG OUTPUTS
LOG_PATH="main_log.txt"

GPUS_PER_NODE=8
NNODES=$SLURM_NNODES
NUM_PROCESSES=$(expr $NNODES \* $GPUS_PER_NODE)

# so processes know who to talk to
MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_PORT=6000

# OTHER LAUNCHERS CAN BE USED HERE
export LAUNCHER="accelerate launch \
    --config_file configs/fsdp_config.yaml \
    --main_process_ip $MASTER_ADDR \
    --main_process_port $MASTER_PORT \
    --machine_rank \$SLURM_PROCID \
    --num_processes $NUM_PROCESSES \
    --num_machines $NNODES \
    "
# Note: it is important to escape `$SLURM_PROCID` since we want the srun on each node to evaluate this variable

export PROGRAM="\
train.py \
    --model_name "meta-llama/Llama-2-70b-chat-hf" \
    --dataset_name "smangrul/code-chat-assistant-v1" \
    --max_seq_len 2048 \
    --max_steps 500 \
    --logging_steps 25 \
    --eval_steps 100 \
    --save_steps 250 \
    --bf16 True \
    --packing True \
    --output_dir "/shared_storage/sourab/experiments/full-finetune-llama-chat-asst" \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --dataset_text_field "content" \
    --use_gradient_checkpointing True \
    --learning_rate 5e-5  \
    --lr_scheduler_type "cosine" \
    --weight_decay 0.01 \
    --warmup_ratio 0.03 \
    --use_flash_attn True
"


export CMD="$LAUNCHER $PROGRAM"

srun --jobid $SLURM_JOBID bash -c "$CMD" 2>&1 | tee -a $LOG_PATH

echo "END TIME: $(date)"

///

pos_dat = test_df.groupby('item').nth(1).reset_index()

items = [
    {"name": "item1", "entries": [{"start": 1, "end": 5}, {"start": 2, "end": 6}]},
    {"name": "item2", "entries": [{"start": 7, "end": 10}]},
    {"name": "item3", "entries": [{"start": 6, "end": 8}, {"start": 9, "end": 10}, {"start": 11, "end": 12}]},
    {"name": "item4", "entries": [{"start": 12, "end": 15}]},
    {"name": "item5", "entries": [{"start": 13, "end": 15}, {"start": 17, "end": 19}]},
]

# Sort the items based on the start value of their first entry
sorted_items = sorted(items, key=lambda x: x["entries"][0]["start"])

# Find the valid entry for each item based on the condition
valid_entries = {}
for i in range(len(sorted_items) - 1):
    valid_entry = None
    for entry in reversed(sorted_items[i]["entries"]):
        if entry["end"] <= sorted_items[i + 1]["entries"][0]["start"]:
            valid_entry = entry
            break
    if valid_entry:
        valid_entries[sorted_items[i]["name"]] = valid_entry

# For the last item, just take its last entry
valid_entries[sorted_items[-1]["name"]] = sorted_items[-1]["entries"][-1]

print(valid_entries)



# ... Your Streamlit code ...

# Add a disclaimer at the bottom
disclaimer = """
    <style>
        .footer {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 100%;
            background-color: #f1f1f1;
            text-align: center;
            padding: 10px;
        }
    </style>
    <div class="footer">
        This is a sample disclaimer. Please consult with a professional before making any decisions based on this app.
    </div>
"""

st.markdown(disclaimer, unsafe_allow_html=True)



import re
import pandas as pd
from bs4 import BeautifulSoup
import regex
import requests

def get10k(ticker):
  #edgartools
  company=Company(ticker)
  filings=company.get_filings(form="10-K")
  urltext=filings[0].text_url
  r = requests.get(urltext,headers = {'user-agent': 'my-app/0.0.1'})

  #https://gist.github.com/anshoomehra
  raw_10k = r.text

  doc_start_pattern = re.compile(r'<DOCUMENT>')
  doc_end_pattern = re.compile(r'</DOCUMENT>')
  type_pattern = re.compile(r'<TYPE>[^\n]+')
  document = {}

  doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]
  doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]

  doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]

  for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):
      if doc_type == '10-K':
          document[doc_type] = raw_10k[doc_start:doc_end]
  regex = re.compile(r'(>Item(\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\.{0,1})|(ITEM\s(1A|1B|7A|7|8))')
  matches = regex.finditer(document['10-K'])

  test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])

  test_df.columns = ['item', 'start', 'end']
  test_df['item'] = test_df.item.str.lower()
  test_df.replace('&#160;',' ',regex=True,inplace=True)
  test_df.replace('&nbsp;',' ',regex=True,inplace=True)
  test_df.replace(' ','',regex=True,inplace=True)
  test_df.replace('\.','',regex=True,inplace=True)
  test_df.replace('>','',regex=True,inplace=True)
  pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')
  pos_dat.set_index('item',inplace=True)
  item_1a_raw = document['10-K'][pos_dat['start'].loc['item1a']:pos_dat['start'].loc['item1b']]

  item_7_raw = document['10-K'][pos_dat['start'].loc['item7']:pos_dat['start'].loc['item7a']]

  item_7a_raw = document['10-K'][pos_dat['start'].loc['item7a']:pos_dat['start'].loc['item8']]
  item_1a_content = BeautifulSoup(item_1a_raw, 'lxml').get_text("\n\n")
  item_7_content = BeautifulSoup(item_7_raw, 'lxml').get_text("\n\n")
  item_7a_content = BeautifulSoup(item_7a_raw, 'lxml').get_text("\n\n")
  return([item_1a_content,item_7_content,item_7a_content])
/====
# streamlit_app.py

import streamlit as st
import feedparser
import pandas as pd
import numpy as np

st.set_page_config(
    page_title="Company Dashboard",
    layout="wide",
    initial_sidebar_state="collapsed",
    page_icon=":bar_chart:"
)

def get_rss_feed():
    # Using a placeholder RSS feed (BBC World)
    feed_url = "http://feeds.bbci.co.uk/news/world/rss.xml"
    feed = feedparser.parse(feed_url)
    entries = feed.entries[:5]
    return entries

def display_feed(col):
    entries = get_rss_feed()
    for entry in entries:
        col.write(f"**{entry.title}**")
        col.write(entry.summary)
        col.write("---")

def display_company_info(company, col):
    # Dummy data for demonstration
    if company == "company1":
        data = {
            "Employee": ["Alice", "Bob", "Charlie"],
            "Role": ["Manager", "Engineer", "Analyst"]
        }
        df = pd.DataFrame(data)
        col.write(df)
    elif company == "company2":
        data = {
            "Product": ["Widget A", "Widget B"],
            "Price": [100, 150]
        }
        df = pd.DataFrame(data)
        col.write(df)
    elif company == "company3":
        data = {
            "Client": ["Client X", "Client Y", "Client Z"],
            "Revenue": [5000, 6000, 7000]
        }
        df = pd.DataFrame(data)
        col.write(df)
    else:
        col.write("Select a company from the dropdown to view details.")

def display_line_graph(col):
    # Generating some dummy data for the line graph
    dates = pd.date_range("20210101", periods=100)
    values = np.random.randn(100).cumsum()

    df_graph = pd.DataFrame({"Date": dates, "Value": values})
    
    col.line_chart(df_graph.set_index("Date"), use_container_width=True)

def main():
    st.title("Company Dashboard")

    # Dropdown menu for company selection
    company = st.selectbox("Select a company:", ["", "company1", "company2", "company3"])

    # Two column layout
    col1, col2 = st.beta_columns(2)

    # Display company info in the first column
    display_company_info(company, col1)
    
    # Display line graph in the first column below company info
    col1.subheader("Line Graph")
    display_line_graph(col1)

    # RSS Feed box in the second column
    col2.subheader("Latest News")
    display_feed(col2)

if __name__ == "__main__":
    main()

/===
~/.streamlit/config.toml
[theme]
primaryColor = "#f63366"
backgroundColor = "#1f1f1f"
secondaryBackgroundColor = "#31333F"
textColor = "#fafafa"
font = "sans-serif"

/===
import gradio as gr
import pandas as pd

# Sample DataFrames for demonstration
df1 = pd.DataFrame({
    'A': ['A0', 'A1', 'A2', 'A3'],
    'B': ['B0', 'B1', 'B2', 'B3'],
    'C': ['C0', 'C1', 'C2', 'C3'],
})

df2 = pd.DataFrame({
    'D': ['D0', 'D1', 'D2', 'D3'],
    'E': ['E0', 'E1', 'E2', 'E3'],
    'F': ['F0', 'F1', 'F2', 'F3'],
})

df3 = pd.DataFrame({
    'G': ['G0', 'G1', 'G2', 'G3'],
    'H': ['H0', 'H1', 'H2', 'H3'],
    'I': ['I0', 'I1', 'I2', 'I3'],
})

def display_subsets(choice):
    if choice == "Subset 1":
        return df1.head(2), df2.head(2), df3.head(2)
    elif choice == "Subset 2":
        return df1.tail(2), df2.tail(2), df3.tail(2)
    else:
        return df1, df2, df3

# Define Gradio interface with layout parameter
iface = gr.Interface(
    fn=display_subsets,
    inputs=gr.inputs.Dropdown(choices=["Subset 1", "Subset 2", "Full Data"], label="Choose a Subset"),
    outputs=[
        gr.outputs.Dataframe(type="pandas", label="DataFrame 1"),
        gr.outputs.Dataframe(type="pandas", label="DataFrame 2"),
        gr.outputs.Dataframe(type="pandas", label="DataFrame 3")
    ],
    layout="vertical"  # This positions the outputs below the input
)

iface.launch()

/====
import feedparser
import requests
from bs4 import BeautifulSoup

def parse_edgar_rss(url):
    feed = feedparser.parse(url)
    entries = []

    for entry in feed.entries:
        data = {
            "title": entry.title,
            "link": entry.link,
            "summary": entry.summary,
            "cik": entry.edgar_ciknumber,
            "full_text": get_full_text(entry.link)
        }
        entries.append(data)

    return entries

def get_full_text(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.content, 'html.parser')
    # Extracting the full text can be tricky as the structure might vary. 
    # Here, we're just getting all the text, but you might need to adjust this.
    return soup.get_text()

# Example Usage
rss_url = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&CIK=&type=424B&company=&dateb=&owner=include&start=0&count=40&output=atom"
entries = parse_edgar_rss(rss_url)

for entry in entries:
    print(f"Title: {entry['title']}")
    print(f"Link: {entry['link']}")
    print(f"Summary: {entry['summary']}")
    print(f"CIK: {entry['cik']}")
    print(f"Full Text: {entry['full_text'][:1000]}...")  # Printing only the first 1000 characters for brevity
    print("--------------------------------------------------")

/====
from edgartools import *
company=company("AAPL")
df=company.get_facts().to_pandas()
import numpy as np
[x for x in np.unique(df.loc[df['namespace']=='us-gaap']['fact']) if "hedge" in x.lower()]
/====
import requests
from bs4 import BeautifulSoup

def get_xbrl_links(cik, count=10):
    base_url = "https://www.sec.gov/cgi-bin/browse-edgar"
    params = {
        "action": "getcompany",
        "CIK": cik,
        "type": "10-K",  # You can change this to another form type if needed
        "dateb": "",
        "owner": "exclude",
        "count": count,
        "output": "atom"
    }
    response = requests.get(base_url, params=params)
    soup = BeautifulSoup(response.content, 'lxml')
    links = []

    for entry in soup.find_all('entry'):
        for link in entry.find_all('link'):
            if link.attrs.get('type') == 'text/html':
                report_url = link.attrs.get('href')
                xbrl_link = get_xbrl_link_from_report(report_url)
                if xbrl_link:
                    links.append(xbrl_link)

    return links

def get_xbrl_link_from_report(report_url):
    response = requests.get(report_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    for table_row in soup.find_all('tr'):
        for cell in table_row.find_all('td'):
            if 'XBRL INSTANCE DOCUMENT' in cell.get_text():
                return cell.a['href']

    return None

# Example Usage
cik = "0000320193"  # This is Apple Inc.'s CIK. Replace with the CIK of the company you're interested in.
xbrl_links = get_xbrl_links(cik)
for link in xbrl_links:
    print(link)



////
import random
import json

# List of stocks
stocks = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'FB']
customers = ['Bob', 'Alice', 'John', 'Jane', 'Mary']

# List of comparisons
comparisons = ['>', '<']

# List of window functions
window_functions = [
    {'func': 'mavg', 'desc': 'moving average'},
    {'func': 'mcount', 'desc': 'moving count'},
    {'func': 'msum', 'desc': 'moving sum'},
    {'func': 'mmin', 'desc': 'moving minimum'},
    {'func': 'mmax', 'desc': 'moving maximum'},
    {'func': 'mdev', 'desc': 'moving standard deviation'},
]

# List of aggregate functions
aggregate_functions = [
    {'func': 'sum', 'desc': 'total'},
    {'func': 'count', 'desc': 'number'},
    {'func': 'avg', 'desc': 'average'},
    {'func': 'min', 'desc': 'minimum'},
    {'func': 'max', 'desc': 'maximum'},
    {'func': 'dev', 'desc': 'standard deviation'},
]

# Generate examples
examples = []
for _ in range(100):
    if _ % 2 == 0: # Stocks related questions
        stock = random.choice(stocks)
        comparison = random.choice(comparisons)
        window_function = random.choice(window_functions)
        n = random.randint(3, 5)

        example = {
            "CreateTable": "`stocks set ([] date:2023.06.01 2023.06.02 2023.06.03 2023.06.04 2023.06.05 2023.06.06 2023.06.07; stock:`{}`{}`{}`{}`{}`{}; close:1500 1505 1490 1485 1495 1500 1510)".format(stock, stock, stock, stock, stock, stock),
            "QuestionInEnglish": "On which days was the {}-day {} of closing price for {} {} the closing price?".format(n, window_function['desc'], stock, "above" if comparison == ">" else "below"),
            "whereCond": "(and;enlist (in;`stock;enlist `{}`);({};{}[{}] `close;`close))".format(stock, comparison, window_function['func'], n),
            "groupBy": "(enlist `date)!enlist `date",
            "selectCond": "(enlist `{}_close)!enlist ({}[{}]; `close)".format(window_function['func'], window_function['func'], n),
            "AnswerInQ": "?[`stocks; whereCond; groupBy; selectCond]"
        }
    else: # Orders related questions
        customer = random.choice(customers)
        date_start = "2023.06.{}".format(random.randint(1, 15))
        date_end = "2023.06.{}".format(random.randint(16, 30))
        aggregate_function = random.choice(aggregate_functions)

        example = {
            "CreateTable": "`orders set ([] date:2023.06.01 2023.06.02 2023.06.03 2023.06.04 2023.06.05 2023.06.06 2023.06.07; Customer:`{}`{}`{}`{}`{}`{}; Qty:10 20 30 40 50 60 70)".format(customer, customer, customer, customer, customer, customer),
            "QuestionInEnglish": "What is the {} of quantities of orders for {} between {} and {}?".format(aggregate_function['desc'], customer, date_start, date_end),
            "whereCond": "(and;enlist (in;`Customer;enlist `{}`);(within;`date;(enlist (`{}`;`{}))))".format(customer, date_start, date_end),
            "groupBy": "(enlist `Customer)!enlist `Customer",
            "selectCond": "(enlist `{}_qty)!enlist ({}; `Qty)".format(aggregate_function['func'], aggregate_function['func']),
            "AnswerInQ": "?[`orders; whereCond; groupBy; selectCond]"
        }

    examples.append(example)

# Save as JSON
with open('examples.json', 'w') as f:
    json.dump(examples, f, indent=2)




******
TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged

from transformers import AutoTokenizer
import transformers
import torch

model = "TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    torch_dtype=torch.float16,
    device_map="auto",
)

conversation_text = '''[INST]<<SYS>>
 You are a helpful coding assistant that provides code based on the given query in context.
<</SYS>>
Write a jax program to implement a transformer.[/INST]'''

sequences = pipeline(
    conversation_text,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=2048,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")


conversation_text = '''[INST]<<SYS>>
 You are a helpful coding assistant that provides code based on the given query in context.
<</SYS>>
Can you add deepspeed trainer for a jax transformer model where we save the weights first to cpu memory and then to nvme.[/INST]'''

sequences = pipeline(
    conversation_text,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=2048,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")


/=====
pip installaccelerate # git+https://github.com/huggingface/accelerate.git
pip install bitsandbytes
pip install datasets==2.13.1
pip install transformers # git+https://github.com/huggingface/transformers.git
pip install peft #git+https://github.com/huggingface/peft.git
pip install trl #git+https://github.com/lvwerra/trl.git
pip installscipy

import argparse
import bitsandbytes as bnb
from datasets import load_dataset
from functools import partial
import os
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \
    DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import load_dataset

def load_model(model_name, bnb_config):
    n_gpus = torch.cuda.device_count()
    max_memory = f'{40960}MB'

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto", # dispatch efficiently the model on the available ressources
        max_memory = {i: max_memory for i in range(n_gpus)},
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)

    # Needed for LLaMA tokenizer
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

from datasets import load_dataset

dataset = load_dataset("databricks/databricks-dolly-15k", split="train

print(f'Number of prompts: {len(dataset)}')
print(f'Column names are: {dataset.column_names}')


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )


# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer

def create_bnb_config():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    return bnb_config

def create_peft_config(modules):
    """
    Create Parameter-Efficient Fine-Tuning config for your model
    :param modules: Names of the modules to apply Lora to
    """
    config = LoraConfig(
        r=16,  # dimension of the updated matrices
        lora_alpha=64,  # parameter for scaling
        target_modules=modules,
        lora_dropout=0.1,  # dropout probability for layers
        bias="none",
        task_type="CAUSAL_LM",
    )

    return config

def find_all_linear_names(model):
    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    if 'lm_head' in lora_module_names:  # needed for 16-bit
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

def print_trainable_parameters(model, use_4bit=False):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        # if using DS Zero 3 and the weights are initialized empty
        if num_params == 0 and hasattr(param, "ds_numel"):
            num_params = param.ds_numel

        all_param += num_params
        if param.requires_grad:
            trainable_params += num_params
    if use_4bit:
        trainable_params /= 2
    print(
        f"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}"
    )

model_name = "meta-llama/Llama-2-7b-hf" 

bnb_config = create_bnb_config()

model, tokenizer = load_model(model_name, bnb_config)

max_length = get_max_length(model)

dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)

def train(model, tokenizer, dataset, output_dir):
    # Apply preprocessing to the model to prepare it by
    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning
    model.gradient_checkpointing_enable()

    # 2 - Using the prepare_model_for_kbit_training method from PEFT
    model = prepare_model_for_kbit_training(model)

    # Get lora module names
    modules = find_all_linear_names(model)

    # Create PEFT config for these modules and wrap the model to PEFT
    peft_config = create_peft_config(modules)
    model = get_peft_model(model, peft_config)
    
    # Print information about the percentage of trainable parameters
    print_trainable_parameters(model)
    
    # Training parameters
    trainer = Trainer(
        model=model,
        train_dataset=dataset,
        args=TrainingArguments(
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=2,
            max_steps=20,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=1,
            output_dir="outputs",
            optim="paged_adamw_8bit",
        ),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )

 model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs
    
    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py
    # Verifying the datatypes before training
    
    dtypes = {}
    for _, p in model.named_parameters():
        dtype = p.dtype
        if dtype not in dtypes: dtypes[dtype] = 0
        dtypes[dtype] += p.numel()
    total = 0
    for k, v in dtypes.items(): total+= v
    for k, v in dtypes.items():
        print(k, v, v/total)
     
    do_train = True
    
    # Launch training
    print("Training...")
    
    if do_train:
        train_result = trainer.train()
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
        print(metrics)    
    
    ###
    
    # Saving model
    print("Saving last checkpoint of the model...")
    os.makedirs(output_dir, exist_ok=True)
    trainer.model.save_pretrained(output_dir)
    
    # Free memory for merging weights
    del model
    del trainer
    torch.cuda.empty_cache()
    
    
output_dir = "results/llama2/final_checkpoint"
train(model, tokenizer, dataset, output_dir)
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """
    
    # Add prompt to each sample
    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)
    
    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields
    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=["instruction", "context", "response", "text", "category"],
    )

    # Filter out samples that have input_ids exceeding max_length
    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)
    
    # Shuffle dataset
    dataset = dataset.shuffle(seed=seed)

    return dataset


====

import openpyxl
import json

def extract_excel_data(file_path):
    wb = openpyxl.load_workbook(file_path, data_only=False)  # load workbook
    data = {}
    for sheet in wb.sheetnames:
        ws = wb[sheet]  # get the worksheet
        sheet_data = []
        for row in ws.iter_rows():
            for cell in row:
                cell_data = {
                    'address': cell.coordinate,
                    'value': cell.value,
                    'formula': cell.data_type == 'f' and cell.value or None
                }
                sheet_data.append(cell_data)
        data[sheet] = sheet_data
    return data

def save_to_json(data, file_path):
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)

# Use these functions
excel_data = extract_excel_data('your_file.xlsx')  # replace with your Excel file path
save_to_json(excel_data, 'output.json')  # replace with desired JSON output file path



///
CREATE TABLE relationship_types (
    relationship_type INT PRIMARY KEY,
    description VARCHAR(255)
);


----

CREATE TABLE nodes (
    node VARCHAR(255) PRIMARY KEY,
    class VARCHAR(255)
);

INSERT INTO relationship_types (relationship_type, description) VALUES
(1, 'Type 1'),
(2, 'Type 2'),
(3, 'Type 3');

----

INSERT INTO nodes (node, class) VALUES
('A', 'X'),
('B', 'X'),
('C', 'Y'),
('D', 'X'),
('E', 'Y');
---

WITH RECURSIVE trace_back(source, destination) AS (
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.source = n.node
    WHERE e.destination = 'E' AND n.class = 'X'
    UNION ALL
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.source = n.node
    INNER JOIN trace_back tb ON tb.source = e.destination
    WHERE n.class = 'X'
)
SELECT DISTINCT source FROM trace_back;
--

WITH RECURSIVE trace_forward(source, destination) AS (
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    WHERE e.source = 'A' AND n.class = 'X'
    UNION ALL
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    INNER JOIN trace_forward tf ON tf.destination = e.source
    WHERE n.class = 'X'
)
SELECT DISTINCT destination FROM trace_forward;
---

To select based on both the node type and the relationship type, you can modify the recursive query we discussed above to include conditions on both of these fields.

Here's how you would do it for nodes of class 'X' that 'A' connects to, using only relationships of type 1:

sql
Copy code
WITH RECURSIVE trace_forward(source, destination, relationship_type) AS (
    SELECT e.source, e.destination, e.relationship_type
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    WHERE e.source = 'A' AND n.class = 'X' AND e.relationship_type = 1
    UNION ALL
    SELECT e.source, e.destination, e.relationship_type
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    INNER JOIN trace_forward tf ON tf.destination = e.source
    WHERE n.class = 'X' AND e.relationship_type = 1
)
SELECT DISTINCT destination FROM trace_forward;

/====
import glob
results=[]
dirs=['001','002','003','004','005','006']
for diri in dirs:
  dir_path = r'/content/drive/MyDrive/Citi-master-2/batch_0001/'+diri+'/*Item7_excerpt.txt'
  res = glob.glob(dir_path)
  dir_path = r'/content/drive/MyDrive/Citi-master-2/batch_0001/'+diri+'/*Item7A_excerpt.txt'
  res2 = glob.glob(dir_path)
  rest=res+res2
  for res in rest:
      print(res)
      with open(res,"r") as f:
        ticker=res.split("/")[-1].split("_")[0]
        a=(f.read())
        words = ["interest rate hedges", "derivatives","swaps", "rate lock"]

        # Your input string
        s = "I like to eat apple and banana."

        # Iterate over the list of words
        for word in words:
            # If the word is found in the string, print it
            for match in re.finditer(r'\b' + word + r'\b', a):
                start=match.start()
                result={'ticker':ticker,'found':word,'context':a[(start-300):start+300]}
                results.append(result)
    #            print(result)

df=pd.DataFrame(results)
df['value']='X'
df2=df.drop_duplicates(subset=['ticker','found'])
pd.pivot(df2,index='ticker',columns='found',values='value').fillna(' ')

////

''
requirements.txt file contents:

langchain==0.0.154
PyPDF2==3.0.1
python-dotenv==1.0.0
streamlit==1.18.1
faiss-cpu==1.7.4
streamlit-extras
'''


import streamlit as st
from dotenv import load_dotenv
import pickle
from PyPDF2 import PdfReader
from streamlit_extras.add_vertical_space import add_vertical_space
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.callbacks import get_openai_callback
import os

# Sidebar contents
with st.sidebar:
    st.title('🤗💬 LLM Chat App')
    st.markdown('''
    ## About
    This app is an LLM-powered chatbot built using:
    - [Streamlit](https://streamlit.io/)
    - [LangChain](https://python.langchain.com/)
    - [OpenAI](https://platform.openai.com/docs/models) LLM model

    ''')
    add_vertical_space(5)
    st.write('Made with ❤️ by [Prompt Engineer](https://youtube.com/@engineerprompt)')

load_dotenv()

def main():
    st.header("Chat with PDF 💬")


    # upload a PDF file
    pdf = st.file_uploader("Upload your PDF", type='pdf')

    # st.write(pdf)
    if pdf is not None:
        pdf_reader = PdfReader(pdf)
        
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
            )
        chunks = text_splitter.split_text(text=text)

        # # embeddings
        store_name = pdf.name[:-4]
        st.write(f'{store_name}')
        # st.write(chunks)

        if os.path.exists(f"{store_name}.pkl"):
            with open(f"{store_name}.pkl", "rb") as f:
                VectorStore = pickle.load(f)
            # st.write('Embeddings Loaded from the Disk')s
        else:
            embeddings = OpenAIEmbeddings()
            VectorStore = FAISS.from_texts(chunks, embedding=embeddings)
            with open(f"{store_name}.pkl", "wb") as f:
                pickle.dump(VectorStore, f)

        # embeddings = OpenAIEmbeddings()
        # VectorStore = FAISS.from_texts(chunks, embedding=embeddings)

        # Accept user questions/query
        query = st.text_input("Ask questions about your PDF file:")
        # st.write(query)

        if query:
            docs = VectorStore.similarity_search(query=query, k=3)

            llm = OpenAI()
            chain = load_qa_chain(llm=llm, chain_type="stuff")
            with get_openai_callback() as cb:
                response = chain.run(input_documents=docs, question=query)
                print(cb)
            st.write(response)

if __name__ == '__main__':
    main()

/=====
https://replicate.com/p/d2kfrf2ivzhsljxyaru4prfwdu

/===

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-40b"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")
//==
The paper "Gorilla: Large Language Model Connected with Massive APIs" introduces Gorilla, a fine-tuned Large Language Model (LLM) that outperforms GPT-4 in writing API calls. The authors address the challenges faced by LLMs in effectively using tools via API calls, such as generating accurate input arguments and avoiding hallucination of incorrect API usage. Gorilla, when combined with a document retriever, adapts to document changes at test-time, mitigates hallucination issues, and keeps up with frequently updated documentation. The model's effectiveness is evaluated using APIBench, a dataset of HuggingFace, TorchHub, and TensorHub APIs, demonstrating increased reliability and applicability of LLM outputs. The Gorilla model and related resources are available at https://gorilla.cs.berkeley.edu.


The paper "QLoRA: Efficient Finetuning of Quantized LLMs" presents QLoRA, a method for efficient finetuning of Large Language Models (LLMs) that enables finetuning a 65B parameter model on a single 48GB GPU. QLoRA introduces innovations like a new data type, 4-bit NormalFloat (NF4), double quantization, and paged optimizers to save memory without compromising performance. The authors' model family, Guanaco, outperforms all previous models on the Vicuna benchmark, achieving near ChatGPT performance with significantly less finetuning time. The paper provides a detailed analysis of instruction following and chatbot performance across multiple datasets and model scales, demonstrating that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results. The authors also critique current chatbot benchmarks and release all models and code, including CUDA kernels for 4-bit training.
https://arxiv.org/abs/2305.14314

The paper titled “Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling” introduces Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, the authors infer CoT recipes that work consistently well for a set of training samples. Their method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems1.

models vicuna 13b with mpt
Falcon-40B, developed by TII, is a 40B parameter causal decoder-only model trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It outperforms other open-source models like LLaMA, StableLM, RedPajama, and MPT, making it the best open-source model currently available. Falcon-40B features an architecture optimized for inference, with FlashAttention and multiquery. It is available under a license allowing commercial use. However, it is a raw, pretrained model and should be further finetuned for most use cases. The model is trained mostly on English, German, Spanish, French, with limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish. It is recommended to finetune Falcon-40B for specific tasks and to take appropriate precautions for any production use.
https://huggingface.co/tiiuae/falcon-40b

optimizer

https://arxiv.org/pdf/2305.14342.pdf
The paper "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training" by Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma introduces Sophia, a scalable second-order optimizer designed to improve the efficiency of language model pre-training. Sophia uses a lightweight estimate of the diagonal Hessian as the pre-conditioner and employs element-wise clipping to control the worst-case update size, mitigating the negative impact of non-convexity and rapid Hessian changes. The optimizer estimates the diagonal Hessian only every few iterations, minimizing time and memory overhead. In tests with GPT-2 models of various sizes, Sophia achieved a 2x speed-up compared to Adam in terms of the number of steps, total compute, and wall-clock time. The authors provide theoretical evidence that Sophia adapts to the curvature in different components of the parameters, which can be highly heterogeneous for language modeling tasks.

alibi


The video discusses the release of MPT-7B, the MosaicML Pretrained Transformer, a new family of open-source large language models (LLMs) that are commercially usable. The video provides general information about the model and compares it to the well-known LLaMA model from Meta AI. The second half of the video is dedicated to explaining ALiBi, a relative positional method used in MPT instead of the commonly used absolute positional embeddings. The video is structured into chapters, starting with an introduction to MPT-7B, followed by detailed information about the model, a discussion on finetuned MPT models, and finally an explanation of ALiBi.
https://www.youtube.com/watch?v=rb02ZnkcW4Y

The paper "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation" by Ofir Press, Noah A. Smith, and Mike Lewis explores how transformer models can extrapolate at inference time for sequences longer than those seen during training. The authors introduce a new and more efficient position method, Attention with Linear Biases (ALiBi), which biases query-key attention scores with a penalty proportional to their distance, rather than adding positional embeddings to word embeddings. They demonstrate that ALiBi can train a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's recency bias also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.
https://arxiv.org/pdf/2108.12409v2.pdf




////

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

# Document Loader
from langchain.document_loaders import TextLoader
loader = TextLoader('./state_of_the_union.txt')
documents = loader.load()

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text
    
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

!pip install faiss-cpu

from langchain.vectorstores import FAISS

db = FAISS.from_documents(docs, embeddings)


query = "What did the president say about the Supreme Court"
docs = db.similarity_search(query)

from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub


llm=HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0, "max_length":512})

chain = load_qa_chain(llm, chain_type="stuff")

query = "What did the president say about the Supreme Court"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

from langchain.document_loaders import UnstructuredPDFLoader
from langchain.indexes import VectorstoreIndexCreator

loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]
loaders

index = VectorstoreIndexCreator(
    embedding=HuggingFaceEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)

//change

from langchain.chains import RetrievalQA
chain = RetrievalQA.from_chain_type(llm=llm, 
                                    chain_type="stuff", 
                                    retriever=index.vectorstore.as_retriever(), 
                                    input_key="question")
				    
llm=HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0, "max_length":512})


---q&a

import re
import json

def parse_verbs(verbs, text):
    result = []
    for verb in verbs:
        verb_info = {}
        verb_info['verb'] = verb
        
        # Extract verb definition
        verb_defn = re.findall(rf"{verb} ⇒ ([^\n]+)", text)
        if verb_defn:
            verb_info['definition'] = verb_defn[0]
        
        # Extract verb explanation and examples
        verb_explanation = re.search(rf"{verb} ⇒ [^\n]+\n([^\n]+)\n([^2]+)", text, re.DOTALL)
        if verb_explanation:
            verb_info['explanation'] = verb_explanation.group(1).strip()
            verb_info['examples'] = verb_explanation.group(2).strip().split('\n')
        
        result.append(verb_info)
    
    return result

# Test the function
verbs = ['plus', 'negate', 'minus', 'first', 'times', 'sqrt', 'div', 'mod', 'where', 'flip', 'and', 'reverse', 'or', 'asc(desc)', 'less (more)', 'freq', 'equal', 'not', 'match', 'enlist', 'cat', 'sort', '[f]cut', 'count', '[f]take', 'floor', '[f]drop', 'str', 'parse', 'unique', 'find', 'type', '[f]at', 'value', 'dict']
text = """
2.1 plus ⇒ x+y
Add x and y. Arguments can be elements or lists but if both x and y are lists then they must be of equal length.

----

{
        "verb": "plus",
        "definition": "x+y",
        "explanation": "Add x and y. Arguments can be elements or lists but if both x and y are lists then they must be of equal length.",
        "examples": [
            "3+7",
            "10",
            "",
            "a:3;",
            " a+8",
            "11",
            "",
            " 3+4 5 6 7",
            "7 8 9 10",
            "",
            "3 4 5+4 5 6",
            "7 9 11",
            "",
            " 3 4+1"
        ]
    },
    

////
{
  "question": "What is the Shakti programming language?",
  "answer": "Shakti, also known as k9, is a programming language developed by Arthur Whitney and the team at Shakti. The language comes from a lineage of similar languages going back to APL. It's known for its terse syntax and high-speed data processing and analysis capabilities.",
  "example": {
    "input": "{x@(!#x)+\!#y}",
    "output": "This is an example of a Shakti (k9) code snippet. The actual output would depend on the context and the specific values of 'x' and 'y'."
  }
}


import json
import re

input_text = """
2.5 times ⇒ x*y
Mutliply x and y.
3*4
12
3*4 5 6
12 15 18
1 2 3*4 5 6
4 10 18

2.6 sqrt ⇒ %x
Return the square root of x.
%25
5.000e+00
%14.4
3.794e+00

2.7 div ⇒ x%y
Divide x by y.
12%5
2.400e+00
6%2
3.000e+00
"""

# Split input_text into sections using two or more newline characters as the separator
sections = re.split('\n{2,}', input_text.strip())

output = []

for section in sections:
    # Extract section number, verb, usage, and description
    match_header = re.match(r"(\d+\.\d+) (\w+) ⇒ (.*)\n(.*)", section)
    if match_header:
        section_number = match_header.group(1)
        verb = match_header.group(2)
        usage = match_header.group(3)
        description = match_header.group(4)
    
        # Extract examples
        match_examples = re.findall(r"((?:\d+|\%|\*|\s)+)\n((?:\d+|\.\d+|e\+\d+|\s)+)", section)
        examples = [{'input': example[0], 'output': example[1]} for example in match_examples]

        data = {
            'section': section_number,
            'verb': verb,
            'usage': usage,
            'description': description,
            'examples': examples
        }

        output.append(data)

json_output = json.dumps(output, indent=2)
print(json_output)

====section split
import re

# Multi-line string
s = """2.5 times ⇒ x*y
Mutliply x and y.

 3*4
12

3*4 5 6
12 15 18

1 2 3*4 5 6
4 10 18
2.6 sqrt ⇒ %x
Return the square root of x.

 %25
5.000e+00
 %14.4
3.794e+00
2.7 div ⇒ x%y
Divide x by y.

 12%5
2.400e+00

6%2
3.000e+00"""

# Split the string into sections at double newlines
sections = re.split(r'\n{2,}', s)

# Iterate over the sections
for section in sections:
    # Split the section into lines
    lines = section.split('\n')
    
    # Split the first line into its parts
    first_line_parts = re.split(r'\s+|⇒', lines[0])
    first_line_parts = [part.strip() for part in first_line_parts]
    
    print(first_line_parts)

# This will output each first line as a list of its parts

==q answer

[
  {
    "question": "What is the sum of the first three numbers in the reversed list?",
    "answer": "15",
    "example": "_3|1 2 3 4 5"
  },
  {
    "question": "What are the sorted unique elements of the two combined lists?",
    "answer": "1 2 3 4 5 6",
    "example": "^?1 2 3,4 4 5 6"
  },
  {
    "question": "What is the remainder when the sum of the first three numbers is divided by the last number in the list?",
    "answer": "2",
    "example": "_3|1 2 3 4 5!5"
  },
  {
    "question": "What are the first three elements of the sorted list?",
    "answer": "1 2 3",
    "example": "_3^5 1 3 2 4"
  },
  {
    "question": "What is the first element of the list after removing the first three elements?",
    "answer": "4",
    "example": "__3_1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two elements in the reversed list?",
    "answer": "9",
    "example": "+/_2|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two numbers in the sorted list?",
    "answer": "3",
    "example": "+/_2^5 1 3 2 4"
  },
  {
    "question": "What is the maximum value in the reversed list?",
    "answer": "5",
    "example": "|/|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first three elements in the list after removing the first two elements?",
    "answer": "12",
    "example": "+/_3_2_1 2 3 4 5"
  },
  {
    "question": "What is the remainder when the sum of the first two numbers is divided by the last number in the reversed list?",
    "answer": "0",
    "example": "+/_2|1 2 3 4 5!5"
  },
  {
    "question": "What is the sum of the first three numbers in the list after removing duplicates?",
    "answer": "6",
    "example": "+/_3?1 2 3 3 4 5"
  },
  {
    "question": "What is the square root of the first number in the sorted list?",
    "answer": "1",
    "example": "%_*^5 1 3 2 4"
  },
  {
    "question": "What is the sum of the first three numbers in the reversed list?",
    "answer": "15",
    "example": "+/_3|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two numbers in the list after removing the first three elements?",
    "answer": "9",
    "example": "+/_2_3_1 2 3 4 5"
  },
  {
    "question": "What is

===

[
  {
    "question": "How to add two numbers in Shakti?",
    "answer": "You can use the 'plus' verb for addition. For example, to add 3 and 7, you would use the command '3+7'.",
    "example": "3+7"
  },
  {
    "question": "How to subtract two numbers in Shakti?",
    "answer": "You can use the 'minus' verb for subtraction. For example, to subtract 2 from 5, you would use the command '5-2'.",
    "example": "5-2"
  },
  
  ===
  [{
"question": "How do I calculate the average of a list of numbers in Shakti?",
"answer": "You can calculate the average of a list of numbers by summing up the numbers using the '+' operator and then dividing by the count of the numbers using the '%' operator.",
"example": "list: 1 2 3 4 5;\navg: (+/list) % #list;"
},
{
"question": "How do I find the maximum value in a list of numbers in Shakti?",
"answer": "You can find the maximum value in a list of numbers using the 'max' function.",
"example": "list: 1 2 3 4 5;\nmaxValue: max list;"
},
{
"question": "How do I create a list of even numbers up to a given number in Shakti?",
"answer": "You can create a list of even numbers up to a given number by using the 'til' function to create a list from 0 to the number and the 'mod' function with an argument of 2 to check for even numbers.",
"example": "number: 10;\nevenNumbers: 2 * til number % 2;"
},
{
"question": "How do I find the maximum value in a list of numbers in Shakti?",
"answer": "You can find the maximum value in a list of numbers using the 'max' function.",
"example": "list: 1 2 3 4 5;\nmaxValue: max list;"
},
{
"question": "How do I calculate the sum of the squares of a list of numbers in Shakti?",
"answer": "You can calculate the sum of the squares of a list of numbers by squaring the numbers using the 'sqr' function and then summing up the squares using the '+' operator.",
"example": "list: 1 2 3 4 5;\nsumSquares: +/ sqr list;"
},
{
"question": "How do I filter out numbers less than a certain value from a list of numbers in Shakti?",
"answer": "You can filter out numbers less than a certain value from a list of numbers by using the 'where' function with a comparison function.",
"example": "list: 1 2 3 4 5;\nvalue: 3;\nfiltered: list where list > value;"
},
{
"question": "How do I count the number of occurrences of a certain value in a list in Shakti?",
"answer": "You can count the number of occurrences of a certain value in a list by using the 'count' function with a comparison function.",
"example": "list: 1 2 3 4 5;\nvalue: 3;\ncount: count list where list = value;"
},

////

import json
from random import choice

# Definitions of Shakti operations
shakti_definitions = {
    '+': 'addition',
    '-': 'subtraction',
    '*': 'multiplication',
    '%': 'division',
    'til': 'sequence generation',
    'max': 'maximum',
    'min': 'minimum',
    'avg': 'average',
    'count': 'count',
    'where': 'filter'
}

# List of sample data and functions
sample_data = ['list: 1 2 3 4 5', 'number: 10', 'value: 3']
sample_functions = ['avg: (+/list) % #list', 'evenNumbers: 2 * til number % 2', 
                    'maxValue: max list', 'maxIndex: list ? max list', 
                    'stringList: string list', 'sumSquares: +/ sqr list', 
                    'filtered: list where list > value', 
                    'count: count list where list = value']

# Function to generate question, answer, example tuple
def generate_tuple():
    op = choice(list(shakti_definitions.keys()))
    data = choice(sample_data)
    func = choice(sample_functions)

    question = f"How do I perform {shakti_definitions[op]} in Shakti?"
    answer = f"You can perform {shakti_definitions[op]} by using the '{op}' operation."
    example = f"{data};\n{func};"

    return {"question": question, "answer": answer, "example": example}

# Function to generate a number of tuples
def generate_tuples(n):
    return [generate_tuple() for _ in range(n)]

# Generate 5 tuples as an example
tuples = generate_tuples(5)

# Convert to JSON
json_output = json.dumps(tuples, indent=4)

# Print the JSON
print(json_output)

/===

import numpy as np
import pandas as pd

shakti_to_python = {
    '+': {
        'desc': 'addition',
        'python': 'np.add(a, b)',
    },
    '-': {
        'desc': 'subtraction',
        'python': 'np.subtract(a, b)',
    },
    '*': {
        'desc': 'multiplication',
        'python': 'np.multiply(a, b)',
    },
    '%': {
        'desc': 'division',
        'python': 'np.divide(a, b)',
    },
    'til': {
        'desc': 'sequence generation',
        'python': 'np.arange(n)',
    },
    'max': {
        'desc': 'maximum',
        'python': 'np.max(a)',
    },
    'min': {
        'desc': 'minimum',
        'python': 'np.min(a)',
    },
    'avg': {
        'desc': 'average',
        'python': 'np.mean(a)',
    },
    'count': {
        'desc': 'count',
        'python': 'len(a)',
    },
    'where': {
        'desc': 'filter',
        'python': 'a[a > n]',
    }
}

/====
Disclaimer1

Creating a comprehensive mapping between Shakti operations and Python code would involve systematically associating each Shakti operation with an equivalent Python function, likely from libraries such as NumPy and Pandas.

However, please note that Shakti is designed as a high-performance, array-based language that can handle complex data operations in a minimalistic way. Therefore, translating Shakti operations into Python code may result in more verbose and complex Python scripts, as Python is a general-purpose language with a different design philosophy.

Here are some basic examples of Shakti operations and their Python equivalents, using NumPy and Pandas:



Disclaimer2

This dictionary provides a basic translation of some Shakti operations into Python code using NumPy. Note that the Python code is represented as strings, so you would need to use eval() or similar methods to execute this code, which can be dangerous and is generally not recommended. Also, the Python code assumes that the variables (a, b, n) are already defined.

Keep in mind that this is a simplified representation, and it doesn't cover all Shakti operations, especially more complex ones. Creating a comprehensive mapping would require a deep understanding of both Shakti and Python, and it would be a significant undertaking.







=/

results2=[]
badresults2=[]
for keyword in keywords:
      print(keyword)
      keyword2=keyword['link'].split("/")[0]
      url2="https://code.kx.com/q/ref/"+keyword2
      r=requests.get(url2)
      soup=BeautifulSoup(r.text,"html.parser")
      reflist=[x for x in soup.find_all("code",{"class":"language-syntax"})]
      startline=(r.text).find('class="headerlink" href="#'+keyword2) 
      if startline > 10:
        try:
          soup = BeautifulSoup(r.text[startline:-1], 'html.parser')

          instruction = soup.select_one('code.language-syntax').get_text()
          name = soup.select_one('p > em').get_text()
          example = soup.select_one('pre.highlight > code.language-q').get_text()

          def extract_key_and_value_from_li(li):
              key = li.select_one('code').get_text()
              value = li.text.replace(key, '').strip()
              return key, value

          where_items = soup.select('ul > li')

          description = {}
          for item in where_items:
              if item.find('ul'):
                  nested_list_items = item.select('ul > li')
                  nested_dict = {}
                  for nested_item in nested_list_items:
                      key, value = extract_key_and_value_from_li(nested_item)
                      nested_dict[key] = value
                  key, value = extract_key_and_value_from_li(item)
                  description[key] = {**nested_dict, **{'description': value}}
              else:
                  key, value = extract_key_and_value_from_li(item)
                  description[key] = value

          result_description_list = soup.select('ul > li')
          result = {}
          for result_item in result_description_list[-2:]:
              key, value = extract_key_and_value_from_li(result_item)
              result[key] = value

          json_data = {
              'instruction': instruction,
              'name': name,
              'example': example,
              'description': description,
              'result': result
          }
          soup = BeautifulSoup(r.text, 'lxml')

          # Find all 'pre' elements with the 'highlight' class
          pre_elements = soup.find_all('pre', class_='highlight')

          # Extract the text from the found elements
          texts = [pre_element.text for pre_element in pre_elements]

          # Print the extracted texts
          results2.append([keyword,json_data,texts])
          print(keyword," good ")
        except:
          badresults2.append(keyword)
          print(keyword," bad ")
      else:
        badresults2.append(keyword)
        print(keyword," bad ")
	/=====
	
soup = BeautifulSoup(r.text, 'lxml')

# Find all 'pre' elements with the 'highlight' class
pre_elements = soup.find_all('pre', class_='highlight')

# Extract the text from the found elements
texts = [pre_element.text for pre_element in pre_elements]

# Print the extracted texts
for text in texts:
    print(text)

/====

	
	
x^y    ^[x;y]
q)0^1 2 3 0N
1 2 3 0
q)100^1 2 -5 0N 10 0N
1 2 -5 100 10 100
q)1.0^1.2 -4.5 0n 0n 15
1.2 -4.5 1 1 15
q)`nobody^`tom`dick``harry
`tom`dick`nobody`harry
q)1 2 3 4 5^6 0N 8 9 0N
6 2 8 9 5
q)a:11.0 2.1 3.1 0n 4.5 0n
q)type a
9h
q)10^a
11 2.1 3.1 10 4.5 10
q)type 10^a
9h
q)(`a`b`c!1 2 3)^`b`c!0N 30
a| 1
b| 2
c| 30


chatGpt4

public class PalindromeChecker {

    public static boolean isPalindrome(int num) {
        int reverse = 0;
        int temp = num;
        int mask = (Integer.SIZE - Integer.numberOfLeadingZeros(num)) - 1;
        int i = 0;
        do {
            reverse = (reverse << 1) | ((temp >>> i) & 1);
            reverse = (reverse << 1) | ((temp >>> (mask - i)) & 1);
            i++;
        } while (i <= mask / 2);
        return num == reverse;
    }

    public static void main(String[] args) {
        int num = 12321;
        if (isPalindrome(num)) {
            System.out.println(num + " is a palindrome.");
        } else {
            System.out.println(num + " is not a palindrome.");
        }
    }
}

The fastest way to check if an integer is a palindrome would be to use an algorithm that involves only mathematical operations and no loops. One such algorithm involves reversing the digits of the number and comparing it with the original number. Here's an implementation in Java:

public static boolean isPalindrome(int num) {
    if (num < 0) {
        return false; // negative numbers cannot be palindromes
    }
    int reversed = 0;
    int original = num;
    while (num != 0) {
        reversed = reversed * 10 + num % 10;
        num /= 10;
    }
    return original == reversed;
}

correct but what it is a very long bit pattern that you want to check if it is symmetric in a certain basis?

If you have a very long bit pattern that you want to check for symmetry in a certain basis, you can use a bit-manipulation approach that takes advantage of bitwise operations to perform the comparison.

Here's an implementation in Java that checks for symmetry in a certain basis:

public static boolean isSymmetric(long num, int basis) {
    long mask = (1L << basis) - 1L;
    long reversed = 0L;
    while (num != 0L) {
        reversed = (reversed << basis) | (num & mask);
        num >>>= basis;
    }
    return num == reversed;
}



/=====

vicuna

public class PalindromeNumber {
public static void main(String[] args) {
int num = 121;
if (num == 0) {
System.out.println(num + " is a palindrome.");
}
else {
String reverse = String.valueOf(num);
while (reverse.length()!= 0) {
if (reverse.charAt(0)!= reverse.charAt(reverse.length() - 1)) {
System.out.println(num + " is not a palindrome.");
break;
}
reverse = reverse.substring(1, reverse.length() - 1);
}
}
}

This program first initializes the variable `num` to a non-zero value, since 0 is considered a palindrome. It then checks if `num` is equal to 0 using an `if` statement. If `num` is equal to 0, the program prints out that the number is a palindrome.
Otherwise, the program converts the number to a string using the `String.valueOf()` method, and then uses a `while` loop to compare the first and last characters of the string. If the first and last characters are the same, the program continues looping until it finds two characters that are not the same, at which point it prints out that the number is not a palindrome and exits the loop.
This program should print out the following output:

/===
koala

BEGINNING OF CONVERSATION: USER: can you write a java program to check if a number is palindrome or not?

Here's an example Java program that checks if a given integer is a palindrome or not:

```java
import java.util.Scanner;

public class PalindromeChecker {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);

        System.out.print("Enter the number: ");
        int num = sc.nextInt();

        boolean isPalindrome = true;
        for (int i = 0; i < num - 1; i++) {
            if (num % 2!= 0) {
                isPalindrome = false;
                break;
            }
        }

        System.out.println("Is " + num + " a palindrome?");
        System.out.println("Answer: " + isPalindrome);
    }
}

/===
open assistant

public class CheckPalindrome {
    public static void main(String[] args) {
        int num = 123;
        
        // Reverse the digits of the number and compare it with original number
        int reverseNum = 0;
        while (num > 0) {
            int lastDigit = num % 10;
            
            reverseNum += lastDigit * pow(10, len - 1);
            num /= 10;
        }
                
        if (reverseNum == num) {
            System.out.println("Number is a Palindrome");
        } else {
            System.out.println("Number is not a Palindrome");
        }
    }
}

no good

import org.junit.*;

public class CheckPalindromeTest {
	@BeforeClass
	public static void setup() {}
    
    @AfterClass
    public static void tearDown(){}
    
    @Before
    public void setUp(){}
    
    @After
    public void TearDown(){}
    
	/** Test whether 123456789 is a palindrome */  
	@Test
	public void shouldReturnTrueFor123456789(){
            Assert.assertEquals(true , new CheckPalindrome().isPalindrome(123));
	} 
	
	/** Test whether 99 is a palindrome */ 
	@Test
	public void shouldReturnFalseFor99(){
                Assert.assertEquals(false, new CheckPalindrome().isPalindrome(99));
	} 
}

/====



from datasets import load_dataset
from transformers import LLaMATokenizer


tokenizer = LLaMATokenizer.from_pretrained("decapoda-research/llama-7b-hf", add_eos_token=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id

data = load_dataset("json", data_files="alpaca_data.json")


def generate_prompt(data_point):
    # sorry about the formatting disaster gotta move fast
    if data_point["instruction"]:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{data_point["instruction"]}

### Input:
{data_point["input"]}

### Response:
{data_point["output"]}"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{data_point["instruction"]}

### Response:
{data_point["output"]}"""


data = data.map(lambda data_point: {"prompt": tokenizer(generate_prompt(data_point))})

import os

# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
import torch.nn as nn
import bitsandbytes as bnb
from datasets import load_dataset
import transformers
from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer
from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model

# Setting for A100 - For 3090 
MICRO_BATCH_SIZE = 8  # change to 4 for 3090
BATCH_SIZE = 128
GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE
EPOCHS = 2  # paper uses 3
LEARNING_RATE = 2e-5  # from the original paper
CUTOFF_LEN = 256  # 256 accounts for about 96% of the data
LORA_R = 4
LORA_ALPHA = 16
LORA_DROPOUT = 0.05


model = LLaMAForCausalLM.from_pretrained(
    "decapoda-research/llama-7b-hf",
    load_in_8bit=True,
    device_map="auto",
)
tokenizer = LLaMATokenizer.from_pretrained(
    "decapoda-research/llama-7b-hf", add_eos_token=True
)

model = prepare_model_for_int8_training(model)

config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, config)
tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token
data = load_dataset("json", data_files="alpaca_data.json")

def generate_prompt(data_point):
    # sorry about the formatting disaster gotta move fast
    if data_point["input"]:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
{data_point["instruction"]}
### Input:
{data_point["input"]}
### Response:
{data_point["output"]}"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{data_point["instruction"]}
### Response:
{data_point["output"]}"""


data = data.shuffle().map(
    lambda data_point: tokenizer(
        generate_prompt(data_point),
        truncation=True,
        max_length=CUTOFF_LEN,
        padding="max_length",
    )
)

trainer = transformers.Trainer(
    model=model,
    train_dataset=data["train"],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=MICRO_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=100,
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=True,
        logging_steps=1,
        output_dir="lora-alpaca",
        save_total_limit=3,
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False
trainer.train(resume_from_checkpoint=False)

model.save_pretrained("lora-alpaca")

/=====
// First, create a wrapper function for your C code:

#include <stdio.h>

void c_code_wrapper(void *context, void (*callback)(void *context)) {
    // Your C code here
    callback(context);
}

// Then expose this wrapper as a Rust extern function:

extern "C" fn c_code_wrapper(context: *mut c_void, callback: extern fn(*mut c_void)) {
    unsafe {
        c_code_wrapper(context, callback);
    }
}

// Finally, call the wrapper from Rust using an async closure:

let mut context = 0;
let callback = |context: *mut c_void| {
    // Your Rust code here
};

let future = async {
    c_code_wrapper(&mut context, callback);
};

// The future will execute the C code asynchronously.

from bs4 import BeautifulSoup
import requests

r=requests.get("http://code.kx.com/q/ref")
soup=BeautifulSoup(r.text,"html.parser")
reflist=[x for x in soup.find_all('a') if str(x)[4:7] == "ref"]
reflist2=[x for x in reflist if len(str(x).split('"'))>3]
[{'link':(str(x)).split('"')[1],'descr':(str(x)).split('"')[3]} for x in reflist2[:-2]]
reflist=[x for x in soup.find_all("div",{"class":"md-content"})]


******
TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged

from transformers import AutoTokenizer
import transformers
import torch

model = "TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    torch_dtype=torch.float16,
    device_map="auto",
)

conversation_text = '''[INST]<<SYS>>
 You are a helpful coding assistant that provides code based on the given query in context.
<</SYS>>
Write a jax program to implement a transformer.[/INST]'''

sequences = pipeline(
    conversation_text,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=2048,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")


conversation_text = '''[INST]<<SYS>>
 You are a helpful coding assistant that provides code based on the given query in context.
<</SYS>>
Can you add deepspeed trainer for a jax transformer model where we save the weights first to cpu memory and then to nvme.[/INST]'''

sequences = pipeline(
    conversation_text,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=2048,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")


/=====
pip installaccelerate # git+https://github.com/huggingface/accelerate.git
pip install bitsandbytes
pip install datasets==2.13.1
pip install transformers # git+https://github.com/huggingface/transformers.git
pip install peft #git+https://github.com/huggingface/peft.git
pip install trl #git+https://github.com/lvwerra/trl.git
pip installscipy

import argparse
import bitsandbytes as bnb
from datasets import load_dataset
from functools import partial
import os
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \
    DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import load_dataset

def load_model(model_name, bnb_config):
    n_gpus = torch.cuda.device_count()
    max_memory = f'{40960}MB'

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto", # dispatch efficiently the model on the available ressources
        max_memory = {i: max_memory for i in range(n_gpus)},
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)

    # Needed for LLaMA tokenizer
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

from datasets import load_dataset

dataset = load_dataset("databricks/databricks-dolly-15k", split="train

print(f'Number of prompts: {len(dataset)}')
print(f'Column names are: {dataset.column_names}')


def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )


# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py
def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer

def create_bnb_config():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    return bnb_config

def create_peft_config(modules):
    """
    Create Parameter-Efficient Fine-Tuning config for your model
    :param modules: Names of the modules to apply Lora to
    """
    config = LoraConfig(
        r=16,  # dimension of the updated matrices
        lora_alpha=64,  # parameter for scaling
        target_modules=modules,
        lora_dropout=0.1,  # dropout probability for layers
        bias="none",
        task_type="CAUSAL_LM",
    )

    return config

def find_all_linear_names(model):
    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    if 'lm_head' in lora_module_names:  # needed for 16-bit
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

def print_trainable_parameters(model, use_4bit=False):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        num_params = param.numel()
        # if using DS Zero 3 and the weights are initialized empty
        if num_params == 0 and hasattr(param, "ds_numel"):
            num_params = param.ds_numel

        all_param += num_params
        if param.requires_grad:
            trainable_params += num_params
    if use_4bit:
        trainable_params /= 2
    print(
        f"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}"
    )

model_name = "meta-llama/Llama-2-7b-hf" 

bnb_config = create_bnb_config()

model, tokenizer = load_model(model_name, bnb_config)

max_length = get_max_length(model)

dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)

def train(model, tokenizer, dataset, output_dir):
    # Apply preprocessing to the model to prepare it by
    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning
    model.gradient_checkpointing_enable()

    # 2 - Using the prepare_model_for_kbit_training method from PEFT
    model = prepare_model_for_kbit_training(model)

    # Get lora module names
    modules = find_all_linear_names(model)

    # Create PEFT config for these modules and wrap the model to PEFT
    peft_config = create_peft_config(modules)
    model = get_peft_model(model, peft_config)
    
    # Print information about the percentage of trainable parameters
    print_trainable_parameters(model)
    
    # Training parameters
    trainer = Trainer(
        model=model,
        train_dataset=dataset,
        args=TrainingArguments(
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            warmup_steps=2,
            max_steps=20,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=1,
            output_dir="outputs",
            optim="paged_adamw_8bit",
        ),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )

 model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs
    
    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py
    # Verifying the datatypes before training
    
    dtypes = {}
    for _, p in model.named_parameters():
        dtype = p.dtype
        if dtype not in dtypes: dtypes[dtype] = 0
        dtypes[dtype] += p.numel()
    total = 0
    for k, v in dtypes.items(): total+= v
    for k, v in dtypes.items():
        print(k, v, v/total)
     
    do_train = True
    
    # Launch training
    print("Training...")
    
    if do_train:
        train_result = trainer.train()
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
        print(metrics)    
    
    ###
    
    # Saving model
    print("Saving last checkpoint of the model...")
    os.makedirs(output_dir, exist_ok=True)
    trainer.model.save_pretrained(output_dir)
    
    # Free memory for merging weights
    del model
    del trainer
    torch.cuda.empty_cache()
    
    
output_dir = "results/llama2/final_checkpoint"
train(model, tokenizer, dataset, output_dir)
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """
    
    # Add prompt to each sample
    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)
    
    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields
    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=["instruction", "context", "response", "text", "category"],
    )

    # Filter out samples that have input_ids exceeding max_length
    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)
    
    # Shuffle dataset
    dataset = dataset.shuffle(seed=seed)

    return dataset


====

import openpyxl
import json

def extract_excel_data(file_path):
    wb = openpyxl.load_workbook(file_path, data_only=False)  # load workbook
    data = {}
    for sheet in wb.sheetnames:
        ws = wb[sheet]  # get the worksheet
        sheet_data = []
        for row in ws.iter_rows():
            for cell in row:
                cell_data = {
                    'address': cell.coordinate,
                    'value': cell.value,
                    'formula': cell.data_type == 'f' and cell.value or None
                }
                sheet_data.append(cell_data)
        data[sheet] = sheet_data
    return data

def save_to_json(data, file_path):
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)

# Use these functions
excel_data = extract_excel_data('your_file.xlsx')  # replace with your Excel file path
save_to_json(excel_data, 'output.json')  # replace with desired JSON output file path



///
CREATE TABLE relationship_types (
    relationship_type INT PRIMARY KEY,
    description VARCHAR(255)
);


----

CREATE TABLE nodes (
    node VARCHAR(255) PRIMARY KEY,
    class VARCHAR(255)
);

INSERT INTO relationship_types (relationship_type, description) VALUES
(1, 'Type 1'),
(2, 'Type 2'),
(3, 'Type 3');

----

INSERT INTO nodes (node, class) VALUES
('A', 'X'),
('B', 'X'),
('C', 'Y'),
('D', 'X'),
('E', 'Y');
---

WITH RECURSIVE trace_back(source, destination) AS (
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.source = n.node
    WHERE e.destination = 'E' AND n.class = 'X'
    UNION ALL
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.source = n.node
    INNER JOIN trace_back tb ON tb.source = e.destination
    WHERE n.class = 'X'
)
SELECT DISTINCT source FROM trace_back;
--

WITH RECURSIVE trace_forward(source, destination) AS (
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    WHERE e.source = 'A' AND n.class = 'X'
    UNION ALL
    SELECT e.source, e.destination
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    INNER JOIN trace_forward tf ON tf.destination = e.source
    WHERE n.class = 'X'
)
SELECT DISTINCT destination FROM trace_forward;
---

To select based on both the node type and the relationship type, you can modify the recursive query we discussed above to include conditions on both of these fields.

Here's how you would do it for nodes of class 'X' that 'A' connects to, using only relationships of type 1:

sql
Copy code
WITH RECURSIVE trace_forward(source, destination, relationship_type) AS (
    SELECT e.source, e.destination, e.relationship_type
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    WHERE e.source = 'A' AND n.class = 'X' AND e.relationship_type = 1
    UNION ALL
    SELECT e.source, e.destination, e.relationship_type
    FROM dag_edges e
    JOIN nodes n ON e.destination = n.node
    INNER JOIN trace_forward tf ON tf.destination = e.source
    WHERE n.class = 'X' AND e.relationship_type = 1
)
SELECT DISTINCT destination FROM trace_forward;

/====
import glob
results=[]
dirs=['001','002','003','004','005','006']
for diri in dirs:
  dir_path = r'/content/drive/MyDrive/Citi-master-2/batch_0001/'+diri+'/*Item7_excerpt.txt'
  res = glob.glob(dir_path)
  dir_path = r'/content/drive/MyDrive/Citi-master-2/batch_0001/'+diri+'/*Item7A_excerpt.txt'
  res2 = glob.glob(dir_path)
  rest=res+res2
  for res in rest:
      print(res)
      with open(res,"r") as f:
        ticker=res.split("/")[-1].split("_")[0]
        a=(f.read())
        words = ["interest rate hedges", "derivatives","swaps", "rate lock"]

        # Your input string
        s = "I like to eat apple and banana."

        # Iterate over the list of words
        for word in words:
            # If the word is found in the string, print it
            for match in re.finditer(r'\b' + word + r'\b', a):
                start=match.start()
                result={'ticker':ticker,'found':word,'context':a[(start-300):start+300]}
                results.append(result)
    #            print(result)

df=pd.DataFrame(results)
df['value']='X'
df2=df.drop_duplicates(subset=['ticker','found'])
pd.pivot(df2,index='ticker',columns='found',values='value').fillna(' ')

////

''
requirements.txt file contents:

langchain==0.0.154
PyPDF2==3.0.1
python-dotenv==1.0.0
streamlit==1.18.1
faiss-cpu==1.7.4
streamlit-extras
'''


import streamlit as st
from dotenv import load_dotenv
import pickle
from PyPDF2 import PdfReader
from streamlit_extras.add_vertical_space import add_vertical_space
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.callbacks import get_openai_callback
import os

# Sidebar contents
with st.sidebar:
    st.title('🤗💬 LLM Chat App')
    st.markdown('''
    ## About
    This app is an LLM-powered chatbot built using:
    - [Streamlit](https://streamlit.io/)
    - [LangChain](https://python.langchain.com/)
    - [OpenAI](https://platform.openai.com/docs/models) LLM model

    ''')
    add_vertical_space(5)
    st.write('Made with ❤️ by [Prompt Engineer](https://youtube.com/@engineerprompt)')

load_dotenv()

def main():
    st.header("Chat with PDF 💬")


    # upload a PDF file
    pdf = st.file_uploader("Upload your PDF", type='pdf')

    # st.write(pdf)
    if pdf is not None:
        pdf_reader = PdfReader(pdf)
        
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
            )
        chunks = text_splitter.split_text(text=text)

        # # embeddings
        store_name = pdf.name[:-4]
        st.write(f'{store_name}')
        # st.write(chunks)

        if os.path.exists(f"{store_name}.pkl"):
            with open(f"{store_name}.pkl", "rb") as f:
                VectorStore = pickle.load(f)
            # st.write('Embeddings Loaded from the Disk')s
        else:
            embeddings = OpenAIEmbeddings()
            VectorStore = FAISS.from_texts(chunks, embedding=embeddings)
            with open(f"{store_name}.pkl", "wb") as f:
                pickle.dump(VectorStore, f)

        # embeddings = OpenAIEmbeddings()
        # VectorStore = FAISS.from_texts(chunks, embedding=embeddings)

        # Accept user questions/query
        query = st.text_input("Ask questions about your PDF file:")
        # st.write(query)

        if query:
            docs = VectorStore.similarity_search(query=query, k=3)

            llm = OpenAI()
            chain = load_qa_chain(llm=llm, chain_type="stuff")
            with get_openai_callback() as cb:
                response = chain.run(input_documents=docs, question=query)
                print(cb)
            st.write(response)

if __name__ == '__main__':
    main()

/=====
https://replicate.com/p/d2kfrf2ivzhsljxyaru4prfwdu

/===

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-40b"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")
//==
The paper "Gorilla: Large Language Model Connected with Massive APIs" introduces Gorilla, a fine-tuned Large Language Model (LLM) that outperforms GPT-4 in writing API calls. The authors address the challenges faced by LLMs in effectively using tools via API calls, such as generating accurate input arguments and avoiding hallucination of incorrect API usage. Gorilla, when combined with a document retriever, adapts to document changes at test-time, mitigates hallucination issues, and keeps up with frequently updated documentation. The model's effectiveness is evaluated using APIBench, a dataset of HuggingFace, TorchHub, and TensorHub APIs, demonstrating increased reliability and applicability of LLM outputs. The Gorilla model and related resources are available at https://gorilla.cs.berkeley.edu.


The paper "QLoRA: Efficient Finetuning of Quantized LLMs" presents QLoRA, a method for efficient finetuning of Large Language Models (LLMs) that enables finetuning a 65B parameter model on a single 48GB GPU. QLoRA introduces innovations like a new data type, 4-bit NormalFloat (NF4), double quantization, and paged optimizers to save memory without compromising performance. The authors' model family, Guanaco, outperforms all previous models on the Vicuna benchmark, achieving near ChatGPT performance with significantly less finetuning time. The paper provides a detailed analysis of instruction following and chatbot performance across multiple datasets and model scales, demonstrating that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results. The authors also critique current chatbot benchmarks and release all models and code, including CUDA kernels for 4-bit training.
https://arxiv.org/abs/2305.14314

The paper titled “Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling” introduces Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, the authors infer CoT recipes that work consistently well for a set of training samples. Their method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems1.

models vicuna 13b with mpt
Falcon-40B, developed by TII, is a 40B parameter causal decoder-only model trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It outperforms other open-source models like LLaMA, StableLM, RedPajama, and MPT, making it the best open-source model currently available. Falcon-40B features an architecture optimized for inference, with FlashAttention and multiquery. It is available under a license allowing commercial use. However, it is a raw, pretrained model and should be further finetuned for most use cases. The model is trained mostly on English, German, Spanish, French, with limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish. It is recommended to finetune Falcon-40B for specific tasks and to take appropriate precautions for any production use.
https://huggingface.co/tiiuae/falcon-40b

optimizer

https://arxiv.org/pdf/2305.14342.pdf
The paper "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training" by Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma introduces Sophia, a scalable second-order optimizer designed to improve the efficiency of language model pre-training. Sophia uses a lightweight estimate of the diagonal Hessian as the pre-conditioner and employs element-wise clipping to control the worst-case update size, mitigating the negative impact of non-convexity and rapid Hessian changes. The optimizer estimates the diagonal Hessian only every few iterations, minimizing time and memory overhead. In tests with GPT-2 models of various sizes, Sophia achieved a 2x speed-up compared to Adam in terms of the number of steps, total compute, and wall-clock time. The authors provide theoretical evidence that Sophia adapts to the curvature in different components of the parameters, which can be highly heterogeneous for language modeling tasks.

alibi


The video discusses the release of MPT-7B, the MosaicML Pretrained Transformer, a new family of open-source large language models (LLMs) that are commercially usable. The video provides general information about the model and compares it to the well-known LLaMA model from Meta AI. The second half of the video is dedicated to explaining ALiBi, a relative positional method used in MPT instead of the commonly used absolute positional embeddings. The video is structured into chapters, starting with an introduction to MPT-7B, followed by detailed information about the model, a discussion on finetuned MPT models, and finally an explanation of ALiBi.
https://www.youtube.com/watch?v=rb02ZnkcW4Y

The paper "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation" by Ofir Press, Noah A. Smith, and Mike Lewis explores how transformer models can extrapolate at inference time for sequences longer than those seen during training. The authors introduce a new and more efficient position method, Attention with Linear Biases (ALiBi), which biases query-key attention scores with a penalty proportional to their distance, rather than adding positional embeddings to word embeddings. They demonstrate that ALiBi can train a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's recency bias also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.
https://arxiv.org/pdf/2108.12409v2.pdf




////

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

# Document Loader
from langchain.document_loaders import TextLoader
loader = TextLoader('./state_of_the_union.txt')
documents = loader.load()

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text
    
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

!pip install faiss-cpu

from langchain.vectorstores import FAISS

db = FAISS.from_documents(docs, embeddings)


query = "What did the president say about the Supreme Court"
docs = db.similarity_search(query)

from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub


llm=HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0, "max_length":512})

chain = load_qa_chain(llm, chain_type="stuff")

query = "What did the president say about the Supreme Court"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

from langchain.document_loaders import UnstructuredPDFLoader
from langchain.indexes import VectorstoreIndexCreator

loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]
loaders

index = VectorstoreIndexCreator(
    embedding=HuggingFaceEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)

//change

from langchain.chains import RetrievalQA
chain = RetrievalQA.from_chain_type(llm=llm, 
                                    chain_type="stuff", 
                                    retriever=index.vectorstore.as_retriever(), 
                                    input_key="question")
				    
llm=HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0, "max_length":512})


---q&a

import re
import json

def parse_verbs(verbs, text):
    result = []
    for verb in verbs:
        verb_info = {}
        verb_info['verb'] = verb
        
        # Extract verb definition
        verb_defn = re.findall(rf"{verb} ⇒ ([^\n]+)", text)
        if verb_defn:
            verb_info['definition'] = verb_defn[0]
        
        # Extract verb explanation and examples
        verb_explanation = re.search(rf"{verb} ⇒ [^\n]+\n([^\n]+)\n([^2]+)", text, re.DOTALL)
        if verb_explanation:
            verb_info['explanation'] = verb_explanation.group(1).strip()
            verb_info['examples'] = verb_explanation.group(2).strip().split('\n')
        
        result.append(verb_info)
    
    return result

# Test the function
verbs = ['plus', 'negate', 'minus', 'first', 'times', 'sqrt', 'div', 'mod', 'where', 'flip', 'and', 'reverse', 'or', 'asc(desc)', 'less (more)', 'freq', 'equal', 'not', 'match', 'enlist', 'cat', 'sort', '[f]cut', 'count', '[f]take', 'floor', '[f]drop', 'str', 'parse', 'unique', 'find', 'type', '[f]at', 'value', 'dict']
text = """
2.1 plus ⇒ x+y
Add x and y. Arguments can be elements or lists but if both x and y are lists then they must be of equal length.

----

{
        "verb": "plus",
        "definition": "x+y",
        "explanation": "Add x and y. Arguments can be elements or lists but if both x and y are lists then they must be of equal length.",
        "examples": [
            "3+7",
            "10",
            "",
            "a:3;",
            " a+8",
            "11",
            "",
            " 3+4 5 6 7",
            "7 8 9 10",
            "",
            "3 4 5+4 5 6",
            "7 9 11",
            "",
            " 3 4+1"
        ]
    },
    

////
{
  "question": "What is the Shakti programming language?",
  "answer": "Shakti, also known as k9, is a programming language developed by Arthur Whitney and the team at Shakti. The language comes from a lineage of similar languages going back to APL. It's known for its terse syntax and high-speed data processing and analysis capabilities.",
  "example": {
    "input": "{x@(!#x)+\!#y}",
    "output": "This is an example of a Shakti (k9) code snippet. The actual output would depend on the context and the specific values of 'x' and 'y'."
  }
}


import json
import re

input_text = """
2.5 times ⇒ x*y
Mutliply x and y.
3*4
12
3*4 5 6
12 15 18
1 2 3*4 5 6
4 10 18

2.6 sqrt ⇒ %x
Return the square root of x.
%25
5.000e+00
%14.4
3.794e+00

2.7 div ⇒ x%y
Divide x by y.
12%5
2.400e+00
6%2
3.000e+00
"""

# Split input_text into sections using two or more newline characters as the separator
sections = re.split('\n{2,}', input_text.strip())

output = []

for section in sections:
    # Extract section number, verb, usage, and description
    match_header = re.match(r"(\d+\.\d+) (\w+) ⇒ (.*)\n(.*)", section)
    if match_header:
        section_number = match_header.group(1)
        verb = match_header.group(2)
        usage = match_header.group(3)
        description = match_header.group(4)
    
        # Extract examples
        match_examples = re.findall(r"((?:\d+|\%|\*|\s)+)\n((?:\d+|\.\d+|e\+\d+|\s)+)", section)
        examples = [{'input': example[0], 'output': example[1]} for example in match_examples]

        data = {
            'section': section_number,
            'verb': verb,
            'usage': usage,
            'description': description,
            'examples': examples
        }

        output.append(data)

json_output = json.dumps(output, indent=2)
print(json_output)

====section split
import re

# Multi-line string
s = """2.5 times ⇒ x*y
Mutliply x and y.

 3*4
12

3*4 5 6
12 15 18

1 2 3*4 5 6
4 10 18
2.6 sqrt ⇒ %x
Return the square root of x.

 %25
5.000e+00
 %14.4
3.794e+00
2.7 div ⇒ x%y
Divide x by y.

 12%5
2.400e+00

6%2
3.000e+00"""

# Split the string into sections at double newlines
sections = re.split(r'\n{2,}', s)

# Iterate over the sections
for section in sections:
    # Split the section into lines
    lines = section.split('\n')
    
    # Split the first line into its parts
    first_line_parts = re.split(r'\s+|⇒', lines[0])
    first_line_parts = [part.strip() for part in first_line_parts]
    
    print(first_line_parts)

# This will output each first line as a list of its parts

==q answer

[
  {
    "question": "What is the sum of the first three numbers in the reversed list?",
    "answer": "15",
    "example": "_3|1 2 3 4 5"
  },
  {
    "question": "What are the sorted unique elements of the two combined lists?",
    "answer": "1 2 3 4 5 6",
    "example": "^?1 2 3,4 4 5 6"
  },
  {
    "question": "What is the remainder when the sum of the first three numbers is divided by the last number in the list?",
    "answer": "2",
    "example": "_3|1 2 3 4 5!5"
  },
  {
    "question": "What are the first three elements of the sorted list?",
    "answer": "1 2 3",
    "example": "_3^5 1 3 2 4"
  },
  {
    "question": "What is the first element of the list after removing the first three elements?",
    "answer": "4",
    "example": "__3_1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two elements in the reversed list?",
    "answer": "9",
    "example": "+/_2|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two numbers in the sorted list?",
    "answer": "3",
    "example": "+/_2^5 1 3 2 4"
  },
  {
    "question": "What is the maximum value in the reversed list?",
    "answer": "5",
    "example": "|/|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first three elements in the list after removing the first two elements?",
    "answer": "12",
    "example": "+/_3_2_1 2 3 4 5"
  },
  {
    "question": "What is the remainder when the sum of the first two numbers is divided by the last number in the reversed list?",
    "answer": "0",
    "example": "+/_2|1 2 3 4 5!5"
  },
  {
    "question": "What is the sum of the first three numbers in the list after removing duplicates?",
    "answer": "6",
    "example": "+/_3?1 2 3 3 4 5"
  },
  {
    "question": "What is the square root of the first number in the sorted list?",
    "answer": "1",
    "example": "%_*^5 1 3 2 4"
  },
  {
    "question": "What is the sum of the first three numbers in the reversed list?",
    "answer": "15",
    "example": "+/_3|1 2 3 4 5"
  },
  {
    "question": "What is the sum of the first two numbers in the list after removing the first three elements?",
    "answer": "9",
    "example": "+/_2_3_1 2 3 4 5"
  },
  {
    "question": "What is

===

[
  {
    "question": "How to add two numbers in Shakti?",
    "answer": "You can use the 'plus' verb for addition. For example, to add 3 and 7, you would use the command '3+7'.",
    "example": "3+7"
  },
  {
    "question": "How to subtract two numbers in Shakti?",
    "answer": "You can use the 'minus' verb for subtraction. For example, to subtract 2 from 5, you would use the command '5-2'.",
    "example": "5-2"
  },
  
  ===
  [{
"question": "How do I calculate the average of a list of numbers in Shakti?",
"answer": "You can calculate the average of a list of numbers by summing up the numbers using the '+' operator and then dividing by the count of the numbers using the '%' operator.",
"example": "list: 1 2 3 4 5;\navg: (+/list) % #list;"
},
{
"question": "How do I find the maximum value in a list of numbers in Shakti?",
"answer": "You can find the maximum value in a list of numbers using the 'max' function.",
"example": "list: 1 2 3 4 5;\nmaxValue: max list;"
},
{
"question": "How do I create a list of even numbers up to a given number in Shakti?",
"answer": "You can create a list of even numbers up to a given number by using the 'til' function to create a list from 0 to the number and the 'mod' function with an argument of 2 to check for even numbers.",
"example": "number: 10;\nevenNumbers: 2 * til number % 2;"
},
{
"question": "How do I find the maximum value in a list of numbers in Shakti?",
"answer": "You can find the maximum value in a list of numbers using the 'max' function.",
"example": "list: 1 2 3 4 5;\nmaxValue: max list;"
},
{
"question": "How do I calculate the sum of the squares of a list of numbers in Shakti?",
"answer": "You can calculate the sum of the squares of a list of numbers by squaring the numbers using the 'sqr' function and then summing up the squares using the '+' operator.",
"example": "list: 1 2 3 4 5;\nsumSquares: +/ sqr list;"
},
{
"question": "How do I filter out numbers less than a certain value from a list of numbers in Shakti?",
"answer": "You can filter out numbers less than a certain value from a list of numbers by using the 'where' function with a comparison function.",
"example": "list: 1 2 3 4 5;\nvalue: 3;\nfiltered: list where list > value;"
},
{
"question": "How do I count the number of occurrences of a certain value in a list in Shakti?",
"answer": "You can count the number of occurrences of a certain value in a list by using the 'count' function with a comparison function.",
"example": "list: 1 2 3 4 5;\nvalue: 3;\ncount: count list where list = value;"
},

////

import json
from random import choice

# Definitions of Shakti operations
shakti_definitions = {
    '+': 'addition',
    '-': 'subtraction',
    '*': 'multiplication',
    '%': 'division',
    'til': 'sequence generation',
    'max': 'maximum',
    'min': 'minimum',
    'avg': 'average',
    'count': 'count',
    'where': 'filter'
}

# List of sample data and functions
sample_data = ['list: 1 2 3 4 5', 'number: 10', 'value: 3']
sample_functions = ['avg: (+/list) % #list', 'evenNumbers: 2 * til number % 2', 
                    'maxValue: max list', 'maxIndex: list ? max list', 
                    'stringList: string list', 'sumSquares: +/ sqr list', 
                    'filtered: list where list > value', 
                    'count: count list where list = value']

# Function to generate question, answer, example tuple
def generate_tuple():
    op = choice(list(shakti_definitions.keys()))
    data = choice(sample_data)
    func = choice(sample_functions)

    question = f"How do I perform {shakti_definitions[op]} in Shakti?"
    answer = f"You can perform {shakti_definitions[op]} by using the '{op}' operation."
    example = f"{data};\n{func};"

    return {"question": question, "answer": answer, "example": example}

# Function to generate a number of tuples
def generate_tuples(n):
    return [generate_tuple() for _ in range(n)]

# Generate 5 tuples as an example
tuples = generate_tuples(5)

# Convert to JSON
json_output = json.dumps(tuples, indent=4)

# Print the JSON
print(json_output)

/===

import numpy as np
import pandas as pd

shakti_to_python = {
    '+': {
        'desc': 'addition',
        'python': 'np.add(a, b)',
    },
    '-': {
        'desc': 'subtraction',
        'python': 'np.subtract(a, b)',
    },
    '*': {
        'desc': 'multiplication',
        'python': 'np.multiply(a, b)',
    },
    '%': {
        'desc': 'division',
        'python': 'np.divide(a, b)',
    },
    'til': {
        'desc': 'sequence generation',
        'python': 'np.arange(n)',
    },
    'max': {
        'desc': 'maximum',
        'python': 'np.max(a)',
    },
    'min': {
        'desc': 'minimum',
        'python': 'np.min(a)',
    },
    'avg': {
        'desc': 'average',
        'python': 'np.mean(a)',
    },
    'count': {
        'desc': 'count',
        'python': 'len(a)',
    },
    'where': {
        'desc': 'filter',
        'python': 'a[a > n]',
    }
}

/====
Disclaimer1

Creating a comprehensive mapping between Shakti operations and Python code would involve systematically associating each Shakti operation with an equivalent Python function, likely from libraries such as NumPy and Pandas.

However, please note that Shakti is designed as a high-performance, array-based language that can handle complex data operations in a minimalistic way. Therefore, translating Shakti operations into Python code may result in more verbose and complex Python scripts, as Python is a general-purpose language with a different design philosophy.

Here are some basic examples of Shakti operations and their Python equivalents, using NumPy and Pandas:



Disclaimer2

This dictionary provides a basic translation of some Shakti operations into Python code using NumPy. Note that the Python code is represented as strings, so you would need to use eval() or similar methods to execute this code, which can be dangerous and is generally not recommended. Also, the Python code assumes that the variables (a, b, n) are already defined.

Keep in mind that this is a simplified representation, and it doesn't cover all Shakti operations, especially more complex ones. Creating a comprehensive mapping would require a deep understanding of both Shakti and Python, and it would be a significant undertaking.







=/

results2=[]
badresults2=[]
for keyword in keywords:
      print(keyword)
      keyword2=keyword['link'].split("/")[0]
      url2="https://code.kx.com/q/ref/"+keyword2
      r=requests.get(url2)
      soup=BeautifulSoup(r.text,"html.parser")
      reflist=[x for x in soup.find_all("code",{"class":"language-syntax"})]
      startline=(r.text).find('class="headerlink" href="#'+keyword2) 
      if startline > 10:
        try:
          soup = BeautifulSoup(r.text[startline:-1], 'html.parser')

          instruction = soup.select_one('code.language-syntax').get_text()
          name = soup.select_one('p > em').get_text()
          example = soup.select_one('pre.highlight > code.language-q').get_text()

          def extract_key_and_value_from_li(li):
              key = li.select_one('code').get_text()
              value = li.text.replace(key, '').strip()
              return key, value

          where_items = soup.select('ul > li')

          description = {}
          for item in where_items:
              if item.find('ul'):
                  nested_list_items = item.select('ul > li')
                  nested_dict = {}
                  for nested_item in nested_list_items:
                      key, value = extract_key_and_value_from_li(nested_item)
                      nested_dict[key] = value
                  key, value = extract_key_and_value_from_li(item)
                  description[key] = {**nested_dict, **{'description': value}}
              else:
                  key, value = extract_key_and_value_from_li(item)
                  description[key] = value

          result_description_list = soup.select('ul > li')
          result = {}
          for result_item in result_description_list[-2:]:
              key, value = extract_key_and_value_from_li(result_item)
              result[key] = value

          json_data = {
              'instruction': instruction,
              'name': name,
              'example': example,
              'description': description,
              'result': result
          }
          soup = BeautifulSoup(r.text, 'lxml')

          # Find all 'pre' elements with the 'highlight' class
          pre_elements = soup.find_all('pre', class_='highlight')

          # Extract the text from the found elements
          texts = [pre_element.text for pre_element in pre_elements]

          # Print the extracted texts
          results2.append([keyword,json_data,texts])
          print(keyword," good ")
        except:
          badresults2.append(keyword)
          print(keyword," bad ")
      else:
        badresults2.append(keyword)
        print(keyword," bad ")
	/=====
	
soup = BeautifulSoup(r.text, 'lxml')

# Find all 'pre' elements with the 'highlight' class
pre_elements = soup.find_all('pre', class_='highlight')

# Extract the text from the found elements
texts = [pre_element.text for pre_element in pre_elements]

# Print the extracted texts
for text in texts:
    print(text)

/====

	
	
x^y    ^[x;y]
q)0^1 2 3 0N
1 2 3 0
q)100^1 2 -5 0N 10 0N
1 2 -5 100 10 100
q)1.0^1.2 -4.5 0n 0n 15
1.2 -4.5 1 1 15
q)`nobody^`tom`dick``harry
`tom`dick`nobody`harry
q)1 2 3 4 5^6 0N 8 9 0N
6 2 8 9 5
q)a:11.0 2.1 3.1 0n 4.5 0n
q)type a
9h
q)10^a
11 2.1 3.1 10 4.5 10
q)type 10^a
9h
q)(`a`b`c!1 2 3)^`b`c!0N 30
a| 1
b| 2
c| 30


chatGpt4

public class PalindromeChecker {

    public static boolean isPalindrome(int num) {
        int reverse = 0;
        int temp = num;
        int mask = (Integer.SIZE - Integer.numberOfLeadingZeros(num)) - 1;
        int i = 0;
        do {
            reverse = (reverse << 1) | ((temp >>> i) & 1);
            reverse = (reverse << 1) | ((temp >>> (mask - i)) & 1);
            i++;
        } while (i <= mask / 2);
        return num == reverse;
    }

    public static void main(String[] args) {
        int num = 12321;
        if (isPalindrome(num)) {
            System.out.println(num + " is a palindrome.");
        } else {
            System.out.println(num + " is not a palindrome.");
        }
    }
}

The fastest way to check if an integer is a palindrome would be to use an algorithm that involves only mathematical operations and no loops. One such algorithm involves reversing the digits of the number and comparing it with the original number. Here's an implementation in Java:

public static boolean isPalindrome(int num) {
    if (num < 0) {
        return false; // negative numbers cannot be palindromes
    }
    int reversed = 0;
    int original = num;
    while (num != 0) {
        reversed = reversed * 10 + num % 10;
        num /= 10;
    }
    return original == reversed;
}

correct but what it is a very long bit pattern that you want to check if it is symmetric in a certain basis?

If you have a very long bit pattern that you want to check for symmetry in a certain basis, you can use a bit-manipulation approach that takes advantage of bitwise operations to perform the comparison.

Here's an implementation in Java that checks for symmetry in a certain basis:

public static boolean isSymmetric(long num, int basis) {
    long mask = (1L << basis) - 1L;
    long reversed = 0L;
    while (num != 0L) {
        reversed = (reversed << basis) | (num & mask);
        num >>>= basis;
    }
    return num == reversed;
}



/=====

vicuna

public class PalindromeNumber {
public static void main(String[] args) {
int num = 121;
if (num == 0) {
System.out.println(num + " is a palindrome.");
}
else {
String reverse = String.valueOf(num);
while (reverse.length()!= 0) {
if (reverse.charAt(0)!= reverse.charAt(reverse.length() - 1)) {
System.out.println(num + " is not a palindrome.");
break;
}
reverse = reverse.substring(1, reverse.length() - 1);
}
}
}

This program first initializes the variable `num` to a non-zero value, since 0 is considered a palindrome. It then checks if `num` is equal to 0 using an `if` statement. If `num` is equal to 0, the program prints out that the number is a palindrome.
Otherwise, the program converts the number to a string using the `String.valueOf()` method, and then uses a `while` loop to compare the first and last characters of the string. If the first and last characters are the same, the program continues looping until it finds two characters that are not the same, at which point it prints out that the number is not a palindrome and exits the loop.
This program should print out the following output:

/===
koala

BEGINNING OF CONVERSATION: USER: can you write a java program to check if a number is palindrome or not?

Here's an example Java program that checks if a given integer is a palindrome or not:

```java
import java.util.Scanner;

public class PalindromeChecker {
    public static void main(String[] args) {
        Scanner sc = new Scanner(System.in);

        System.out.print("Enter the number: ");
        int num = sc.nextInt();

        boolean isPalindrome = true;
        for (int i = 0; i < num - 1; i++) {
            if (num % 2!= 0) {
                isPalindrome = false;
                break;
            }
        }

        System.out.println("Is " + num + " a palindrome?");
        System.out.println("Answer: " + isPalindrome);
    }
}

/===
open assistant

public class CheckPalindrome {
    public static void main(String[] args) {
        int num = 123;
        
        // Reverse the digits of the number and compare it with original number
        int reverseNum = 0;
        while (num > 0) {
            int lastDigit = num % 10;
            
            reverseNum += lastDigit * pow(10, len - 1);
            num /= 10;
        }
                
        if (reverseNum == num) {
            System.out.println("Number is a Palindrome");
        } else {
            System.out.println("Number is not a Palindrome");
        }
    }
}

no good

import org.junit.*;

public class CheckPalindromeTest {
	@BeforeClass
	public static void setup() {}
    
    @AfterClass
    public static void tearDown(){}
    
    @Before
    public void setUp(){}
    
    @After
    public void TearDown(){}
    
	/** Test whether 123456789 is a palindrome */  
	@Test
	public void shouldReturnTrueFor123456789(){
            Assert.assertEquals(true , new CheckPalindrome().isPalindrome(123));
	} 
	
	/** Test whether 99 is a palindrome */ 
	@Test
	public void shouldReturnFalseFor99(){
                Assert.assertEquals(false, new CheckPalindrome().isPalindrome(99));
	} 
}

/====



from datasets import load_dataset
from transformers import LLaMATokenizer


tokenizer = LLaMATokenizer.from_pretrained("decapoda-research/llama-7b-hf", add_eos_token=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id

data = load_dataset("json", data_files="alpaca_data.json")


def generate_prompt(data_point):
    # sorry about the formatting disaster gotta move fast
    if data_point["instruction"]:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{data_point["instruction"]}

### Input:
{data_point["input"]}

### Response:
{data_point["output"]}"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{data_point["instruction"]}

### Response:
{data_point["output"]}"""


data = data.map(lambda data_point: {"prompt": tokenizer(generate_prompt(data_point))})

import os

# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
import torch.nn as nn
import bitsandbytes as bnb
from datasets import load_dataset
import transformers
from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer
from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model

# Setting for A100 - For 3090 
MICRO_BATCH_SIZE = 8  # change to 4 for 3090
BATCH_SIZE = 128
GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE
EPOCHS = 2  # paper uses 3
LEARNING_RATE = 2e-5  # from the original paper
CUTOFF_LEN = 256  # 256 accounts for about 96% of the data
LORA_R = 4
LORA_ALPHA = 16
LORA_DROPOUT = 0.05


model = LLaMAForCausalLM.from_pretrained(
    "decapoda-research/llama-7b-hf",
    load_in_8bit=True,
    device_map="auto",
)
tokenizer = LLaMATokenizer.from_pretrained(
    "decapoda-research/llama-7b-hf", add_eos_token=True
)

model = prepare_model_for_int8_training(model)

config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, config)
tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token
data = load_dataset("json", data_files="alpaca_data.json")

def generate_prompt(data_point):
    # sorry about the formatting disaster gotta move fast
    if data_point["input"]:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
### Instruction:
{data_point["instruction"]}
### Input:
{data_point["input"]}
### Response:
{data_point["output"]}"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
{data_point["instruction"]}
### Response:
{data_point["output"]}"""


data = data.shuffle().map(
    lambda data_point: tokenizer(
        generate_prompt(data_point),
        truncation=True,
        max_length=CUTOFF_LEN,
        padding="max_length",
    )
)

trainer = transformers.Trainer(
    model=model,
    train_dataset=data["train"],
    args=transformers.TrainingArguments(
        per_device_train_batch_size=MICRO_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=100,
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=True,
        logging_steps=1,
        output_dir="lora-alpaca",
        save_total_limit=3,
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False
trainer.train(resume_from_checkpoint=False)

model.save_pretrained("lora-alpaca")

/=====
// First, create a wrapper function for your C code:

#include <stdio.h>

void c_code_wrapper(void *context, void (*callback)(void *context)) {
    // Your C code here
    callback(context);
}

// Then expose this wrapper as a Rust extern function:

extern "C" fn c_code_wrapper(context: *mut c_void, callback: extern fn(*mut c_void)) {
    unsafe {
        c_code_wrapper(context, callback);
    }
}

// Finally, call the wrapper from Rust using an async closure:

let mut context = 0;
let callback = |context: *mut c_void| {
    // Your Rust code here
};

let future = async {
    c_code_wrapper(&mut context, callback);
};

// The future will execute the C code asynchronously.

from bs4 import BeautifulSoup
import requests

r=requests.get("http://code.kx.com/q/ref")
soup=BeautifulSoup(r.text,"html.parser")
reflist=[x for x in soup.find_all('a') if str(x)[4:7] == "ref"]
reflist2=[x for x in reflist if len(str(x).split('"'))>3]
[{'link':(str(x)).split('"')[1],'descr':(str(x)).split('"')[3]} for x in reflist2[:-2]]
reflist=[x for x in soup.find_all("div",{"class":"md-content"})]

